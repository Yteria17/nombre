# üìö COOKBOOK - Guide de D√©pannage et Recettes

Guide pratique de troubleshooting et recettes pour r√©soudre les probl√®mes courants avec les r√©seaux de neurones.

---

## üéØ Table des Mati√®res

1. [Probl√®mes d'Apprentissage](#probl√®mes-dapprentissage)
2. [Probl√®mes de Performance](#probl√®mes-de-performance)
3. [Recettes Communes](#recettes-communes)
4. [Optimisation des Hyperparam√®tres](#optimisation-des-hyperparam√®tres)
5. [Checklist de Debugging](#checklist-de-debugging)

---

## üî• Probl√®mes d'Apprentissage

### ‚ùå Probl√®me: La Loss ne Diminue Pas

**Sympt√¥mes:**
```
Epoch 1 - Loss: 2.3025
Epoch 2 - Loss: 2.3024
Epoch 3 - Loss: 2.3023
...
```

**Causes possibles et solutions:**

#### 1. **Learning Rate trop faible**

```python
# ‚ùå Mauvais
model = NeuralNetwork([784, 128, 10], learning_rate=0.00001)

# ‚úÖ Bon
model = NeuralNetwork([784, 128, 10], learning_rate=0.01)
```

**Action:** Augmenter le learning rate (essayer 0.001, 0.01, 0.1)

#### 2. **Poids mal initialis√©s**

```python
# V√©rifier l'initialisation
W = model.parameters['W1']
print(f"Mean: {W.mean():.6f}, Std: {W.std():.6f}")

# Devrait √™tre proche de:
# Mean: ~0, Std: ~sqrt(2/n_input)
```

**Action:** Le code utilise d√©j√† He initialization, mais v√©rifier que c'est bien appliqu√©.

#### 3. **Donn√©es non normalis√©es**

```python
# V√©rifier les donn√©es
print(f"X_train - Min: {X_train.min()}, Max: {X_train.max()}")

# ‚úÖ Devrait √™tre [0, 1]
```

**Action:** Normaliser les donn√©es: `X = X / 255.0`

---

### ‚ùå Probl√®me: La Loss Explose (NaN)

**Sympt√¥mes:**
```
Epoch 1 - Loss: 2.305
Epoch 2 - Loss: 156.4
Epoch 3 - Loss: nan
```

**Causes possibles et solutions:**

#### 1. **Learning Rate trop √©lev√©**

```python
# ‚ùå Mauvais
model = NeuralNetwork([784, 128, 10], learning_rate=1.0)

# ‚úÖ Bon
model = NeuralNetwork([784, 128, 10], learning_rate=0.01)
```

**Action:** R√©duire le learning rate d'un facteur 10

#### 2. **Gradient Exploding**

```python
# Ajouter gradient clipping (si impl√©ment√©)
max_grad_norm = 5.0
for key in gradients:
    grad_norm = np.linalg.norm(gradients[key])
    if grad_norm > max_grad_norm:
        gradients[key] *= max_grad_norm / grad_norm
```

**Action:** R√©duire le learning rate ou ajouter gradient clipping

---

### ‚ùå Probl√®me: Surapprentissage (Overfitting)

**Sympt√¥mes:**
```
Train Accuracy: 0.995
Val Accuracy:   0.920
Test Accuracy:  0.918
```

**Causes et solutions:**

#### 1. **R√©seau trop grand pour le dataset**

```python
# ‚ùå Trop de param√®tres
model = NeuralNetwork([784, 1024, 512, 256, 128, 10])

# ‚úÖ Plus raisonnable
model = NeuralNetwork([784, 256, 128, 10])
```

#### 2. **Pas assez de donn√©es d'entra√Ænement**

**Solutions:**
- Utiliser data augmentation
- Augmenter le dataset
- R√©duire la complexit√© du mod√®le

#### 3. **Trop d'√©poques**

```python
# Surveiller val_acc et arr√™ter quand elle stagne
# ou utiliser early stopping
```

---

### ‚ùå Probl√®me: Sous-apprentissage (Underfitting)

**Sympt√¥mes:**
```
Train Accuracy: 0.850
Val Accuracy:   0.845
Test Accuracy:  0.843
```

**Solutions:**

#### 1. **Augmenter la capacit√© du r√©seau**

```python
# ‚ùå Trop petit
model = NeuralNetwork([784, 32, 10])

# ‚úÖ Plus de capacit√©
model = NeuralNetwork([784, 256, 128, 64, 10])
```

#### 2. **Entra√Æner plus longtemps**

```python
# Augmenter le nombre d'√©poques
model.train(X_train, y_train, X_val, y_val, epochs=30)
```

#### 3. **Changer l'optimiseur**

```python
# ‚ùå SGD basique peut √™tre trop lent
model = NeuralNetwork([784, 256, 128, 10], optimizer='sgd')

# ‚úÖ Adam est souvent plus efficace
model = NeuralNetwork([784, 256, 128, 10], optimizer='adam')
```

---

## ‚ö° Probl√®mes de Performance

### ‚ùå Probl√®me: Entra√Ænement Trop Lent

**Solutions:**

#### 1. **Augmenter la taille des batches**

```python
# ‚ùå Petits batches = beaucoup d'it√©rations
model.train(X_train, y_train, X_val, y_val, batch_size=16)

# ‚úÖ Plus rapide
model.train(X_train, y_train, X_val, y_val, batch_size=128)
```

**Note:** Batch size trop grand peut r√©duire la g√©n√©ralisation.

#### 2. **R√©duire la complexit√© du mod√®le**

```python
# Si le mod√®le est trop complexe pour vos besoins
# Commencer simple et augmenter si n√©cessaire
```

#### 3. **R√©duire le nombre de features**

```python
# Pour MNIST, on utilise d√©j√† tous les pixels (784)
# Pour d'autres datasets, consid√©rer PCA ou feature selection
```

---

### ‚ùå Probl√®me: Accuracy Plafonne √† ~10%

**Cause:** Le mod√®le pr√©dit toujours la m√™me classe (ou pr√©dit au hasard)

**Solutions:**

#### 1. **V√©rifier l'architecture**

```python
# V√©rifier qu'il y a bien 10 neurones en sortie pour MNIST
print(model.layer_dims)  # Devrait terminer par 10
```

#### 2. **V√©rifier la fonction de loss**

```python
# S'assurer d'utiliser cross-entropy pour classification
```

#### 3. **V√©rifier les labels**

```python
# Les labels doivent √™tre 0-9, pas 1-10
print(f"Labels uniques: {np.unique(y_train)}")
```

---

## üç≥ Recettes Communes

### üìñ Recette 1: Entra√Æner un Mod√®le de Base

```python
from src.network import NeuralNetwork
from src.utils import load_mnist_data

# Charger les donn√©es
X_train, y_train, X_val, y_val, X_test, y_test = load_mnist_data()

# Cr√©er le mod√®le
model = NeuralNetwork(
    layer_dims=[784, 256, 128, 10],
    learning_rate=0.01,
    optimizer='adam'
)

# Entra√Æner
model.train(X_train, y_train, X_val, y_val,
            epochs=15, batch_size=128, verbose=True)

# √âvaluer
test_acc = model.accuracy(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# Sauvegarder
model.save('models/my_model.pkl')
```

**R√©sultat attendu:** ~96-97% accuracy

---

### üìñ Recette 2: Optimiser les Hyperparam√®tres

```python
# Tester diff√©rentes configurations
configs = [
    {'layers': [784, 128, 10], 'lr': 0.01, 'opt': 'adam'},
    {'layers': [784, 256, 128, 10], 'lr': 0.01, 'opt': 'adam'},
    {'layers': [784, 512, 256, 10], 'lr': 0.005, 'opt': 'adam'},
]

results = []

for config in configs:
    model = NeuralNetwork(
        layer_dims=config['layers'],
        learning_rate=config['lr'],
        optimizer=config['opt']
    )

    model.train(X_train, y_train, X_val, y_val, epochs=10)

    val_acc = model.accuracy(X_val, y_val)
    results.append((config, val_acc))

# Trier par accuracy
results.sort(key=lambda x: x[1], reverse=True)
best_config, best_acc = results[0]
print(f"Meilleure config: {best_config} - Acc: {best_acc:.4f}")
```

---

### üìñ Recette 3: Cr√©er un Ensemble de Mod√®les

```python
# Entra√Æner plusieurs mod√®les
models = []

for i in range(5):
    model = NeuralNetwork([784, 256, 128, 10], learning_rate=0.01, optimizer='adam')
    model.train(X_train, y_train, X_val, y_val, epochs=10, verbose=False)
    models.append(model)
    print(f"Model {i+1} - Val Acc: {model.accuracy(X_val, y_val):.4f}")

# Pr√©dictions par vote
def ensemble_predict(models, X):
    all_preds = [model.predict(X) for model in models]
    # Vote majoritaire
    ensemble_preds = []
    for i in range(X.shape[0]):
        votes = [preds[i] for preds in all_preds]
        ensemble_preds.append(max(set(votes), key=votes.count))
    return np.array(ensemble_preds)

# √âvaluer l'ensemble
y_pred_ensemble = ensemble_predict(models, X_test)
ensemble_acc = np.mean(y_pred_ensemble == y_test)
print(f"\nEnsemble Accuracy: {ensemble_acc:.4f}")
```

**R√©sultat attendu:** +1-2% vs mod√®le individuel

---

### üìñ Recette 4: D√©boguer un Mod√®le qui Ne Converge Pas

```python
# 1. V√©rifier les donn√©es
print("="*50)
print("V√âRIFICATION DES DONN√âES")
print("="*50)
print(f"X_train shape: {X_train.shape}")
print(f"X_train range: [{X_train.min():.3f}, {X_train.max():.3f}]")
print(f"y_train shape: {y_train.shape}")
print(f"y_train unique: {np.unique(y_train)}")

# 2. V√©rifier l'initialisation
model = NeuralNetwork([784, 128, 10], learning_rate=0.01)
print("\n" + "="*50)
print("V√âRIFICATION DE L'INITIALISATION")
print("="*50)
for key, param in model.parameters.items():
    print(f"{key}: shape={param.shape}, mean={param.mean():.6f}, std={param.std():.6f}")

# 3. Test sur un petit batch
print("\n" + "="*50)
print("TEST SUR PETIT BATCH")
print("="*50)
X_batch = X_train[:32]
y_batch = y_train[:32]

for epoch in range(10):
    # Forward
    A, cache = model.forward(X_batch)

    # Loss
    Y_batch = model.one_hot_encode(y_batch)
    loss = model.compute_loss(Y_batch, A)

    # Backward
    grads = model.backward(Y_batch, cache)

    # Update
    model.update_parameters(grads)

    print(f"Epoch {epoch+1}: Loss = {loss:.4f}")

print("\n‚úì Si la loss diminue sur ce petit batch, le mod√®le fonctionne !")
```

---

### üìñ Recette 5: Visualiser l'Apprentissage

```python
from src import visualize
from src.metrics import confusion_matrix

# Apr√®s entra√Ænement
# 1. Courbes d'apprentissage
visualize.plot_training_history(model.history, save_path='training.png')

# 2. Matrice de confusion
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred, num_classes=10)
visualize.plot_confusion_matrix(cm, class_names=[str(i) for i in range(10)],
                                save_path='confusion.png')

# 3. Poids de la premi√®re couche
visualize.plot_weights_visualization(model.parameters['W1'], n_neurons=64,
                                    save_path='weights.png')

# 4. Exemples de pr√©dictions
y_probs, _ = model.forward(X_test)
visualize.plot_sample_predictions(X_test, y_test, y_pred, y_probs,
                                  n_samples=25, save_path='predictions.png')

print("‚úì Visualisations sauvegard√©es!")
```

---

## üéõÔ∏è Optimisation des Hyperparam√®tres

### Learning Rate

| Valeur | Effet | Quand utiliser |
|--------|-------|----------------|
| 0.0001 | Tr√®s lent, stable | R√©seaux tr√®s profonds |
| 0.001  | Lent, stable | Architecture complexe |
| 0.01   | **Bon d√©faut** | La plupart des cas |
| 0.1    | Rapide, instable | R√©seaux simples, exp√©rimentation |
| 1.0    | Trop rapide, diverge | ‚ùå √âviter |

**Recette:** Commencer √† 0.01, diviser par 10 si √ßa diverge, multiplier par 10 si trop lent.

---

### Batch Size

| Taille | Avantages | Inconv√©nients |
|--------|-----------|---------------|
| 16-32  | Bonne g√©n√©ralisation | Tr√®s lent |
| 64-128 | **Bon compromis** | - |
| 256-512 | Tr√®s rapide | Moins bonne g√©n√©ralisation |
| 1024+  | Maximum de vitesse | Mauvaise g√©n√©ralisation |

**R√®gle d'or:** 128 est un bon d√©faut pour MNIST.

---

### Nombre de Couches

| Architecture | Capacit√© | Quand utiliser |
|-------------|----------|----------------|
| [784, 128, 10] | Faible | Baseline rapide |
| [784, 256, 128, 10] | **Moyenne** | **Recommand√©** |
| [784, 512, 256, 128, 10] | √âlev√©e | Dataset complexe |
| [784, 256, 128, 64, 32, 10] | Tr√®s √©lev√©e | Risque d'overfitting |

**Recette:** Commencer simple, augmenter si underfitting.

---

### Optimiseurs

| Optimiseur | Vitesse | Stabilit√© | Quand utiliser |
|-----------|---------|-----------|----------------|
| SGD | Lent | Stable | Baseline, compr√©hension |
| Momentum | Moyen | Stable | Alternative √† Adam |
| **Adam** | **Rapide** | **Tr√®s stable** | **Par d√©faut** |

**Recommandation:** Toujours commencer avec Adam.

---

## ‚úÖ Checklist de Debugging

### Avant l'Entra√Ænement

- [ ] **Donn√©es normalis√©es** : X entre [0, 1]
- [ ] **Labels corrects** : y entre [0, 9]
- [ ] **Architecture valide** : Input=784, Output=10
- [ ] **Batch size raisonnable** : 64-128
- [ ] **Learning rate appropri√©** : 0.001-0.01
- [ ] **Train/val/test splits** : Pas de fuite de donn√©es

### Pendant l'Entra√Ænement

- [ ] **Loss diminue** : Doit descendre progressivement
- [ ] **Accuracy augmente** : Sur train ET val
- [ ] **Pas de NaN** : V√©rifier les valeurs
- [ ] **Val acc suit train acc** : Gap < 5%
- [ ] **Logs clairs** : Afficher les m√©triques

### Apr√®s l'Entra√Ænement

- [ ] **Test accuracy raisonnable** : ~96-98% pour MNIST
- [ ] **Matrice de confusion** : Erreurs logiques ?
- [ ] **Pas de surapprentissage** : Train-Val gap < 5%
- [ ] **Reproductibilit√©** : Fixer le random seed
- [ ] **Mod√®le sauvegard√©** : .pkl existe et charge correctement

---

## üö® Erreurs Communes et Solutions

### Erreur: "IndexError: index out of bounds"

**Cause:** Mauvaise dimension des donn√©es

```python
# V√©rifier
print(f"X shape: {X.shape}")  # Devrait √™tre (n, 784)
print(f"y shape: {y.shape}")  # Devrait √™tre (n,)
```

---

### Erreur: "ValueError: operands could not be broadcast"

**Cause:** Incompatibilit√© de dimensions dans les calculs matriciels

```python
# V√©rifier les dimensions
print(f"W shape: {W.shape}")
print(f"X shape: {X.shape}")

# S'assurer que les multiplications sont coh√©rentes
# Z = X @ W (n, d) @ (d, h) ‚Üí (n, h)
```

---

### Erreur: "RuntimeWarning: overflow in exp"

**Cause:** Valeurs trop grandes dans softmax/sigmoid

**Solution:** D√©j√† g√©r√© par la soustraction du max dans softmax, mais v√©rifier le learning rate.

---

### Accuracy Reste √† 10%

**Cause:** Le mod√®le pr√©dit toujours la m√™me classe

**Solution:**
1. V√©rifier que les poids ne sont pas tous √† z√©ro
2. V√©rifier que le learning rate n'est pas trop faible
3. S'assurer que les labels sont corrects

---

## üí° Astuces et Best Practices

### 1. Toujours Commencer Simple

```python
# ‚úÖ Bon workflow
# 1. Baseline simple
model = NeuralNetwork([784, 128, 10], learning_rate=0.01, optimizer='adam')
model.train(X_train, y_train, X_val, y_val, epochs=5)

# 2. Si √ßa marche, complexifier
model = NeuralNetwork([784, 256, 128, 64, 10], learning_rate=0.01, optimizer='adam')
model.train(X_train, y_train, X_val, y_val, epochs=15)
```

### 2. Monitorer Pendant l'Entra√Ænement

```python
# Afficher verbose=True au d√©but
model.train(X_train, y_train, X_val, y_val, epochs=10, verbose=True)

# Observer:
# - Loss descend r√©guli√®rement ?
# - Val acc augmente avec train acc ?
# - Pas de plateau pr√©matur√© ?
```

### 3. Sauvegarder Souvent

```python
# Sauvegarder le meilleur mod√®le
best_val_acc = 0
for epoch in range(50):
    # ... entra√Ænement ...
    val_acc = model.accuracy(X_val, y_val)

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        model.save('models/best_model.pkl')
        print(f"‚úì Nouveau meilleur: {val_acc:.4f}")
```

### 4. Comparer Plusieurs Configurations

```python
# Utiliser le script benchmark
python benchmark.py
```

### 5. Visualiser pour Comprendre

```python
# Toujours cr√©er des visualisations
python train.py --visualize
python evaluate.py --model models/best_model.pkl --visualize
```

---

## üìû Support

Si vous rencontrez un probl√®me non couvert ici :

1. V√©rifier les notebooks (`notebooks/07_debugging_gradient_checking.ipynb`)
2. Consulter `CLAUDE.md` pour la documentation compl√®te
3. Examiner les tests (`tests/`) pour des exemples
4. Utiliser le mode verbose pour plus de d√©tails

---

**Bonne chance avec vos r√©seaux de neurones ! üöÄ**
