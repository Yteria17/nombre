{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Introduction aux R√©seaux de Neurones\n",
    "\n",
    "Bienvenue dans ce tutoriel interactif sur les r√©seaux de neurones ! \n",
    "\n",
    "Dans ce notebook, nous allons comprendre :\n",
    "- üéØ **Qu'est-ce qu'un r√©seau de neurones ?**\n",
    "- üîç **Comment fonctionne un neurone artificiel ?**\n",
    "- üèóÔ∏è **Comment construire un r√©seau ?**\n",
    "- üìä **Visualiser les concepts de base**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports n√©cessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Configuration pour de beaux graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques charg√©es avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Qu'est-ce qu'un Neurone Artificiel ?\n",
    "\n",
    "Un neurone artificiel est inspir√© du neurone biologique. Il a trois composants principaux :\n",
    "\n",
    "### üì• Les Entr√©es (Inputs)\n",
    "Le neurone re√ßoit plusieurs valeurs num√©riques : $x_1, x_2, ..., x_n$\n",
    "\n",
    "### ‚öñÔ∏è Les Poids (Weights)\n",
    "Chaque entr√©e est multipli√©e par un **poids** : $w_1, w_2, ..., w_n$\n",
    "\n",
    "Les poids d√©terminent l'importance de chaque entr√©e.\n",
    "\n",
    "### ‚ûï Le Biais (Bias)\n",
    "Un terme constant $b$ est ajout√© pour permettre plus de flexibilit√©.\n",
    "\n",
    "### ‚ö° La Fonction d'Activation\n",
    "Une fonction non-lin√©aire $f$ transforme le r√©sultat final.\n",
    "\n",
    "### üìê Formule Math√©matique\n",
    "\n",
    "$$\n",
    "\\text{output} = f\\left(\\sum_{i=1}^{n} w_i \\cdot x_i + b\\right)\n",
    "$$\n",
    "\n",
    "Ou plus simplement :\n",
    "\n",
    "$$\n",
    "y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_neuron():\n",
    "    \"\"\"\n",
    "    Visualise un neurone artificiel simple\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Entr√©es\n",
    "    inputs = [8, 6, 4]\n",
    "    input_labels = ['$x_1$', '$x_2$', '$x_3$']\n",
    "    colors_input = ['#3498db', '#2ecc71', '#f39c12']\n",
    "    \n",
    "    for i, (y_pos, label, color) in enumerate(zip(inputs, input_labels, colors_input)):\n",
    "        # Cercles pour les entr√©es\n",
    "        circle = plt.Circle((1, y_pos), 0.3, color=color, ec='black', linewidth=2, zorder=3)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(1, y_pos, label, ha='center', va='center', fontsize=16, fontweight='bold', color='white')\n",
    "        ax.text(0.2, y_pos, f'Entr√©e {i+1}', ha='right', va='center', fontsize=12)\n",
    "    \n",
    "    # Le neurone au centre\n",
    "    neuron_x, neuron_y = 5, 6\n",
    "    neuron_circle = plt.Circle((neuron_x, neuron_y), 0.8, color='#e74c3c', ec='black', linewidth=3, zorder=3)\n",
    "    ax.add_patch(neuron_circle)\n",
    "    ax.text(neuron_x, neuron_y+0.2, 'Œ£', ha='center', va='center', fontsize=28, fontweight='bold', color='white')\n",
    "    ax.text(neuron_x, neuron_y-0.3, '$f$', ha='center', va='center', fontsize=20, fontweight='bold', color='white')\n",
    "    ax.text(neuron_x, neuron_y-1.3, 'Neurone', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Fl√®ches des entr√©es vers le neurone avec poids\n",
    "    weights = ['$w_1$', '$w_2$', '$w_3$']\n",
    "    for i, (y_pos, weight, color) in enumerate(zip(inputs, weights, colors_input)):\n",
    "        arrow = FancyArrowPatch((1.3, y_pos), (neuron_x-0.9, neuron_y),\n",
    "                               arrowstyle='->', mutation_scale=30, linewidth=2.5,\n",
    "                               color=color, zorder=2)\n",
    "        ax.add_patch(arrow)\n",
    "        # Position du poids sur la fl√®che\n",
    "        mid_x, mid_y = (1.3 + neuron_x-0.9)/2, (y_pos + neuron_y)/2\n",
    "        ax.text(mid_x, mid_y+0.3, weight, ha='center', va='bottom', fontsize=14, \n",
    "               fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', edgecolor=color, linewidth=2))\n",
    "    \n",
    "    # Biais\n",
    "    bias_circle = plt.Circle((3, 2), 0.3, color='#9b59b6', ec='black', linewidth=2, zorder=3)\n",
    "    ax.add_patch(bias_circle)\n",
    "    ax.text(3, 2, '$b$', ha='center', va='center', fontsize=16, fontweight='bold', color='white')\n",
    "    ax.text(3, 1.3, 'Biais', ha='center', va='center', fontsize=12)\n",
    "    arrow_bias = FancyArrowPatch((3.3, 2.2), (neuron_x-0.6, neuron_y-0.7),\n",
    "                                arrowstyle='->', mutation_scale=30, linewidth=2.5,\n",
    "                                color='#9b59b6', zorder=2)\n",
    "    ax.add_patch(arrow_bias)\n",
    "    \n",
    "    # Sortie\n",
    "    output_circle = plt.Circle((9, 6), 0.3, color='#16a085', ec='black', linewidth=2, zorder=3)\n",
    "    ax.add_patch(output_circle)\n",
    "    ax.text(9, 6, '$y$', ha='center', va='center', fontsize=16, fontweight='bold', color='white')\n",
    "    ax.text(9.8, 6, 'Sortie', ha='left', va='center', fontsize=12)\n",
    "    arrow_output = FancyArrowPatch((neuron_x+0.8, neuron_y), (8.7, 6),\n",
    "                                  arrowstyle='->', mutation_scale=30, linewidth=3,\n",
    "                                  color='#16a085', zorder=2)\n",
    "    ax.add_patch(arrow_output)\n",
    "    \n",
    "    # Titre et formule\n",
    "    ax.text(5, 9.5, 'üß† Anatomie d\\'un Neurone Artificiel', ha='center', va='top', \n",
    "           fontsize=18, fontweight='bold')\n",
    "    ax.text(5, 0.5, '$y = f(w_1x_1 + w_2x_2 + w_3x_3 + b)$', ha='center', va='bottom',\n",
    "           fontsize=16, bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_neuron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Impl√©mentons un Neurone Simple\n",
    "\n",
    "Cr√©ons maintenant un neurone artificiel en Python !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuron:\n",
    "    \"\"\"\n",
    "    Un neurone artificiel simple\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs):\n",
    "        # Initialisation al√©atoire des poids et du biais\n",
    "        self.weights = np.random.randn(num_inputs)\n",
    "        self.bias = np.random.randn()\n",
    "        \n",
    "    def activate(self, x):\n",
    "        \"\"\"\n",
    "        Fonction d'activation sigmoid: f(x) = 1 / (1 + e^(-x))\n",
    "        Transforme n'importe quelle valeur en un nombre entre 0 et 1\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Calcule la sortie du neurone\n",
    "        \"\"\"\n",
    "        # √âtape 1: Somme pond√©r√©e\n",
    "        weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
    "        print(f\"  Somme pond√©r√©e: {weighted_sum:.4f}\")\n",
    "        \n",
    "        # √âtape 2: Activation\n",
    "        output = self.activate(weighted_sum)\n",
    "        print(f\"  Apr√®s activation: {output:.4f}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Cr√©ons un neurone avec 3 entr√©es\n",
    "neuron = SimpleNeuron(num_inputs=3)\n",
    "\n",
    "print(\"üß† Notre neurone a √©t√© cr√©√© !\")\n",
    "print(f\"\\nPoids: {neuron.weights}\")\n",
    "print(f\"Biais: {neuron.bias:.4f}\")\n",
    "\n",
    "# Testons avec des entr√©es\n",
    "test_inputs = np.array([0.5, 0.8, 0.2])\n",
    "print(f\"\\nüì• Entr√©es: {test_inputs}\")\n",
    "print(\"\\nüîÑ Calcul en cours...\")\n",
    "output = neuron.forward(test_inputs)\n",
    "print(f\"\\nüì§ Sortie finale: {output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Les Fonctions d'Activation\n",
    "\n",
    "Les fonctions d'activation sont essentielles ! Elles introduisent de la **non-lin√©arit√©** dans le r√©seau.\n",
    "\n",
    "Sans elles, un r√©seau de neurones ne serait qu'une r√©gression lin√©aire glorifi√©e ! üòÖ\n",
    "\n",
    "Voyons les principales fonctions d'activation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_functions():\n",
    "    \"\"\"\n",
    "    Visualise les principales fonctions d'activation\n",
    "    \"\"\"\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    \n",
    "    # D√©finition des fonctions\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    tanh = np.tanh(x)\n",
    "    relu = np.maximum(0, x)\n",
    "    leaky_relu = np.where(x > 0, x, 0.1 * x)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('üé® Les Fonctions d\\'Activation Principales', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Sigmoid\n",
    "    axes[0, 0].plot(x, sigmoid, linewidth=3, color='#3498db')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_title('Sigmoid: œÉ(x) = 1/(1+e‚ÅªÀ£)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('x')\n",
    "    axes[0, 0].set_ylabel('œÉ(x)')\n",
    "    axes[0, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[0, 0].text(0.5, 0.2, 'üìä Sortie: [0, 1]\\n‚úÖ Bon pour probabilit√©s\\n‚ö†Ô∏è Probl√®me: gradient dispara√Æt', \n",
    "                   transform=axes[0, 0].transAxes, fontsize=10,\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Tanh\n",
    "    axes[0, 1].plot(x, tanh, linewidth=3, color='#2ecc71')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_title('Tanh: tanh(x)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('x')\n",
    "    axes[0, 1].set_ylabel('tanh(x)')\n",
    "    axes[0, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[0, 1].text(0.5, 0.2, 'üìä Sortie: [-1, 1]\\n‚úÖ Centr√© sur z√©ro\\n‚ö†Ô∏è Aussi gradient dispara√Æt', \n",
    "                   transform=axes[0, 1].transAxes, fontsize=10,\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    # ReLU\n",
    "    axes[1, 0].plot(x, relu, linewidth=3, color='#e74c3c')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_title('ReLU: max(0, x)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('x')\n",
    "    axes[1, 0].set_ylabel('ReLU(x)')\n",
    "    axes[1, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[1, 0].text(0.5, 0.6, 'üìä Sortie: [0, ‚àû)\\n‚úÖ Simple et efficace\\n‚úÖ Pas de gradient qui dispara√Æt\\n‚≠ê Le plus utilis√©!', \n",
    "                   transform=axes[1, 0].transAxes, fontsize=10,\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "    \n",
    "    # Leaky ReLU\n",
    "    axes[1, 1].plot(x, leaky_relu, linewidth=3, color='#f39c12')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_title('Leaky ReLU: max(0.1x, x)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].set_ylabel('Leaky ReLU(x)')\n",
    "    axes[1, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[1, 1].text(0.5, 0.6, 'üìä Sortie: (-‚àû, ‚àû)\\n‚úÖ R√©sout \"dying ReLU\"\\n‚úÖ Petit gradient n√©gatif', \n",
    "                   transform=axes[1, 1].transAxes, fontsize=10,\n",
    "                   bbox=dict(boxstyle='round', facecolor='moccasin', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_activation_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Pourquoi les fonctions d'activation sont importantes ?\n",
    "\n",
    "**Sans fonction d'activation**, m√™me avec des millions de neurones :\n",
    "```\n",
    "y = w‚ÇÅ(w‚ÇÇ(w‚ÇÉx + b‚ÇÉ) + b‚ÇÇ) + b‚ÇÅ\n",
    "```\n",
    "Se simplifie toujours en :\n",
    "```\n",
    "y = Wx + B  (une simple ligne droite !)\n",
    "```\n",
    "\n",
    "**Avec fonction d'activation**, le r√©seau peut apprendre des patterns complexes :\n",
    "- Reconna√Ætre des visages\n",
    "- Comprendre du texte\n",
    "- Jouer aux √©checs\n",
    "- Et bien plus !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Un R√©seau de Neurones Complet\n",
    "\n",
    "Un neurone seul, c'est bien, mais un **r√©seau** de neurones, c'est puissant ! üí™\n",
    "\n",
    "### Architecture typique :\n",
    "\n",
    "```\n",
    "Couche d'entr√©e  ‚Üí  Couches cach√©es  ‚Üí  Couche de sortie\n",
    "     (Input)          (Hidden)            (Output)\n",
    "```\n",
    "\n",
    "Visualisons cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network():\n",
    "    \"\"\"\n",
    "    Visualise un r√©seau de neurones complet\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # D√©finir les couches\n",
    "    layers = [\n",
    "        {'name': 'Entr√©e\\n(Input)', 'neurons': 4, 'x': 1, 'color': '#3498db'},\n",
    "        {'name': 'Cach√©e 1\\n(Hidden 1)', 'neurons': 5, 'x': 3.5, 'color': '#2ecc71'},\n",
    "        {'name': 'Cach√©e 2\\n(Hidden 2)', 'neurons': 5, 'x': 6, 'color': '#f39c12'},\n",
    "        {'name': 'Sortie\\n(Output)', 'neurons': 3, 'x': 8.5, 'color': '#e74c3c'}\n",
    "    ]\n",
    "    \n",
    "    # Stocker les positions des neurones\n",
    "    neuron_positions = []\n",
    "    \n",
    "    # Dessiner les couches\n",
    "    for layer_idx, layer in enumerate(layers):\n",
    "        n_neurons = layer['neurons']\n",
    "        x_pos = layer['x']\n",
    "        color = layer['color']\n",
    "        \n",
    "        # Calculer l'espacement vertical\n",
    "        y_start = 5 - (n_neurons - 1) * 0.6\n",
    "        positions = []\n",
    "        \n",
    "        for i in range(n_neurons):\n",
    "            y_pos = y_start + i * 1.2\n",
    "            positions.append((x_pos, y_pos))\n",
    "            \n",
    "            # Dessiner le neurone\n",
    "            circle = plt.Circle((x_pos, y_pos), 0.25, color=color, ec='black', linewidth=2, zorder=3)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        neuron_positions.append(positions)\n",
    "        \n",
    "        # Label de la couche\n",
    "        ax.text(x_pos, 1, layer['name'], ha='center', va='top', fontsize=12, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round', facecolor=color, alpha=0.3, edgecolor='black', linewidth=2))\n",
    "    \n",
    "    # Dessiner les connexions entre couches\n",
    "    for layer_idx in range(len(neuron_positions) - 1):\n",
    "        current_layer = neuron_positions[layer_idx]\n",
    "        next_layer = neuron_positions[layer_idx + 1]\n",
    "        \n",
    "        for x1, y1 in current_layer:\n",
    "            for x2, y2 in next_layer:\n",
    "                # Ligne entre neurones\n",
    "                ax.plot([x1, x2], [y1, y2], 'gray', linewidth=0.5, alpha=0.3, zorder=1)\n",
    "    \n",
    "    # Titre\n",
    "    ax.text(5, 9.5, 'üèóÔ∏è Architecture d\\'un R√©seau de Neurones Multi-Couches', \n",
    "           ha='center', va='top', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Ajouter des annotations\n",
    "    ax.text(5, 0.2, 'üí° Chaque neurone est connect√© √† tous les neurones de la couche suivante',\n",
    "           ha='center', va='bottom', fontsize=11, style='italic',\n",
    "           bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    # Fl√®ches directionnelles\n",
    "    for i in range(len(layers) - 1):\n",
    "        arrow_x = (layers[i]['x'] + layers[i+1]['x']) / 2\n",
    "        ax.annotate('', xy=(arrow_x + 0.3, 8.5), xytext=(arrow_x - 0.3, 8.5),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Comment un R√©seau Apprend ?\n",
    "\n",
    "L'apprentissage d'un r√©seau de neurones se fait en **4 √©tapes** :\n",
    "\n",
    "### üîÑ Le Cycle d'Apprentissage\n",
    "\n",
    "1. **Forward Propagation** (Propagation Avant)\n",
    "   - Les donn√©es passent de l'entr√©e vers la sortie\n",
    "   - Le r√©seau fait une pr√©diction\n",
    "\n",
    "2. **Calcul de l'Erreur** (Loss)\n",
    "   - On compare la pr√©diction avec la vraie r√©ponse\n",
    "   - On calcule √† quel point on s'est tromp√©\n",
    "\n",
    "3. **Backpropagation** (R√©tropropagation)\n",
    "   - On propage l'erreur en arri√®re dans le r√©seau\n",
    "   - On calcule comment ajuster chaque poids\n",
    "\n",
    "4. **Mise √† Jour des Poids**\n",
    "   - On ajuste les poids pour r√©duire l'erreur\n",
    "   - Le r√©seau devient meilleur !\n",
    "\n",
    "Ce cycle se r√©p√®te des milliers de fois jusqu'√† ce que le r√©seau soit performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learning_process():\n",
    "    \"\"\"\n",
    "    Visualise le processus d'apprentissage\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # √âtapes du processus\n",
    "    steps = [\n",
    "        {\n",
    "            'name': '1. Forward\\nPropagation',\n",
    "            'pos': (2, 7),\n",
    "            'color': '#3498db',\n",
    "            'icon': '‚û°Ô∏è',\n",
    "            'desc': 'Donn√©es ‚Üí Pr√©diction'\n",
    "        },\n",
    "        {\n",
    "            'name': '2. Calcul\\nde l\\'Erreur',\n",
    "            'pos': (5, 8.5),\n",
    "            'color': '#f39c12',\n",
    "            'icon': 'üìä',\n",
    "            'desc': 'Pr√©diction vs R√©alit√©'\n",
    "        },\n",
    "        {\n",
    "            'name': '3. Back-\\npropagation',\n",
    "            'pos': (8, 7),\n",
    "            'color': '#e74c3c',\n",
    "            'icon': '‚¨ÖÔ∏è',\n",
    "            'desc': 'Propagation de l\\'erreur'\n",
    "        },\n",
    "        {\n",
    "            'name': '4. Mise √† jour\\ndes Poids',\n",
    "            'pos': (5, 5.5),\n",
    "            'color': '#2ecc71',\n",
    "            'icon': 'üîÑ',\n",
    "            'desc': 'Ajustement des param√®tres'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Dessiner les √©tapes\n",
    "    for i, step in enumerate(steps):\n",
    "        x, y = step['pos']\n",
    "        \n",
    "        # Bo√Æte de l'√©tape\n",
    "        box = FancyBboxPatch((x-0.8, y-0.5), 1.6, 1, \n",
    "                            boxstyle=\"round,pad=0.1\", \n",
    "                            facecolor=step['color'], \n",
    "                            edgecolor='black', \n",
    "                            linewidth=3,\n",
    "                            alpha=0.7)\n",
    "        ax.add_patch(box)\n",
    "        \n",
    "        # Texte\n",
    "        ax.text(x, y+0.15, step['icon'], ha='center', va='center', fontsize=30)\n",
    "        ax.text(x, y-0.25, step['name'], ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "        ax.text(x, y-0.8, step['desc'], ha='center', va='top', fontsize=9, style='italic')\n",
    "    \n",
    "    # Fl√®ches entre les √©tapes\n",
    "    arrow_pairs = [\n",
    "        (0, 1),  # 1 -> 2\n",
    "        (1, 2),  # 2 -> 3\n",
    "        (2, 3),  # 3 -> 4\n",
    "        (3, 0),  # 4 -> 1 (cycle)\n",
    "    ]\n",
    "    \n",
    "    for i, j in arrow_pairs:\n",
    "        x1, y1 = steps[i]['pos']\n",
    "        x2, y2 = steps[j]['pos']\n",
    "        \n",
    "        # Ajuster les positions de d√©but et fin\n",
    "        if i == 3 and j == 0:  # Fl√®che de retour\n",
    "            arrow = FancyArrowPatch(\n",
    "                (x1-0.8, y1), (x2-0.8, y2-0.5),\n",
    "                arrowstyle='->', mutation_scale=30, linewidth=3,\n",
    "                color='purple', linestyle='dashed', zorder=2\n",
    "            )\n",
    "            ax.text(1.5, 6, 'R√©p√©ter\\njusqu\\'√†\\nconvergence', ha='center', va='center', \n",
    "                   fontsize=10, fontweight='bold', color='purple')\n",
    "        else:\n",
    "            arrow = FancyArrowPatch(\n",
    "                (x1, y1), (x2, y2),\n",
    "                arrowstyle='->', mutation_scale=30, linewidth=3,\n",
    "                color='black', zorder=2\n",
    "            )\n",
    "        ax.add_patch(arrow)\n",
    "    \n",
    "    # Titre\n",
    "    ax.text(5, 9.7, 'üéì Le Cycle d\\'Apprentissage d\\'un R√©seau de Neurones', \n",
    "           ha='center', va='top', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Zone d'information\n",
    "    info_box = FancyBboxPatch((1, 1.5), 8, 2.5,\n",
    "                             boxstyle=\"round,pad=0.15\",\n",
    "                             facecolor='lightyellow',\n",
    "                             edgecolor='black',\n",
    "                             linewidth=2,\n",
    "                             alpha=0.8)\n",
    "    ax.add_patch(info_box)\n",
    "    \n",
    "    info_text = (\n",
    "        \"üìö Ce qu'il faut retenir:\\n\\n\"\n",
    "        \"‚Ä¢ Forward: Le r√©seau fait une pr√©diction\\n\"\n",
    "        \"‚Ä¢ Loss: On mesure l'erreur commise\\n\"\n",
    "        \"‚Ä¢ Backprop: On calcule comment corriger les poids\\n\"\n",
    "        \"‚Ä¢ Update: On am√©liore le r√©seau\\n\"\n",
    "        \"‚Ä¢ On r√©p√®te ce cycle des milliers de fois !\"\n",
    "    )\n",
    "    ax.text(5, 3, info_text, ha='center', va='center', fontsize=11, family='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_learning_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Exemple Concret : Apprendre la Fonction XOR\n",
    "\n",
    "Le XOR (OU exclusif) est un probl√®me classique en machine learning.\n",
    "\n",
    "### Probl√®me :\n",
    "```\n",
    "Entr√©e 1  |  Entr√©e 2  |  Sortie\n",
    "   0      |     0      |    0\n",
    "   0      |     1      |    1\n",
    "   1      |     0      |    1\n",
    "   1      |     1      |    0\n",
    "```\n",
    "\n",
    "Ce probl√®me n'est **pas lin√©airement s√©parable** - il faut un r√©seau de neurones pour le r√©soudre !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset XOR\n",
    "X_xor = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "\n",
    "y_xor = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]])\n",
    "\n",
    "print(\"Dataset XOR:\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"Entr√©es: {X_xor[i]} ‚Üí Sortie: {y_xor[i][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_xor_problem():\n",
    "    \"\"\"\n",
    "    Visualise le probl√®me XOR\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Graphique 1: Points XOR\n",
    "    colors = ['red' if y[0] == 0 else 'blue' for y in y_xor]\n",
    "    ax1.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=500, edgecolors='black', linewidth=3, alpha=0.7)\n",
    "    \n",
    "    for i, (x, y) in enumerate(X_xor):\n",
    "        label = f'({x[0]},{x[1]})\\n‚Üí{y_xor[i][0]}'\n",
    "        ax1.annotate(label, (x[0], x[1]), fontsize=12, ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlim(-0.5, 1.5)\n",
    "    ax1.set_ylim(-0.5, 1.5)\n",
    "    ax1.set_xlabel('Entr√©e 1', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Entr√©e 2', fontsize=14, fontweight='bold')\n",
    "    ax1.set_title('üéØ Le Probl√®me XOR', fontsize=16, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks([0, 1])\n",
    "    ax1.set_yticks([0, 1])\n",
    "    \n",
    "    # L√©gende\n",
    "    red_patch = mpatches.Patch(color='red', label='Sortie = 0')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Sortie = 1')\n",
    "    ax1.legend(handles=[red_patch, blue_patch], fontsize=12, loc='upper right')\n",
    "    \n",
    "    # Texte explicatif\n",
    "    ax1.text(0.5, -0.35, '‚ùå Impossible de s√©parer avec une seule ligne droite !', \n",
    "            ha='center', fontsize=11, fontweight='bold', color='red',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    # Graphique 2: R√©seau pour XOR\n",
    "    ax2.axis('off')\n",
    "    ax2.set_xlim(0, 10)\n",
    "    ax2.set_ylim(0, 10)\n",
    "    ax2.set_title('üß† R√©seau de Neurones pour XOR', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Dessiner le r√©seau\n",
    "    # Couche d'entr√©e\n",
    "    input_neurons = [(2, 6), (2, 4)]\n",
    "    for i, (x, y) in enumerate(input_neurons):\n",
    "        circle = plt.Circle((x, y), 0.3, color='#3498db', ec='black', linewidth=2)\n",
    "        ax2.add_patch(circle)\n",
    "        ax2.text(0.5, y, f'x{i+1}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Couche cach√©e\n",
    "    hidden_neurons = [(5, 7), (5, 5), (5, 3)]\n",
    "    for x, y in hidden_neurons:\n",
    "        circle = plt.Circle((x, y), 0.3, color='#2ecc71', ec='black', linewidth=2)\n",
    "        ax2.add_patch(circle)\n",
    "    \n",
    "    # Couche de sortie\n",
    "    output_neuron = (8, 5)\n",
    "    circle = plt.Circle(output_neuron, 0.3, color='#e74c3c', ec='black', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(9.5, 5, 'y', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Connexions\n",
    "    for ix, iy in input_neurons:\n",
    "        for hx, hy in hidden_neurons:\n",
    "            ax2.plot([ix+0.3, hx-0.3], [iy, hy], 'gray', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    for hx, hy in hidden_neurons:\n",
    "        ax2.plot([hx+0.3, output_neuron[0]-0.3], [hy, output_neuron[1]], 'gray', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Labels\n",
    "    ax2.text(2, 8, 'Entr√©e', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax2.text(5, 8.5, 'Cach√©e', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax2.text(8, 6.5, 'Sortie', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Info\n",
    "    info = \"‚úÖ Avec une couche cach√©e,\\nle r√©seau peut apprendre XOR !\"\n",
    "    ax2.text(5, 1, info, ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_xor_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ R√©capitulatif\n",
    "\n",
    "Dans ce notebook, nous avons appris :\n",
    "\n",
    "### ‚úÖ Concepts Cl√©s\n",
    "\n",
    "1. **Un neurone artificiel** :\n",
    "   - Re√ßoit des entr√©es\n",
    "   - Les multiplie par des poids\n",
    "   - Ajoute un biais\n",
    "   - Applique une fonction d'activation\n",
    "\n",
    "2. **Les fonctions d'activation** :\n",
    "   - Sigmoid: sortie [0, 1]\n",
    "   - Tanh: sortie [-1, 1]\n",
    "   - ReLU: sortie [0, ‚àû) ‚≠ê (la plus utilis√©e)\n",
    "   - Leaky ReLU: am√©lioration de ReLU\n",
    "\n",
    "3. **Architecture d'un r√©seau** :\n",
    "   - Couche d'entr√©e (input)\n",
    "   - Couches cach√©es (hidden)\n",
    "   - Couche de sortie (output)\n",
    "\n",
    "4. **Processus d'apprentissage** :\n",
    "   - Forward propagation\n",
    "   - Calcul de l'erreur\n",
    "   - Backpropagation\n",
    "   - Mise √† jour des poids\n",
    "\n",
    "### üìö Prochaines √âtapes\n",
    "\n",
    "Dans les prochains notebooks, nous allons :\n",
    "\n",
    "1. **Explorer le dataset MNIST** - Des vraies images de chiffres manuscrits\n",
    "2. **Impl√©menter la forward propagation** - En d√©tail avec du code\n",
    "3. **Comprendre la backpropagation** - Les math√©matiques de l'apprentissage\n",
    "4. **Construire notre premier r√©seau** - Reconnaissance de chiffres !\n",
    "5. **Optimiser et am√©liorer** - Techniques avanc√©es\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Pr√™t √† continuer ?\n",
    "\n",
    "Passe au notebook suivant : **`01_exploration_mnist.ipynb`** pour d√©couvrir le dataset MNIST !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Pour aller plus loin\n",
    "\n",
    "### Ressources recommand√©es :\n",
    "\n",
    "- üì∫ [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) - Excellentes visualisations\n",
    "- üìñ [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - Livre gratuit en ligne\n",
    "- üéì [Andrew Ng - Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) - Cours complet\n",
    "\n",
    "### Exp√©rimentations sugg√©r√©es :\n",
    "\n",
    "1. Modifie les poids du neurone simple et observe les changements\n",
    "2. Change les fonctions d'activation et vois l'impact sur les sorties\n",
    "3. Essaie de r√©soudre XOR avec diff√©rentes architectures de r√©seau\n",
    "\n",
    "---\n",
    "\n",
    "**Bonne continuation dans ton apprentissage des r√©seaux de neurones ! üéìüöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
