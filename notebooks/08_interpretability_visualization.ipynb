{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Interpr√©tabilit√© et Visualisation des R√©seaux de Neurones\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "Dans ce notebook, nous allons **ouvrir la bo√Æte noire** des r√©seaux de neurones et comprendre :\n",
    "\n",
    "- üß† **Ce que les neurones apprennent** - Visualisation des poids\n",
    "- üîç **Comment le r√©seau prend ses d√©cisions** - Activation maps\n",
    "- üé® **Quelles parties des images sont importantes** - Saliency maps\n",
    "- üìà **Analyse des erreurs** - O√π et pourquoi le mod√®le se trompe\n",
    "- üî¨ **Statistiques des activations** - Distribution des neurones\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Pourquoi l'Interpr√©tabilit√© est Importante ?\n",
    "\n",
    "### Les R√©seaux de Neurones sont souvent vus comme des **bo√Ætes noires**:\n",
    "\n",
    "```\n",
    "Input (image) ‚Üí [??? MAGIE ???] ‚Üí Output (pr√©diction)\n",
    "```\n",
    "\n",
    "### Probl√®mes de cette approche :\n",
    "\n",
    "1. ‚ùå **Pas de confiance** : Comment faire confiance √† un mod√®le qu'on ne comprend pas ?\n",
    "2. ‚ùå **Difficile √† d√©boguer** : Pourquoi le mod√®le fait-il des erreurs ?\n",
    "3. ‚ùå **Biais cach√©s** : Le mod√®le peut apprendre des patterns incorrects\n",
    "4. ‚ùå **Pas d'insights** : On ne comprend pas ce que les donn√©es nous apprennent\n",
    "\n",
    "### Solutions : Techniques d'Interpr√©tabilit√© ‚úÖ\n",
    "\n",
    "1. **Visualisation des poids** ‚Üí Voir ce que chaque neurone cherche\n",
    "2. **Activation maps** ‚Üí Observer les neurones en action\n",
    "3. **Saliency maps** ‚Üí Identifier les pixels importants\n",
    "4. **Analyse des erreurs** ‚Üí Comprendre les faiblesses\n",
    "5. **t-SNE / PCA** ‚Üí Visualiser l'espace des features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Ajouter le dossier parent au path\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.network import NeuralNetwork\n",
    "from src.utils import load_mnist_data\n",
    "from src import visualize\n",
    "from src.metrics import confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Imports r√©ussis !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Chargement du Mod√®le Entra√Æn√©\n",
    "\n",
    "Commen√ßons par charger un mod√®le d√©j√† entra√Æn√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "print(\"Chargement des donn√©es MNIST...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist_data()\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]:,} exemples\")\n",
    "print(f\"Val: {X_val.shape[0]:,} exemples\")\n",
    "print(f\"Test: {X_test.shape[0]:,} exemples\")\n",
    "\n",
    "# Charger le mod√®le entra√Æn√© (ou en entra√Æner un nouveau)\n",
    "model_path = Path('../models/best_model.pkl')\n",
    "\n",
    "if model_path.exists():\n",
    "    print(f\"\\nüìÇ Chargement du mod√®le: {model_path}\")\n",
    "    model = NeuralNetwork.load(model_path)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Aucun mod√®le trouv√©. Entra√Ænement d'un nouveau mod√®le...\")\n",
    "    model = NeuralNetwork(\n",
    "        layer_dims=[784, 256, 128, 10],\n",
    "        learning_rate=0.01,\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    model.train(X_train, y_train, X_val, y_val, epochs=10, batch_size=128)\n",
    "    model.save(model_path)\n",
    "\n",
    "# √âvaluer\n",
    "test_acc = model.accuracy(X_test, y_test)\n",
    "print(f\"\\nüéØ Accuracy sur le test: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Visualisation des Poids de la Premi√®re Couche\n",
    "\n",
    "### üß† Que font les neurones de la premi√®re couche ?\n",
    "\n",
    "Chaque neurone de la premi√®re couche est connect√© √† **tous les pixels** de l'image (784 connexions).\n",
    "\n",
    "En visualisant ces poids, on peut voir **ce que chaque neurone cherche** dans l'image.\n",
    "\n",
    "### Interpr√©tation :\n",
    "\n",
    "- **Pixels blancs (positifs)** : Le neurone s'active quand ces pixels sont pr√©sents\n",
    "- **Pixels noirs (n√©gatifs)** : Le neurone s'active quand ces pixels sont absents\n",
    "- **Pixels gris** : Pas d'importance particuli√®re\n",
    "\n",
    "### Patterns attendus :\n",
    "\n",
    "Les neurones apprennent souvent √† d√©tecter :\n",
    "- Bords horizontaux/verticaux\n",
    "- Coins\n",
    "- Courbes\n",
    "- Formes g√©om√©triques basiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les poids de la premi√®re couche\n",
    "W1 = model.parameters['W1']  # Shape: (784, 256)\n",
    "\n",
    "print(f\"Poids W1 shape: {W1.shape}\")\n",
    "print(f\"Chaque neurone a {W1.shape[0]} poids (un par pixel)\")\n",
    "print(f\"Il y a {W1.shape[1]} neurones dans la premi√®re couche\\n\")\n",
    "\n",
    "# Visualiser 64 neurones\n",
    "visualize.plot_weights_visualization(\n",
    "    W1, \n",
    "    n_neurons=64,\n",
    "    figsize=(14, 14),\n",
    "    save_path='../models/weights_layer1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Analyse des Poids\n",
    "\n",
    "**Question** : Que voyez-vous ?\n",
    "\n",
    "Observez attentivement :\n",
    "\n",
    "1. **Certains neurones d√©tectent des bords** (horizontaux, verticaux, diagonaux)\n",
    "2. **D'autres d√©tectent des formes** (courbes, coins)\n",
    "3. **Certains sont sp√©cialis√©s** pour des zones pr√©cises de l'image\n",
    "4. **Quelques-uns semblent bruyants** (pas de pattern clair) - c'est normal !\n",
    "\n",
    "C'est fascinant : **le r√©seau d√©couvre automatiquement** ces features √† partir des donn√©es ! ü§Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les statistiques des poids\n",
    "print(\"üìä Statistiques des Poids W1:\\n\")\n",
    "print(f\"Mean: {W1.mean():.6f}\")\n",
    "print(f\"Std:  {W1.std():.6f}\")\n",
    "print(f\"Min:  {W1.min():.6f}\")\n",
    "print(f\"Max:  {W1.max():.6f}\")\n",
    "\n",
    "# Distribution des poids\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Histogramme\n",
    "ax1.hist(W1.flatten(), bins=100, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='Z√©ro')\n",
    "ax1.set_xlabel('Valeur du poids', fontsize=12)\n",
    "ax1.set_ylabel('Fr√©quence', fontsize=12)\n",
    "ax1.set_title('Distribution des Poids (Couche 1)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Heatmap de corr√©lation entre neurones\n",
    "sample_neurons = W1[:, :50].T  # Prendre 50 neurones\n",
    "correlation = np.corrcoef(sample_neurons)\n",
    "sns.heatmap(correlation, cmap='coolwarm', center=0, square=True, ax=ax2,\n",
    "            cbar_kws={'label': 'Corr√©lation'})\n",
    "ax2.set_title('Corr√©lation entre Neurones (50 premiers)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation:\")\n",
    "print(\"- Distribution centr√©e sur 0 ‚Üí Bonne initialisation He\")\n",
    "print(\"- Faible corr√©lation entre neurones ‚Üí Chaque neurone apprend des features diff√©rentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Visualisation des Activations (Neurones en Action)\n",
    "\n",
    "### üî• Activation Maps\n",
    "\n",
    "Maintenant, voyons comment les neurones **r√©agissent** √† de vraies images.\n",
    "\n",
    "Pour une image donn√©e :\n",
    "- Certains neurones s'activent fortement (valeurs √©lev√©es)\n",
    "- D'autres restent silencieux (valeurs proches de 0)\n",
    "\n",
    "Cela nous montre **quels neurones sont activ√©s** pour chaque type de chiffre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les activations interm√©diaires\n",
    "def get_layer_activations(model, X):\n",
    "    \"\"\"\n",
    "    R√©cup√®re les activations de toutes les couches\n",
    "    \"\"\"\n",
    "    _, cache = model.forward(X)\n",
    "    \n",
    "    activations = {}\n",
    "    for l in range(1, len(model.layer_dims)):\n",
    "        activations[f'Layer {l}'] = cache[f'A{l}']\n",
    "    \n",
    "    return activations\n",
    "\n",
    "# Prendre quelques exemples\n",
    "n_samples = 5\n",
    "sample_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "X_samples = X_test[sample_indices]\n",
    "y_samples = y_test[sample_indices]\n",
    "\n",
    "# Obtenir les activations\n",
    "activations = get_layer_activations(model, X_samples)\n",
    "\n",
    "print(\"Activations r√©cup√©r√©es:\")\n",
    "for layer_name, acts in activations.items():\n",
    "    print(f\"  {layer_name}: shape {acts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les activations pour chaque couche\n",
    "layer_names = [f'Layer {i}' for i in range(1, len(model.layer_dims)-1)]  # Exclure la sortie\n",
    "\n",
    "visualize.plot_activation_outputs(\n",
    "    activations,\n",
    "    layer_names,\n",
    "    n_samples=3,\n",
    "    n_neurons=32,\n",
    "    figsize=(15, 8),\n",
    "    save_path='../models/activations.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser la sparsit√© des activations (apr√®s ReLU)\n",
    "print(\"\\nüìä Analyse de la Sparsit√© (% de neurones actifs):\\n\")\n",
    "\n",
    "for layer_name, acts in activations.items():\n",
    "    if 'Layer' in layer_name and layer_name != f'Layer {len(model.layer_dims)-1}':\n",
    "        # Compter les neurones actifs (> 0)\n",
    "        active_neurons = (acts > 0).sum() / acts.size * 100\n",
    "        print(f\"{layer_name}: {active_neurons:.1f}% neurones actifs\")\n",
    "\n",
    "print(\"\\nüí° Une sparsit√© de ~50% est id√©ale (gr√¢ce √† ReLU)\")\n",
    "print(\"   - Trop peu (<20%) : Les neurones ne s'activent pas assez\")\n",
    "print(\"   - Trop (>80%) : Risque de surapprentissage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Saliency Maps : Pixels Importants\n",
    "\n",
    "### ‚ùì Question Cl√© :\n",
    "\n",
    "**Quels pixels de l'image ont le plus d'influence sur la d√©cision du r√©seau ?**\n",
    "\n",
    "### üéØ Saliency Map (Carte de Saillance)\n",
    "\n",
    "Une **saliency map** montre l'importance de chaque pixel pour la pr√©diction.\n",
    "\n",
    "**M√©thode** : Calculer le gradient de la sortie par rapport √† l'input\n",
    "\n",
    "```\n",
    "Saliency = |‚àÇoutput / ‚àÇinput|\n",
    "```\n",
    "\n",
    "- **Valeurs √©lev√©es** ‚Üí Le pixel est important\n",
    "- **Valeurs faibles** ‚Üí Le pixel n'a pas d'impact\n",
    "\n",
    "### Utilit√© :\n",
    "\n",
    "‚úÖ V√©rifier que le r√©seau se concentre sur les **bonnes parties** de l'image  \n",
    "‚úÖ D√©tecter si le r√©seau apprend des **biais** (ex: se concentrer sur le fond plut√¥t que le chiffre)  \n",
    "‚úÖ Comprendre les **erreurs** du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(model, x, target_class):\n",
    "    \"\"\"\n",
    "    Calcule la saliency map pour une image\n",
    "    \n",
    "    Args:\n",
    "        model: r√©seau de neurones\n",
    "        x: image (784,)\n",
    "        target_class: classe cible\n",
    "    \n",
    "    Returns:\n",
    "        saliency: gradient de l'output par rapport √† l'input (784,)\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    x_batch = x.reshape(1, -1)\n",
    "    output, cache = model.forward(x_batch)\n",
    "    \n",
    "    # Cr√©er un gradient pour la classe cible\n",
    "    grad_output = np.zeros_like(output)\n",
    "    grad_output[0, target_class] = 1.0\n",
    "    \n",
    "    # Backprop simple (sans changer les poids)\n",
    "    # Gradient de la couche de sortie\n",
    "    dZ = grad_output\n",
    "    \n",
    "    # Backprop √† travers les couches\n",
    "    L = len(model.layer_dims) - 1\n",
    "    \n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        dA_prev = np.dot(dZ, model.parameters[f'W{l}'].T)\n",
    "        \n",
    "        # Si pas la premi√®re couche, appliquer d√©riv√©e ReLU\n",
    "        if l > 1:\n",
    "            dZ = dA_prev * (cache[f'Z{l-1}'] > 0)\n",
    "        else:\n",
    "            # Gradient par rapport √† l'input\n",
    "            saliency = dA_prev\n",
    "    \n",
    "    # Valeur absolue (on s'int√©resse √† la magnitude)\n",
    "    saliency = np.abs(saliency).reshape(784)\n",
    "    \n",
    "    return saliency\n",
    "\n",
    "print(\"‚úì Fonction saliency_map d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer et visualiser les saliency maps\n",
    "n_examples = 10\n",
    "sample_indices = np.random.choice(len(X_test), n_examples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, n_examples, figsize=(16, 6))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    x = X_test[idx]\n",
    "    y_true = y_test[idx]\n",
    "    \n",
    "    # Pr√©diction\n",
    "    pred = model.predict(x.reshape(1, -1))[0]\n",
    "    \n",
    "    # Saliency map\n",
    "    saliency = compute_saliency_map(model, x, pred)\n",
    "    \n",
    "    # Image originale\n",
    "    axes[0, i].imshow(x.reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(f'Vrai: {y_true}\\nPred: {pred}', fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Saliency map\n",
    "    axes[1, i].imshow(saliency.reshape(28, 28), cmap='hot')\n",
    "    axes[1, i].set_title('Saliency', fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Superposition\n",
    "    axes[2, i].imshow(x.reshape(28, 28), cmap='gray', alpha=0.7)\n",
    "    axes[2, i].imshow(saliency.reshape(28, 28), cmap='hot', alpha=0.3)\n",
    "    axes[2, i].set_title('Overlay', fontsize=9)\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Image', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Saliency Map', fontsize=12, fontweight='bold')\n",
    "axes[2, 0].set_ylabel('Superposition', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('üîç Saliency Maps - Pixels Importants pour la D√©cision', \n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/saliency_maps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation:\")\n",
    "print(\"Les zones rouges/jaunes sont les pixels importants pour la d√©cision.\")\n",
    "print(\"Le r√©seau se concentre-t-il sur le chiffre ou sur le fond ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Analyse des Erreurs\n",
    "\n",
    "### üêõ Pourquoi le Mod√®le se Trompe-t-il ?\n",
    "\n",
    "Analyser les erreurs est **crucial** pour am√©liorer le mod√®le.\n",
    "\n",
    "**Questions importantes** :\n",
    "\n",
    "1. Quels chiffres sont les plus confondus ?\n",
    "2. Les erreurs sont-elles compr√©hensibles (m√™me pour un humain) ?\n",
    "3. Y a-t-il des patterns dans les erreurs ?\n",
    "4. Le mod√®le est-il confiant quand il se trompe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire des pr√©dictions sur le test set\n",
    "print(\"Calcul des pr√©dictions sur le test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_probs, _ = model.forward(X_test)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred, num_classes=10)\n",
    "\n",
    "print(\"\\nüìä Matrice de Confusion:\\n\")\n",
    "visualize.plot_confusion_matrix(\n",
    "    cm,\n",
    "    class_names=[str(i) for i in range(10)],\n",
    "    figsize=(12, 5),\n",
    "    save_path='../models/confusion_matrix_analysis.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les paires les plus confondues\n",
    "print(\"\\nüîç Top 10 Confusions:\\n\")\n",
    "\n",
    "confusions = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusions.append((i, j, cm[i, j]))\n",
    "\n",
    "# Trier par nombre de confusions\n",
    "confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(f\"{'Vrai':<6} {'Pr√©dit':<8} {'Erreurs':<10} {'% du vrai'}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for true, pred, count in confusions[:10]:\n",
    "    total_true = cm[true, :].sum()\n",
    "    percentage = (count / total_true) * 100\n",
    "    print(f\"{true:<6} {pred:<8} {count:<10} {percentage:.2f}%\")\n",
    "\n",
    "print(\"\\nüí° Ces confusions sont-elles logiques ?\")\n",
    "print(\"   Ex: 3/5, 4/9, 7/1 sont visuellement similaires\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les exemples mal classifi√©s (avec haute confiance)\n",
    "print(\"\\nüö® Exemples Mal Classifi√©s (avec haute confiance):\\n\")\n",
    "\n",
    "visualize.plot_misclassified_examples(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    y_probs,\n",
    "    n_samples=20,\n",
    "    figsize=(14, 12),\n",
    "    save_path='../models/misclassified_confident.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser la distribution de confiance\n",
    "errors_indices = np.where(y_test != y_pred)[0]\n",
    "correct_indices = np.where(y_test == y_pred)[0]\n",
    "\n",
    "# Confiance sur les pr√©dictions\n",
    "confidence_errors = np.max(y_probs[errors_indices], axis=1)\n",
    "confidence_correct = np.max(y_probs[correct_indices], axis=1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogrammes de confiance\n",
    "ax1.hist(confidence_correct, bins=50, alpha=0.7, color='green', \n",
    "         label=f'Correctes ({len(correct_indices)})', edgecolor='black')\n",
    "ax1.hist(confidence_errors, bins=50, alpha=0.7, color='red', \n",
    "         label=f'Erreurs ({len(errors_indices)})', edgecolor='black')\n",
    "ax1.set_xlabel('Confiance (max probabilit√©)', fontsize=12)\n",
    "ax1.set_ylabel('Fr√©quence', fontsize=12)\n",
    "ax1.set_title('Distribution de Confiance', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [confidence_correct, confidence_errors]\n",
    "ax2.boxplot(data_to_plot, labels=['Correctes', 'Erreurs'], \n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue', color='black'),\n",
    "            medianprops=dict(color='red', linewidth=2))\n",
    "ax2.set_ylabel('Confiance', fontsize=12)\n",
    "ax2.set_title('Comparaison de Confiance', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Statistiques de Confiance:\\n\")\n",
    "print(f\"Pr√©dictions Correctes:\")\n",
    "print(f\"  Mean: {confidence_correct.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(confidence_correct):.4f}\")\n",
    "print(f\"\\nPr√©dictions Incorrectes:\")\n",
    "print(f\"  Mean: {confidence_errors.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(confidence_errors):.4f}\")\n",
    "\n",
    "# Erreurs avec haute confiance (> 0.9)\n",
    "high_conf_errors = np.sum(confidence_errors > 0.9)\n",
    "print(f\"\\n‚ö†Ô∏è Erreurs avec confiance > 90%: {high_conf_errors} ({high_conf_errors/len(errors_indices)*100:.1f}%)\")\n",
    "print(\"   ‚Üí Le mod√®le est tr√®s s√ªr de lui m√™me quand il se trompe !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Visualisation de l'Espace des Features (t-SNE)\n",
    "\n",
    "### üó∫Ô∏è t-SNE : R√©duction de Dimensionnalit√©\n",
    "\n",
    "**t-SNE** (t-Distributed Stochastic Neighbor Embedding) projette les donn√©es haute dimension en 2D.\n",
    "\n",
    "### Pourquoi c'est utile ?\n",
    "\n",
    "- Les features apprises par le r√©seau sont en haute dimension (ex: 128D)\n",
    "- Impossible de les visualiser directement\n",
    "- t-SNE les projette en 2D tout en **pr√©servant les similarit√©s**\n",
    "\n",
    "### Interpr√©tation :\n",
    "\n",
    "‚úÖ **Clusters bien s√©par√©s** ‚Üí Le r√©seau a bien appris √† s√©parer les classes  \n",
    "‚ùå **Clusters m√©lang√©s** ‚Üí Le r√©seau confond certaines classes  \n",
    "üîç **Points isol√©s** ‚Üí Exemples difficiles ou outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer sklearn si n√©cessaire\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "except ImportError:\n",
    "    print(\"Installation de scikit-learn...\")\n",
    "    !pip install scikit-learn\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"‚úì t-SNE disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les features de l'avant-derni√®re couche\n",
    "# (c'est l√† que les repr√©sentations sont les plus riches)\n",
    "n_samples_tsne = 1000  # Limiter pour la vitesse\n",
    "sample_indices = np.random.choice(len(X_test), n_samples_tsne, replace=False)\n",
    "X_sample = X_test[sample_indices]\n",
    "y_sample = y_test[sample_indices]\n",
    "\n",
    "# Forward pass pour obtenir les features\n",
    "_, cache = model.forward(X_sample)\n",
    "features = cache[f'A{len(model.layer_dims)-2}']  # Avant-derni√®re couche\n",
    "\n",
    "print(f\"Features extraites: {features.shape}\")\n",
    "print(f\"Dimension originale: {features.shape[1]}D\")\n",
    "print(f\"\\nCalcul du t-SNE (peut prendre 1-2 minutes)...\")\n",
    "\n",
    "# Appliquer t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "features_2d = tsne.fit_transform(features)\n",
    "\n",
    "print(\"‚úì t-SNE termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser le t-SNE\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Couleurs pour chaque classe\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for digit in range(10):\n",
    "    mask = y_sample == digit\n",
    "    ax.scatter(features_2d[mask, 0], features_2d[mask, 1],\n",
    "              c=[colors[digit]], label=str(digit),\n",
    "              alpha=0.6, edgecolors='black', linewidth=0.5, s=50)\n",
    "\n",
    "ax.set_xlabel('t-SNE Dimension 1', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('t-SNE Dimension 2', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Visualisation t-SNE des Features Apprises', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Chiffre', loc='best', frameon=True, shadow=True)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/tsne_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation:\")\n",
    "print(\"- Clusters bien s√©par√©s ‚Üí Le r√©seau distingue bien les classes\")\n",
    "print(\"- Chevauchements ‚Üí Chiffres visuellement similaires (ex: 3/5, 4/9)\")\n",
    "print(\"- Points isol√©s ‚Üí Exemples atypiques ou mal √©crits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì R√©sum√© et Conclusions\n",
    "\n",
    "### üîë Points Cl√©s de l'Interpr√©tabilit√©\n",
    "\n",
    "1. **Visualisation des Poids** üß†\n",
    "   - Montre ce que chaque neurone cherche dans l'image\n",
    "   - R√©v√®le des d√©tecteurs de bords, coins, courbes\n",
    "   - Permet de v√©rifier que le r√©seau apprend des features sens√©es\n",
    "\n",
    "2. **Activation Maps** üî•\n",
    "   - Montre quels neurones s'activent pour chaque image\n",
    "   - Analyse de la sparsit√© (id√©alement ~50% avec ReLU)\n",
    "   - Permet de d√©tecter les neurones \"morts\" ou trop actifs\n",
    "\n",
    "3. **Saliency Maps** üéØ\n",
    "   - Identifie les pixels importants pour la d√©cision\n",
    "   - V√©rifie que le r√©seau se concentre sur les bonnes zones\n",
    "   - D√©tecte les biais potentiels\n",
    "\n",
    "4. **Analyse des Erreurs** üêõ\n",
    "   - Comprendre o√π et pourquoi le mod√®le se trompe\n",
    "   - Identifier les confusions logiques (3/5, 4/9, etc.)\n",
    "   - Analyser la confiance (le mod√®le sait-il quand il ne sait pas ?)\n",
    "\n",
    "5. **Visualisation t-SNE** üó∫Ô∏è\n",
    "   - Projette les features en 2D\n",
    "   - Montre la s√©paration des classes dans l'espace appris\n",
    "   - R√©v√®le les exemples difficiles et outliers\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Applications Pratiques\n",
    "\n",
    "Ces techniques d'interpr√©tabilit√© sont **essentielles** pour :\n",
    "\n",
    "‚úÖ **Debugging** : Trouver pourquoi le mod√®le ne performe pas  \n",
    "‚úÖ **Confiance** : Comprendre et faire confiance aux pr√©dictions  \n",
    "‚úÖ **Am√©lioration** : Identifier les faiblesses et les corriger  \n",
    "‚úÖ **Transparence** : Expliquer les d√©cisions (crucial en m√©decine, finance, etc.)  \n",
    "‚úÖ **D√©tection de biais** : S'assurer que le mod√®le est √©quitable\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Pour Aller Plus Loin\n",
    "\n",
    "**Techniques avanc√©es d'interpr√©tabilit√©** :\n",
    "\n",
    "1. **Grad-CAM** : Version am√©lior√©e des saliency maps pour les CNN\n",
    "2. **LIME** : Explication locale avec mod√®les lin√©aires\n",
    "3. **SHAP** : Valeurs de Shapley pour l'importance des features\n",
    "4. **Adversarial Examples** : Trouver les faiblesses du mod√®le\n",
    "5. **Feature Visualization** : G√©n√©rer des images qui maximisent l'activation\n",
    "\n",
    "**Biblioth√®ques utiles** :\n",
    "- `captum` (PyTorch)\n",
    "- `tf-explain` (TensorFlow)\n",
    "- `lime`\n",
    "- `shap`\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Exercices\n",
    "\n",
    "1. **Modifier l'architecture** et observer l'impact sur les poids visualis√©s\n",
    "2. **Comparer les saliency maps** entre pr√©dictions correctes et incorrectes\n",
    "3. **Identifier les neurones morts** (jamais activ√©s) dans votre r√©seau\n",
    "4. **Analyser les erreurs** : Y a-t-il des patterns sp√©cifiques ?\n",
    "5. **t-SNE sur diff√©rentes couches** : Comment l'espace des features √©volue ?\n",
    "\n",
    "---\n",
    "\n",
    "**F√©licitations ! üéâ**\n",
    "\n",
    "Vous savez maintenant **ouvrir la bo√Æte noire** et comprendre ce qui se passe √† l'int√©rieur de vos r√©seaux de neurones !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
