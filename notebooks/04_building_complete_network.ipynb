{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Construire Votre Premier R√©seau Complet\n",
    "\n",
    "Maintenant qu'on comprend tous les concepts, construisons un r√©seau de neurones complet from scratch !\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "1. **Cr√©er une classe NeuralNetwork** modulaire et r√©utilisable\n",
    "2. **Entra√Æner sur MNIST complet** avec toutes les optimisations\n",
    "3. **Suivre les m√©triques** pendant l'entra√Ænement\n",
    "4. **Visualiser les r√©sultats** et analyser les performances\n",
    "5. **Sauvegarder et charger** le mod√®le entra√Æn√©\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Ready to build!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Classe NeuralNetwork Compl√®te\n",
    "\n",
    "Cr√©ons une classe qui encapsule tout ce qu'on a appris !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    R√©seau de neurones multi-couches from scratch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialise le r√©seau\n",
    "        \n",
    "        Args:\n",
    "            layer_dims: liste des dimensions [input, hidden1, hidden2, ..., output]\n",
    "            learning_rate: taux d'apprentissage\n",
    "        \"\"\"\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = self._initialize_parameters()\n",
    "        self.history = {'loss': [], 'train_acc': [], 'val_acc': []}\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialise les poids avec He initialization\"\"\"\n",
    "        np.random.seed(42)\n",
    "        parameters = {}\n",
    "        L = len(self.layer_dims)\n",
    "        \n",
    "        for l in range(1, L):\n",
    "            parameters[f'W{l}'] = np.random.randn(self.layer_dims[l-1], self.layer_dims[l]) * \\\n",
    "                                 np.sqrt(2 / self.layer_dims[l-1])\n",
    "            parameters[f'b{l}'] = np.zeros((1, self.layer_dims[l]))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        \"\"\"ReLU activation\"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax activation\"\"\"\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \"\"\"\n",
    "        W1, b1 = self.parameters['W1'], self.parameters['b1']\n",
    "        W2, b2 = self.parameters['W2'], self.parameters['b2']\n",
    "        \n",
    "        # Layer 1\n",
    "        Z1 = np.dot(X, W1) + b1\n",
    "        A1 = self.relu(Z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = self.softmax(Z2)\n",
    "        \n",
    "        cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2, 'X': X}\n",
    "        return A2, cache\n",
    "    \n",
    "    def compute_loss(self, Y_true, Y_pred):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss\n",
    "        \"\"\"\n",
    "        n_samples = Y_true.shape[0]\n",
    "        epsilon = 1e-7\n",
    "        Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(Y_true * np.log(Y_pred)) / n_samples\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, Y, cache):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \"\"\"\n",
    "        X = cache['X']\n",
    "        A1 = cache['A1']\n",
    "        A2 = cache['A2']\n",
    "        Z1 = cache['Z1']\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        W2 = self.parameters['W2']\n",
    "        \n",
    "        # Layer 2 gradients\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = np.dot(A1.T, dZ2) / n_samples\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / n_samples\n",
    "        \n",
    "        # Layer 1 gradients\n",
    "        dA1 = np.dot(dZ2, W2.T)\n",
    "        dZ1 = dA1 * (Z1 > 0)\n",
    "        dW1 = np.dot(X.T, dZ1) / n_samples\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / n_samples\n",
    "        \n",
    "        gradients = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        \"\"\"\n",
    "        Mise √† jour des poids par descente de gradient\n",
    "        \"\"\"\n",
    "        for key in self.parameters.keys():\n",
    "            self.parameters[key] -= self.learning_rate * gradients['d' + key]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Fait des pr√©dictions\n",
    "        \"\"\"\n",
    "        A2, _ = self.forward(X)\n",
    "        return np.argmax(A2, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcule l'accuracy\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def one_hot_encode(self, y, n_classes=10):\n",
    "        \"\"\"\n",
    "        One-hot encoding\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros((y.shape[0], n_classes))\n",
    "        one_hot[np.arange(y.shape[0]), y] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=128, verbose=True):\n",
    "        \"\"\"\n",
    "        Entra√Æne le r√©seau\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        Y_train = self.one_hot_encode(y_train)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üéì D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\nConfiguration:\")\n",
    "            print(f\"  ‚Ä¢ Architecture: {' ‚Üí '.join(map(str, self.layer_dims))}\")\n",
    "            print(f\"  ‚Ä¢ Learning rate: {self.learning_rate}\")\n",
    "            print(f\"  ‚Ä¢ Batch size: {batch_size}\")\n",
    "            print(f\"  ‚Ä¢ √âpoques: {epochs}\")\n",
    "            print(f\"  ‚Ä¢ Exemples d'entra√Ænement: {n_samples:,}\")\n",
    "            print(f\"  ‚Ä¢ Batches par √©poque: {n_batches}\")\n",
    "            print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time()\n",
    "            \n",
    "            # M√©langer les donn√©es\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            Y_shuffled = Y_train[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                X_batch = X_shuffled[start:end]\n",
    "                Y_batch = Y_shuffled[start:end]\n",
    "                \n",
    "                # Forward\n",
    "                A2, cache = self.forward(X_batch)\n",
    "                \n",
    "                # Loss\n",
    "                loss = self.compute_loss(Y_batch, A2)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "                # Backward\n",
    "                gradients = self.backward(Y_batch, cache)\n",
    "                \n",
    "                # Update\n",
    "                self.update_parameters(gradients)\n",
    "            \n",
    "            # M√©triques\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            train_acc = self.accuracy(X_train, y_train)\n",
    "            val_acc = self.accuracy(X_val, y_val)\n",
    "            \n",
    "            # Sauvegarder l'historique\n",
    "            self.history['loss'].append(avg_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            \n",
    "            epoch_time = time() - epoch_start\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"√âpoque {epoch+1:2d}/{epochs} - \"\n",
    "                      f\"Loss: {avg_loss:.4f} - \"\n",
    "                      f\"Train Acc: {train_acc:.4f} - \"\n",
    "                      f\"Val Acc: {val_acc:.4f} - \"\n",
    "                      f\"Temps: {epoch_time:.2f}s\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"‚úÖ ENTRA√éNEMENT TERMIN√â\")\n",
    "            print(\"=\"*70)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Sauvegarde le mod√®le\n",
    "        \"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'parameters': self.parameters,\n",
    "                'layer_dims': self.layer_dims,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'history': self.history\n",
    "            }, f)\n",
    "        print(f\"\\nüíæ Mod√®le sauvegard√© dans: {filepath}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        \"\"\"\n",
    "        Charge un mod√®le sauvegard√©\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        model = NeuralNetwork(data['layer_dims'], data['learning_rate'])\n",
    "        model.parameters = data['parameters']\n",
    "        model.history = data['history']\n",
    "        print(f\"\\nüìÇ Mod√®le charg√© depuis: {filepath}\")\n",
    "        return model\n",
    "\n",
    "print(\"‚úÖ Classe NeuralNetwork cr√©√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Chargement des Donn√©es MNIST Compl√®tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_mnist_data\n",
    "\n",
    "print(\"‚è≥ Chargement de MNIST...\")\n",
    "X_train, y_train, X_test, y_test = load_mnist_data()\n",
    "\n",
    "print(\"\\n‚úÖ Donn√©es charg√©es!\")\n",
    "print(f\"\\nTrain set: {X_train.shape[0]:,} exemples\")\n",
    "print(f\"Test set:  {X_test.shape[0]:,} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Entra√Ænement du R√©seau\n",
    "\n",
    "C'est le moment de v√©rit√© ! Entra√Ænons notre r√©seau sur MNIST complet !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le r√©seau\n",
    "model = NeuralNetwork(\n",
    "    layer_dims=[784, 128, 10],\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Entra√Æner\n",
    "model.train(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(model):\n",
    "    \"\"\"\n",
    "    Visualise les courbes d'apprentissage\n",
    "    \"\"\"\n",
    "    history = model.history\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('üìä R√©sultats de l\\'Entra√Ænement', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(epochs, history['loss'], 'o-', linewidth=3, markersize=8,\n",
    "            color='#e74c3c', label='Loss')\n",
    "    ax1.set_xlabel('√âpoque', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('üìâ √âvolution de la Loss', fontsize=15, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(fontsize=12)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax2.plot(epochs, [acc*100 for acc in history['train_acc']],\n",
    "            'o-', linewidth=3, markersize=8, color='#2ecc71', label='Train')\n",
    "    ax2.plot(epochs, [acc*100 for acc in history['val_acc']],\n",
    "            's-', linewidth=3, markersize=8, color='#3498db', label='Validation')\n",
    "    ax2.set_xlabel('√âpoque', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('üìà √âvolution de l\\'Accuracy', fontsize=15, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(fontsize=12)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üèÜ R√âSULTATS FINAUX\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìä Loss: {history['loss'][0]:.4f} ‚Üí {history['loss'][-1]:.4f}\")\n",
    "    print(f\"   R√©duction: {(1-history['loss'][-1]/history['loss'][0])*100:.1f}%\")\n",
    "    print(f\"\\nüéØ Train Accuracy: {history['train_acc'][-1]:.2%}\")\n",
    "    print(f\"üéØ Val Accuracy: {history['val_acc'][-1]:.2%}\")\n",
    "    \n",
    "    # Comparaison\n",
    "    baseline = 0.10  # Random guessing\n",
    "    improvement = (history['val_acc'][-1] - baseline) / baseline * 100\n",
    "    print(f\"\\nüí™ Am√©lioration vs random: +{improvement:.0f}%\")\n",
    "\n",
    "plot_training_results(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Affiche la matrice de confusion\n",
    "    \"\"\"\n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Normaliser\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Visualiser\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "               xticklabels=range(10), yticklabels=range(10),\n",
    "               cbar_kws={'label': 'Accuracy'})\n",
    "    plt.xlabel('Pr√©diction', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Vrai Label', fontsize=14, fontweight='bold')\n",
    "    plt.title('üéØ Matrice de Confusion (Normalis√©e)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyse\n",
    "    print(\"\\nüìä Accuracy par classe:\")\n",
    "    for i in range(10):\n",
    "        acc = cm_normalized[i, i]\n",
    "        print(f\"   Chiffre {i}: {acc:.2%}\")\n",
    "    \n",
    "    # Erreurs les plus fr√©quentes\n",
    "    cm_errors = cm_normalized.copy()\n",
    "    np.fill_diagonal(cm_errors, 0)\n",
    "    \n",
    "    print(\"\\n‚ùå Confusions les plus fr√©quentes:\")\n",
    "    for _ in range(5):\n",
    "        i, j = np.unravel_index(cm_errors.argmax(), cm_errors.shape)\n",
    "        if cm_errors[i, j] > 0.01:\n",
    "            print(f\"   {i} confondu avec {j}: {cm_errors[i,j]:.1%}\")\n",
    "            cm_errors[i, j] = 0\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Visualiser des Pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, X, y, n_samples=20):\n",
    "    \"\"\"\n",
    "    Affiche des pr√©dictions du mod√®le\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    preds = model.predict(X[indices])\n",
    "    probs, _ = model.forward(X[indices])\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 5, figsize=(16, 13))\n",
    "    fig.suptitle('üé® Pr√©dictions du Mod√®le Entra√Æn√©', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        image = X[indices[idx]].reshape(28, 28)\n",
    "        true_label = y[indices[idx]]\n",
    "        pred_label = preds[idx]\n",
    "        confidence = probs[idx, pred_label]\n",
    "        \n",
    "        # Afficher l'image\n",
    "        ax.imshow(image, cmap='gray_r')\n",
    "        \n",
    "        # Titre avec couleur\n",
    "        color = 'green' if pred_label == true_label else 'red'\n",
    "        title = f'Vrai: {true_label} | Pred: {pred_label}\\nConf: {confidence:.1%}'\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold', color=color)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Bordure\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(color)\n",
    "            spine.set_linewidth(3)\n",
    "            spine.set_visible(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Stats\n",
    "    correct = np.sum(preds == y[indices])\n",
    "    print(f\"\\n‚úÖ Correct: {correct}/{n_samples} ({correct/n_samples:.1%})\")\n",
    "\n",
    "show_predictions(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Sauvegarder le Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le dossier models s'il n'existe pas\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Sauvegarder\n",
    "model_path = models_dir / 'mnist_network.pkl'\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Mod√®le pr√™t √† √™tre utilis√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ R√©capitulatif\n",
    "\n",
    "**F√©licitations ! Tu as construit et entra√Æn√© ton premier r√©seau de neurones from scratch ! üéâ**\n",
    "\n",
    "### ‚úÖ Ce que nous avons accompli\n",
    "\n",
    "1. **Cr√©√© une classe NeuralNetwork** compl√®te et modulaire\n",
    "2. **Entra√Æn√© sur MNIST** avec ~95%+ d'accuracy\n",
    "3. **Visualis√© les performances** avec des graphiques clairs\n",
    "4. **Analys√© les erreurs** avec la matrice de confusion\n",
    "5. **Sauvegard√© le mod√®le** pour une utilisation future\n",
    "\n",
    "### üèÜ R√©sultats typiques attendus\n",
    "\n",
    "Avec ce r√©seau simple (784 ‚Üí 128 ‚Üí 10):\n",
    "- **Train accuracy**: ~96-98%\n",
    "- **Test accuracy**: ~95-97%\n",
    "- **Temps d'entra√Ænement**: 2-5 minutes sur CPU\n",
    "\n",
    "### üí° Points cl√©s\n",
    "\n",
    "- üèóÔ∏è **Architecture modulaire** : facile √† r√©utiliser et √©tendre\n",
    "- üìä **Suivi des m√©triques** : important pour comprendre l'apprentissage\n",
    "- üéØ **Bonnes performances** : ~95% avec un r√©seau simple!\n",
    "- üíæ **Sauvegarde/Chargement** : pratique pour r√©utiliser le mod√®le\n",
    "\n",
    "### üöÄ Prochaine √âtape\n",
    "\n",
    "Maintenant, comment am√©liorer encore plus les performances ?\n",
    "\n",
    "**‚û°Ô∏è Dernier notebook: `05_improvements_optimization.ipynb`**\n",
    "\n",
    "On va explorer:\n",
    "- Diff√©rentes architectures\n",
    "- Techniques d'optimisation avanc√©es\n",
    "- Data augmentation\n",
    "- R√©gularisation\n",
    "- Et atteindre 98%+ d'accuracy !\n",
    "\n",
    "---\n",
    "\n",
    "**Tu as maintenant un r√©seau de neurones fonctionnel ! Awesome ! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
