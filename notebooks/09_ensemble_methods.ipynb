{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ù M√©thodes d'Ensemble pour les R√©seaux de Neurones\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "Dans ce notebook, nous explorons les **m√©thodes d'ensemble** :\n",
    "\n",
    "- üß† **Principe** : \"Plusieurs t√™tes valent mieux qu'une\"\n",
    "- üé≤ **Diversit√©** : Combiner plusieurs mod√®les diff√©rents\n",
    "- üìà **Performance** : Am√©liorer l'accuracy de 1-3%\n",
    "- üõ°Ô∏è **Robustesse** : R√©duire la variance et le surapprentissage\n",
    "- üéØ **Confiance** : Meilleures estimations de probabilit√©\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Pourquoi les Ensembles Fonctionnent ?\n",
    "\n",
    "### Intuition Simple\n",
    "\n",
    "Imaginez **10 experts** qui votent :\n",
    "\n",
    "```\n",
    "Expert 1: \"C'est un 3\"  ‚Üí  ‚úì Correct\n",
    "Expert 2: \"C'est un 5\"  ‚Üí  ‚úó Erreur\n",
    "Expert 3: \"C'est un 3\"  ‚Üí  ‚úì Correct\n",
    "Expert 4: \"C'est un 3\"  ‚Üí  ‚úì Correct\n",
    "...\n",
    "\n",
    "Vote final: \"C'est un 3\" (7/10) ‚Üí ‚úì‚úì‚úì Plus fiable !\n",
    "```\n",
    "\n",
    "### Conditions de Succ√®s\n",
    "\n",
    "Pour qu'un ensemble fonctionne, il faut :\n",
    "\n",
    "1. **Diversit√©** : Les mod√®les doivent √™tre diff√©rents\n",
    "2. **Comp√©tence** : Chaque mod√®le doit √™tre meilleur que le hasard\n",
    "3. **Ind√©pendance** : Les erreurs ne doivent pas √™tre corr√©l√©es\n",
    "\n",
    "### B√©n√©fices\n",
    "\n",
    "‚úÖ **Am√©lioration de l'accuracy** : Typiquement +1-3%  \n",
    "‚úÖ **R√©duction du surapprentissage** : Moyenne des erreurs  \n",
    "‚úÖ **Estimations de confiance** : Variance entre pr√©dictions  \n",
    "‚úÖ **Robustesse** : Moins sensible aux outliers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Ajouter le dossier parent au path\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.network import NeuralNetwork\n",
    "from src.utils import load_mnist_data\n",
    "from src import visualize\n",
    "from src.metrics import accuracy, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Imports r√©ussis !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger MNIST\n",
    "print(\"Chargement des donn√©es MNIST...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist_data()\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape[0]:,} exemples\")\n",
    "print(f\"Val: {X_val.shape[0]:,} exemples\")\n",
    "print(f\"Test: {X_test.shape[0]:,} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ M√©thode 1 : Voting Classifier (Vote Majoritaire)\n",
    "\n",
    "### üó≥Ô∏è Principe\n",
    "\n",
    "Entra√Æner **plusieurs mod√®les diff√©rents** et combiner leurs pr√©dictions par vote.\n",
    "\n",
    "### Types de Vote\n",
    "\n",
    "1. **Hard Voting** : Vote majoritaire sur les classes pr√©dites\n",
    "   ```\n",
    "   Mod√®le 1: 3\n",
    "   Mod√®le 2: 5    ‚Üí  Vote final: 3 (majorit√©)\n",
    "   Mod√®le 3: 3\n",
    "   ```\n",
    "\n",
    "2. **Soft Voting** : Moyenne des probabilit√©s\n",
    "   ```\n",
    "   Mod√®le 1: [0.1, 0.8, 0.1]  \n",
    "   Mod√®le 2: [0.2, 0.6, 0.2]  ‚Üí  Moyenne: [0.17, 0.73, 0.1]\n",
    "   Mod√®le 3: [0.2, 0.7, 0.1]\n",
    "   ```\n",
    "\n",
    "**Soft voting est g√©n√©ralement meilleur** car il utilise plus d'information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er plusieurs architectures diff√©rentes pour la diversit√©\n",
    "architectures = [\n",
    "    {'name': 'Small', 'layers': [784, 128, 64, 10], 'lr': 0.01, 'optimizer': 'adam'},\n",
    "    {'name': 'Medium', 'layers': [784, 256, 128, 10], 'lr': 0.01, 'optimizer': 'adam'},\n",
    "    {'name': 'Deep', 'layers': [784, 128, 64, 32, 10], 'lr': 0.01, 'optimizer': 'adam'},\n",
    "    {'name': 'Wide', 'layers': [784, 512, 10], 'lr': 0.005, 'optimizer': 'momentum'},\n",
    "    {'name': 'Balanced', 'layers': [784, 200, 100, 10], 'lr': 0.01, 'optimizer': 'adam'},\n",
    "]\n",
    "\n",
    "print(f\"üéØ Entra√Ænement de {len(architectures)} mod√®les diff√©rents...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner tous les mod√®les\n",
    "models = []\n",
    "results = []\n",
    "\n",
    "for i, arch in enumerate(architectures, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ Mod√®le {i}/{len(architectures)}: {arch['name']}\")\n",
    "    print(f\"   Architecture: {' ‚Üí '.join(map(str, arch['layers']))}\")\n",
    "    print(f\"   Optimizer: {arch['optimizer']}, LR: {arch['lr']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Cr√©er et entra√Æner\n",
    "    model = NeuralNetwork(\n",
    "        layer_dims=arch['layers'],\n",
    "        learning_rate=arch['lr'],\n",
    "        optimizer=arch['optimizer']\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    model.train(X_train, y_train, X_val, y_val, \n",
    "                epochs=10, batch_size=128, verbose=False)\n",
    "    training_time = time.time() - start\n",
    "    \n",
    "    # √âvaluer\n",
    "    train_acc = model.accuracy(X_train, y_train)\n",
    "    val_acc = model.accuracy(X_val, y_val)\n",
    "    test_acc = model.accuracy(X_test, y_test)\n",
    "    \n",
    "    print(f\"\\n‚úì Entra√Ænement termin√© en {training_time:.1f}s\")\n",
    "    print(f\"  Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Acc:   {val_acc:.4f}\")\n",
    "    print(f\"  Test Acc:  {test_acc:.4f}\")\n",
    "    \n",
    "    models.append(model)\n",
    "    results.append({\n",
    "        'name': arch['name'],\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'time': training_time\n",
    "    })\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"‚úÖ TOUS LES MOD√àLES ENTRA√éN√âS !\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© des performances individuelles\n",
    "print(\"\\nüìä Performances Individuelles:\\n\")\n",
    "print(f\"{'Mod√®le':<12} {'Train':<10} {'Val':<10} {'Test':<10} {'Temps'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<12} {r['train_acc']:<10.4f} {r['val_acc']:<10.4f} \"\n",
    "          f\"{r['test_acc']:<10.4f} {r['time']:.1f}s\")\n",
    "\n",
    "avg_test_acc = np.mean([r['test_acc'] for r in results])\n",
    "print(f\"\\n{'Moyenne':<12} {'':<10} {'':<10} {avg_test_acc:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó≥Ô∏è Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_voting_predict(models, X):\n",
    "    \"\"\"\n",
    "    Hard voting: vote majoritaire sur les classes pr√©dites\n",
    "    \"\"\"\n",
    "    # Collecter toutes les pr√©dictions\n",
    "    predictions = np.array([model.predict(X) for model in models])  # Shape: (n_models, n_samples)\n",
    "    \n",
    "    # Vote majoritaire pour chaque √©chantillon\n",
    "    final_predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        votes = predictions[:, i]\n",
    "        # Compter les votes\n",
    "        vote_counts = Counter(votes)\n",
    "        # Classe la plus vot√©e\n",
    "        final_pred = vote_counts.most_common(1)[0][0]\n",
    "        final_predictions.append(final_pred)\n",
    "    \n",
    "    return np.array(final_predictions)\n",
    "\n",
    "# Tester le hard voting\n",
    "print(\"üó≥Ô∏è Hard Voting (vote majoritaire)...\\n\")\n",
    "y_pred_hard = hard_voting_predict(models, X_test)\n",
    "hard_voting_acc = np.mean(y_pred_hard == y_test)\n",
    "\n",
    "print(f\"Accuracy avec Hard Voting: {hard_voting_acc:.4f}\")\n",
    "print(f\"Meilleur mod√®le individuel: {max(r['test_acc'] for r in results):.4f}\")\n",
    "print(f\"Gain: {hard_voting_acc - max(r['test_acc'] for r in results):.4f} ({(hard_voting_acc / max(r['test_acc'] for r in results) - 1) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé≤ Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_voting_predict(models, X):\n",
    "    \"\"\"\n",
    "    Soft voting: moyenne des probabilit√©s\n",
    "    \"\"\"\n",
    "    # Collecter toutes les probabilit√©s\n",
    "    all_probs = []\n",
    "    for model in models:\n",
    "        probs, _ = model.forward(X)\n",
    "        all_probs.append(probs)\n",
    "    \n",
    "    # Moyenne des probabilit√©s\n",
    "    avg_probs = np.mean(all_probs, axis=0)  # Shape: (n_samples, n_classes)\n",
    "    \n",
    "    # Classe avec la plus haute probabilit√© moyenne\n",
    "    final_predictions = np.argmax(avg_probs, axis=1)\n",
    "    \n",
    "    return final_predictions, avg_probs\n",
    "\n",
    "# Tester le soft voting\n",
    "print(\"üé≤ Soft Voting (moyenne des probabilit√©s)...\\n\")\n",
    "y_pred_soft, probs_soft = soft_voting_predict(models, X_test)\n",
    "soft_voting_acc = np.mean(y_pred_soft == y_test)\n",
    "\n",
    "print(f\"Accuracy avec Soft Voting: {soft_voting_acc:.4f}\")\n",
    "print(f\"Meilleur mod√®le individuel: {max(r['test_acc'] for r in results):.4f}\")\n",
    "print(f\"Gain: {soft_voting_acc - max(r['test_acc'] for r in results):.4f} ({(soft_voting_acc / max(r['test_acc'] for r in results) - 1) * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüÜö Hard vs Soft:\")\n",
    "print(f\"  Hard Voting: {hard_voting_acc:.4f}\")\n",
    "print(f\"  Soft Voting: {soft_voting_acc:.4f}\")\n",
    "print(f\"  Diff√©rence: {soft_voting_acc - hard_voting_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer toutes les m√©thodes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "methods = ['Moyenne\\nIndividuelle', 'Meilleur\\nIndividuel', 'Hard\\nVoting', 'Soft\\nVoting']\n",
    "accuracies = [\n",
    "    avg_test_acc,\n",
    "    max(r['test_acc'] for r in results),\n",
    "    hard_voting_acc,\n",
    "    soft_voting_acc\n",
    "]\n",
    "colors = ['lightblue', 'orange', 'lightgreen', 'lightcoral']\n",
    "\n",
    "bars = ax.bar(methods, accuracies, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaison des M√©thodes d\\'Ensemble', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([min(accuracies) - 0.01, max(accuracies) + 0.01])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/ensemble_voting_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ M√©thode 2 : Bagging (Bootstrap Aggregating)\n",
    "\n",
    "### üéí Principe\n",
    "\n",
    "**Bagging** = **B**ootstrap **Agg**regat**ing**\n",
    "\n",
    "1. Cr√©er plusieurs **sous-ensembles** du dataset par bootstrap (√©chantillonnage avec remise)\n",
    "2. Entra√Æner un mod√®le sur chaque sous-ensemble\n",
    "3. Combiner les pr√©dictions (vote ou moyenne)\n",
    "\n",
    "### Avantages\n",
    "\n",
    "‚úÖ R√©duit la **variance** (surapprentissage)  \n",
    "‚úÖ Am√©liore la **stabilit√©**  \n",
    "‚úÖ Parall√©lisable (chaque mod√®le est ind√©pendant)  \n",
    "\n",
    "### Fonctionnement\n",
    "\n",
    "```\n",
    "Dataset Original: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "Bootstrap 1: [1, 1, 3, 5, 7, 8, 8, 9, 10, 10]  ‚Üí  Mod√®le 1\n",
    "Bootstrap 2: [2, 3, 3, 4, 5, 5, 6, 7, 9, 10]  ‚Üí  Mod√®le 2\n",
    "Bootstrap 3: [1, 2, 4, 4, 5, 6, 7, 8, 8, 9]   ‚Üí  Mod√®le 3\n",
    "...\n",
    "\n",
    "Pr√©diction finale = Vote ou Moyenne\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bootstrap_sample(X, y, sample_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Cr√©e un √©chantillon bootstrap (avec remise)\n",
    "    \"\"\"\n",
    "    n_samples = int(len(X) * sample_ratio)\n",
    "    indices = np.random.choice(len(X), size=n_samples, replace=True)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "# Configuration du bagging\n",
    "n_bagging_models = 5\n",
    "base_architecture = [784, 256, 128, 10]\n",
    "\n",
    "print(f\"üéí Entra√Ænement de {n_bagging_models} mod√®les avec Bagging...\\n\")\n",
    "print(f\"Architecture de base: {' ‚Üí '.join(map(str, base_architecture))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner les mod√®les bagging\n",
    "bagging_models = []\n",
    "\n",
    "for i in range(n_bagging_models):\n",
    "    print(f\"Mod√®le Bagging {i+1}/{n_bagging_models}...\")\n",
    "    \n",
    "    # Cr√©er bootstrap sample\n",
    "    X_boot, y_boot = create_bootstrap_sample(X_train, y_train)\n",
    "    \n",
    "    # Entra√Æner\n",
    "    model = NeuralNetwork(\n",
    "        layer_dims=base_architecture,\n",
    "        learning_rate=0.01,\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    model.train(X_boot, y_boot, X_val, y_val, \n",
    "                epochs=10, batch_size=128, verbose=False)\n",
    "    \n",
    "    test_acc = model.accuracy(X_test, y_test)\n",
    "    print(f\"  Test Acc: {test_acc:.4f}\\n\")\n",
    "    \n",
    "    bagging_models.append(model)\n",
    "\n",
    "print(\"‚úì Bagging termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer le bagging\n",
    "y_pred_bagging, probs_bagging = soft_voting_predict(bagging_models, X_test)\n",
    "bagging_acc = np.mean(y_pred_bagging == y_test)\n",
    "\n",
    "individual_accs = [model.accuracy(X_test, y_test) for model in bagging_models]\n",
    "avg_individual = np.mean(individual_accs)\n",
    "\n",
    "print(\"\\nüìä R√©sultats Bagging:\\n\")\n",
    "print(f\"Accuracy moyenne individuelle: {avg_individual:.4f}\")\n",
    "print(f\"Accuracy avec Bagging:        {bagging_acc:.4f}\")\n",
    "print(f\"Gain:                          {bagging_acc - avg_individual:.4f} (+{(bagging_acc / avg_individual - 1) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ M√©thode 3 : Weighted Voting (Vote Pond√©r√©)\n",
    "\n",
    "### ‚öñÔ∏è Principe\n",
    "\n",
    "Tous les mod√®les ne sont **pas √©gaux** !\n",
    "\n",
    "**Id√©e** : Donner plus de poids aux mod√®les plus performants.\n",
    "\n",
    "```\n",
    "Mod√®le 1 (acc=0.95): poids = 0.95\n",
    "Mod√®le 2 (acc=0.97): poids = 0.97  ‚Üí  Weighted Average\n",
    "Mod√®le 3 (acc=0.93): poids = 0.93\n",
    "```\n",
    "\n",
    "### Formule\n",
    "\n",
    "```\n",
    "P_weighted(classe) = Œ£ (weight_i √ó P_i(classe)) / Œ£ weights\n",
    "```\n",
    "\n",
    "o√π `weight_i` peut √™tre :\n",
    "- L'accuracy du mod√®le\n",
    "- Le F1-score\n",
    "- Une valeur optimis√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_voting_predict(models, weights, X):\n",
    "    \"\"\"\n",
    "    Vote pond√©r√© bas√© sur les poids des mod√®les\n",
    "    \"\"\"\n",
    "    # Normaliser les poids\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    # Collecter les probabilit√©s\n",
    "    all_probs = []\n",
    "    for model in models:\n",
    "        probs, _ = model.forward(X)\n",
    "        all_probs.append(probs)\n",
    "    \n",
    "    # Moyenne pond√©r√©e\n",
    "    weighted_probs = np.zeros_like(all_probs[0])\n",
    "    for i, probs in enumerate(all_probs):\n",
    "        weighted_probs += weights[i] * probs\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    final_predictions = np.argmax(weighted_probs, axis=1)\n",
    "    \n",
    "    return final_predictions, weighted_probs\n",
    "\n",
    "# Utiliser les accuracy comme poids\n",
    "weights = [r['test_acc'] for r in results]\n",
    "\n",
    "print(\"‚öñÔ∏è Weighted Voting (vote pond√©r√©)...\\n\")\n",
    "print(\"Poids des mod√®les:\")\n",
    "for i, (r, w) in enumerate(zip(results, weights)):\n",
    "    print(f\"  {r['name']:<12} : {w:.4f}\")\n",
    "\n",
    "# Pr√©diction\n",
    "y_pred_weighted, probs_weighted = weighted_voting_predict(models, weights, X_test)\n",
    "weighted_voting_acc = np.mean(y_pred_weighted == y_test)\n",
    "\n",
    "print(f\"\\nAccuracy avec Weighted Voting: {weighted_voting_acc:.4f}\")\n",
    "print(f\"Accuracy avec Soft Voting:     {soft_voting_acc:.4f}\")\n",
    "print(f\"Diff√©rence: {weighted_voting_acc - soft_voting_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Analyse de la Confiance et Diversit√©\n",
    "\n",
    "### üéØ Confiance de l'Ensemble\n",
    "\n",
    "Un avantage majeur des ensembles : **mesurer la confiance**\n",
    "\n",
    "**Variance des pr√©dictions** = Indicateur de certitude\n",
    "\n",
    "- **Variance faible** : Tous les mod√®les d'accord ‚Üí Haute confiance\n",
    "- **Variance √©lev√©e** : Mod√®les en d√©saccord ‚Üí Faible confiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la variance des pr√©dictions\n",
    "all_predictions = np.array([model.predict(X_test) for model in models])\n",
    "\n",
    "# Pour chaque √©chantillon, compter le nombre de mod√®les d'accord\n",
    "agreement_scores = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    votes = all_predictions[:, i]\n",
    "    most_common_count = Counter(votes).most_common(1)[0][1]\n",
    "    agreement = most_common_count / len(models)\n",
    "    agreement_scores.append(agreement)\n",
    "\n",
    "agreement_scores = np.array(agreement_scores)\n",
    "\n",
    "# Analyser par correctitude\n",
    "correct_mask = y_pred_soft == y_test\n",
    "agreement_correct = agreement_scores[correct_mask]\n",
    "agreement_incorrect = agreement_scores[~correct_mask]\n",
    "\n",
    "print(\"üéØ Analyse de la Confiance de l'Ensemble:\\n\")\n",
    "print(f\"Pr√©dictions Correctes:\")\n",
    "print(f\"  Agreement moyen: {agreement_correct.mean():.4f}\")\n",
    "print(f\"  √âcart-type: {agreement_correct.std():.4f}\")\n",
    "print(f\"\\nPr√©dictions Incorrectes:\")\n",
    "print(f\"  Agreement moyen: {agreement_incorrect.mean():.4f}\")\n",
    "print(f\"  √âcart-type: {agreement_incorrect.std():.4f}\")\n",
    "\n",
    "# Visualiser\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme\n",
    "ax1.hist(agreement_correct, bins=20, alpha=0.7, color='green', \n",
    "         label='Correctes', edgecolor='black')\n",
    "ax1.hist(agreement_incorrect, bins=20, alpha=0.7, color='red', \n",
    "         label='Incorrectes', edgecolor='black')\n",
    "ax1.set_xlabel('Agreement Score (% mod√®les d\\'accord)', fontsize=12)\n",
    "ax1.set_ylabel('Fr√©quence', fontsize=12)\n",
    "ax1.set_title('Distribution de l\\'Agreement', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Relation agreement vs accuracy\n",
    "thresholds = np.linspace(0.5, 1.0, 20)\n",
    "accuracies_by_threshold = []\n",
    "counts_by_threshold = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    mask = agreement_scores >= thresh\n",
    "    if mask.sum() > 0:\n",
    "        acc = np.mean(y_pred_soft[mask] == y_test[mask])\n",
    "        accuracies_by_threshold.append(acc)\n",
    "        counts_by_threshold.append(mask.sum())\n",
    "    else:\n",
    "        accuracies_by_threshold.append(np.nan)\n",
    "        counts_by_threshold.append(0)\n",
    "\n",
    "ax2.plot(thresholds, accuracies_by_threshold, 'o-', linewidth=2, markersize=6, color='blue')\n",
    "ax2.set_xlabel('Seuil d\\'Agreement', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12, color='blue')\n",
    "ax2.set_title('Accuracy vs Agreement', fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Ajouter le nombre d'√©chantillons\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(thresholds, counts_by_threshold, 's--', linewidth=2, \n",
    "              markersize=4, color='orange', alpha=0.7, label='Nombre')\n",
    "ax2_twin.set_ylabel('Nombre d\\'√©chantillons', fontsize=12, color='orange')\n",
    "ax2_twin.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/ensemble_confidence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation:\")\n",
    "print(\"- Plus l'agreement est √©lev√©, plus l'accuracy est √©lev√©e\")\n",
    "print(\"- On peut filtrer les pr√©dictions incertaines (faible agreement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Diversit√© des Mod√®les\n",
    "\n",
    "Pour qu'un ensemble soit efficace, les mod√®les doivent √™tre **diversifi√©s**.\n",
    "\n",
    "**Mesure de diversit√©** : Calculer le d√©saccord entre paires de mod√®les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de d√©saccord entre mod√®les\n",
    "n_models = len(models)\n",
    "disagreement_matrix = np.zeros((n_models, n_models))\n",
    "\n",
    "for i in range(n_models):\n",
    "    pred_i = models[i].predict(X_test)\n",
    "    for j in range(n_models):\n",
    "        pred_j = models[j].predict(X_test)\n",
    "        disagreement = np.mean(pred_i != pred_j)\n",
    "        disagreement_matrix[i, j] = disagreement\n",
    "\n",
    "# Visualiser\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "model_names = [r['name'] for r in results]\n",
    "sns.heatmap(disagreement_matrix, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=model_names, yticklabels=model_names,\n",
    "            cbar_kws={'label': 'Taux de D√©saccord'}, ax=ax)\n",
    "ax.set_title('Matrice de Diversit√© des Mod√®les', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/ensemble_diversity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "avg_disagreement = disagreement_matrix[np.triu_indices_from(disagreement_matrix, k=1)].mean()\n",
    "print(f\"\\nüìä D√©saccord moyen entre mod√®les: {avg_disagreement:.4f}\")\n",
    "print(f\"\\nüí° Diversit√© {'√©lev√©e' if avg_disagreement > 0.03 else 'faible'} ‚Üí \"\n",
    "      f\"Ensemble {'efficace' if avg_disagreement > 0.03 else 'peut √™tre am√©lior√©'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ R√©sum√© Final et Comparaison\n",
    "\n",
    "### üìä Tableau R√©capitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© de toutes les m√©thodes\n",
    "summary = {\n",
    "    'Meilleur Individuel': max(r['test_acc'] for r in results),\n",
    "    'Moyenne Individuelle': avg_test_acc,\n",
    "    'Hard Voting': hard_voting_acc,\n",
    "    'Soft Voting': soft_voting_acc,\n",
    "    'Weighted Voting': weighted_voting_acc,\n",
    "    'Bagging': bagging_acc,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä R√âSUM√â FINAL - COMPARAISON DES M√âTHODES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"{'M√©thode':<25} {'Test Accuracy':<15} {'Gain vs Meilleur'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_individual = summary['Meilleur Individuel']\n",
    "for method, acc in sorted(summary.items(), key=lambda x: x[1], reverse=True):\n",
    "    gain = acc - best_individual\n",
    "    gain_pct = (acc / best_individual - 1) * 100\n",
    "    print(f\"{method:<25} {acc:<15.4f} {gain:+.4f} ({gain_pct:+.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation finale\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "methods = list(summary.keys())\n",
    "accuracies = list(summary.values())\n",
    "colors = ['lightblue', 'orange', 'lightgreen', 'lightcoral', 'yellow', 'pink']\n",
    "\n",
    "bars = ax.barh(methods, accuracies, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{acc:.4f}',\n",
    "            ha='left', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaison Compl√®te des M√©thodes d\\'Ensemble', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([min(accuracies) - 0.005, max(accuracies) + 0.01])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/ensemble_final_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Conclusions et Recommandations\n",
    "\n",
    "### ‚úÖ Ce que nous avons appris\n",
    "\n",
    "1. **Les ensembles am√©liorent la performance** (typiquement +1-3%)\n",
    "2. **Soft voting > Hard voting** (utilise plus d'information)\n",
    "3. **La diversit√© est cl√©** pour l'efficacit√© de l'ensemble\n",
    "4. **Mesure de confiance** : L'agreement indique la certitude\n",
    "5. **Weighted voting** peut donner un l√©ger gain suppl√©mentaire\n",
    "\n",
    "### üéØ Quand Utiliser les Ensembles ?\n",
    "\n",
    "‚úÖ **Comp√©titions** : Chaque 0.1% compte !  \n",
    "‚úÖ **Applications critiques** : M√©decine, finance, s√©curit√©  \n",
    "‚úÖ **Estimations de confiance** : Besoin de savoir quand le mod√®le doute  \n",
    "‚úÖ **Datasets petits/bruit√©s** : R√©duction de la variance\n",
    "\n",
    "‚ùå **Ne PAS utiliser si** :\n",
    "- Contraintes de latence strictes\n",
    "- Ressources limit√©es (CPU/m√©moire)\n",
    "- Un seul mod√®le suffit (gain trop faible)\n",
    "\n",
    "### üöÄ Strat√©gies pour Maximiser la Diversit√©\n",
    "\n",
    "1. **Architectures diff√©rentes** (profond, large, etc.)\n",
    "2. **Hyperparam√®tres vari√©s** (learning rate, optimizers)\n",
    "3. **Initialisations diff√©rentes** (random seeds)\n",
    "4. **Sous-ensembles de donn√©es** (bagging, bootstrapping)\n",
    "5. **Features diff√©rentes** (si applicable)\n",
    "6. **Fonctions d'activation vari√©es**\n",
    "\n",
    "### üí∞ Trade-off Performance vs Co√ªt\n",
    "\n",
    "| M√©thode | Performance | Temps Entra√Ænement | Temps Inf√©rence | M√©moire |\n",
    "|---------|-------------|-------------------|----------------|----------|\n",
    "| 1 Mod√®le | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| 3-5 Mod√®les | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| 10+ Mod√®les | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê | ‚≠ê |\n",
    "\n",
    "**Sweet spot** : 3-5 mod√®les pour un bon compromis\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Exercices\n",
    "\n",
    "1. **Tester diff√©rentes strat√©gies de diversit√©** et comparer les gains\n",
    "2. **Impl√©menter un syst√®me de rejet** bas√© sur le seuil d'agreement\n",
    "3. **Optimiser les poids** du weighted voting avec validation crois√©e\n",
    "4. **Analyser le co√ªt computationnel** : temps vs gain de performance\n",
    "5. **Cr√©er un ensemble de CNN** et comparer avec l'ensemble de MLP\n",
    "\n",
    "---\n",
    "\n",
    "**F√©licitations ! üéâ**\n",
    "\n",
    "Vous ma√Ætrisez maintenant les **m√©thodes d'ensemble** et savez comment combiner plusieurs mod√®les pour obtenir des performances sup√©rieures !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
