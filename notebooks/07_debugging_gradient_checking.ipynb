{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ› Debugging & Gradient Checking\n",
    "\n",
    "Le debugging est une **compÃ©tence essentielle** en deep learning !\n",
    "\n",
    "## ğŸ¯ Objectifs\n",
    "\n",
    "1. **Gradient Checking** : VÃ©rifier que ta backprop est correcte\n",
    "2. **DÃ©tecter les problÃ¨mes** : Vanishing/exploding gradients\n",
    "3. **Visualiser les gradients** pendant l'entraÃ®nement\n",
    "4. **Erreurs courantes** et comment les corriger\n",
    "5. **Checklist de debugging** complÃ¨te\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Ready to debug!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Gradient Checking : VÃ©rifier la Backprop\n",
    "\n",
    "### ğŸ’¡ L'IdÃ©e\n",
    "\n",
    "Comparer le gradient **analytique** (backprop) avec le gradient **numÃ©rique** (approximation).\n",
    "\n",
    "### ğŸ“ Gradient NumÃ©rique\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} \\approx \\frac{L(w + \\epsilon) - L(w - \\epsilon)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "oÃ¹ $\\epsilon$ est trÃ¨s petit (ex: $10^{-7}$)\n",
    "\n",
    "### âœ… Si backprop est correcte :\n",
    "$$\n",
    "\\text{difference} = \\frac{\\|\\nabla_{\\text{num}} - \\nabla_{\\text{backprop}}\\|}{\\|\\nabla_{\\text{num}}\\| + \\|\\nabla_{\\text{backprop}}\\|} < 10^{-7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Calcule le gradient numÃ©rique\n",
    "    \n",
    "    Args:\n",
    "        f: fonction qui calcule la loss\n",
    "        x: paramÃ¨tre (peut Ãªtre un tableau)\n",
    "        epsilon: petite valeur pour l'approximation\n",
    "    \n",
    "    Returns:\n",
    "        grad: gradient numÃ©rique\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_value = x[idx]\n",
    "        \n",
    "        # f(x + epsilon)\n",
    "        x[idx] = old_value + epsilon\n",
    "        fx_plus = f(x)\n",
    "        \n",
    "        # f(x - epsilon)\n",
    "        x[idx] = old_value - epsilon\n",
    "        fx_minus = f(x)\n",
    "        \n",
    "        # Gradient\n",
    "        grad[idx] = (fx_plus - fx_minus) / (2 * epsilon)\n",
    "        \n",
    "        # Restaurer\n",
    "        x[idx] = old_value\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def gradient_check(grad_analytical, grad_numerical, threshold=1e-7):\n",
    "    \"\"\"\n",
    "    Compare gradient analytique vs numÃ©rique\n",
    "    \"\"\"\n",
    "    numerator = np.linalg.norm(grad_analytical - grad_numerical)\n",
    "    denominator = np.linalg.norm(grad_analytical) + np.linalg.norm(grad_numerical)\n",
    "    difference = numerator / (denominator + 1e-10)\n",
    "    \n",
    "    print(\"\\nğŸ” GRADIENT CHECKING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nDiffÃ©rence: {difference:.2e}\")\n",
    "    print(f\"Threshold: {threshold:.2e}\")\n",
    "    \n",
    "    if difference < threshold:\n",
    "        print(\"\\nâœ… EXCELLENT! Backpropagation est correcte!\")\n",
    "        return True\n",
    "    elif difference < 1e-5:\n",
    "        print(\"\\nâš ï¸ ACCEPTABLE. Backprop probablement correcte.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nâŒ PROBLÃˆME! Backpropagation a une erreur!\")\n",
    "        print(\"\\nğŸ› Ã€ vÃ©rifier:\")\n",
    "        print(\"   â€¢ Formules des dÃ©rivÃ©es\")\n",
    "        print(\"   â€¢ Dimensions des matrices\")\n",
    "        print(\"   â€¢ Ordre des opÃ©rations\")\n",
    "        return False\n",
    "\n",
    "print(\"âœ… Fonctions de gradient checking crÃ©Ã©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Test sur un Exemple Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : f(x) = x^2, donc f'(x) = 2x\n",
    "def simple_function(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "# Gradient analytique\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "grad_analytical = 2 * x\n",
    "\n",
    "# Gradient numÃ©rique\n",
    "grad_numerical = numerical_gradient(simple_function, x.copy())\n",
    "\n",
    "print(\"Test sur f(x) = xÂ²\")\n",
    "print(f\"\\nGradient analytique (2x): {grad_analytical}\")\n",
    "print(f\"Gradient numÃ©rique:       {grad_numerical}\")\n",
    "\n",
    "gradient_check(grad_analytical, grad_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Test sur un Petit RÃ©seau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un mini rÃ©seau pour tester\n",
    "class TinyNetwork:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(3, 2) * 0.01\n",
    "        self.b = np.zeros((1, 2))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        A = 1 / (1 + np.exp(-Z))  # Sigmoid\n",
    "        return A\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        A = self.forward(X)\n",
    "        return np.mean((A - Y) ** 2)\n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Forward\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "        \n",
    "        # Backward\n",
    "        dA = 2 * (A - Y) / m\n",
    "        dZ = dA * A * (1 - A)  # DÃ©rivÃ©e sigmoid\n",
    "        dW = np.dot(X.T, dZ)\n",
    "        db = np.sum(dZ, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW, db\n",
    "\n",
    "# DonnÃ©es de test\n",
    "X_test = np.random.randn(5, 3)\n",
    "Y_test = np.random.randn(5, 2)\n",
    "\n",
    "net = TinyNetwork()\n",
    "\n",
    "# Gradient analytique (backprop)\n",
    "dW_analytical, db_analytical = net.backward(X_test, Y_test)\n",
    "\n",
    "# Gradient numÃ©rique pour W\n",
    "def loss_w(W):\n",
    "    net.W = W.reshape(net.W.shape)\n",
    "    return net.loss(X_test, Y_test)\n",
    "\n",
    "W_flat = net.W.flatten()\n",
    "dW_numerical_flat = numerical_gradient(loss_w, W_flat.copy())\n",
    "dW_numerical = dW_numerical_flat.reshape(net.W.shape)\n",
    "\n",
    "print(\"\\nğŸ§ª TEST SUR UN PETIT RÃ‰SEAU\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGradient par rapport Ã  W:\")\n",
    "result = gradient_check(dW_analytical, dW_numerical)\n",
    "\n",
    "if result:\n",
    "    print(\"\\nğŸ‰ La backpropagation fonctionne correctement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ProblÃ¨mes Courants : Vanishing/Exploding Gradients\n",
    "\n",
    "### ğŸŒŠ Vanishing Gradient\n",
    "\n",
    "**SymptÃ´me** : Les gradients deviennent trÃ¨s petits (~0)\n",
    "\n",
    "**Causes** :\n",
    "- Activation sigmoid/tanh (gradients < 1)\n",
    "- RÃ©seau trÃ¨s profond\n",
    "- Mauvaise initialisation\n",
    "\n",
    "**Solutions** :\n",
    "- âœ… Utiliser ReLU au lieu de sigmoid\n",
    "- âœ… Batch normalization\n",
    "- âœ… Skip connections (ResNet)\n",
    "- âœ… He initialization\n",
    "\n",
    "### ğŸ’¥ Exploding Gradient\n",
    "\n",
    "**SymptÃ´me** : Les gradients deviennent trÃ¨s grands (â†’ âˆ)\n",
    "\n",
    "**Causes** :\n",
    "- Learning rate trop Ã©levÃ©\n",
    "- Poids mal initialisÃ©s\n",
    "- RNN sans prÃ©cautions\n",
    "\n",
    "**Solutions** :\n",
    "- âœ… Gradient clipping\n",
    "- âœ… Learning rate plus petit\n",
    "- âœ… Bonne initialisation\n",
    "- âœ… Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_gradient_flow():\n",
    "    \"\"\"\n",
    "    Simule le flow des gradients dans un rÃ©seau profond\n",
    "    \"\"\"\n",
    "    n_layers = 20\n",
    "    \n",
    "    # ScÃ©nario 1: Sigmoid (vanishing)\n",
    "    gradients_sigmoid = [1.0]\n",
    "    for _ in range(n_layers):\n",
    "        # Gradient de sigmoid â‰ˆ 0.25 en moyenne\n",
    "        gradients_sigmoid.append(gradients_sigmoid[-1] * 0.25)\n",
    "    \n",
    "    # ScÃ©nario 2: Mauvaise init (exploding)\n",
    "    gradients_exploding = [1.0]\n",
    "    for _ in range(n_layers):\n",
    "        # Poids trop grands\n",
    "        gradients_exploding.append(gradients_exploding[-1] * 1.5)\n",
    "    \n",
    "    # ScÃ©nario 3: ReLU + He init (bon)\n",
    "    gradients_good = [1.0]\n",
    "    for _ in range(n_layers):\n",
    "        # Bien Ã©quilibrÃ©\n",
    "        gradients_good.append(gradients_good[-1] * 0.95)\n",
    "    \n",
    "    # Visualiser\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('ğŸŒŠ Gradient Flow dans un RÃ©seau Profond', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    layers = range(n_layers + 1)\n",
    "    \n",
    "    # Vanishing\n",
    "    axes[0].semilogy(layers, gradients_sigmoid, 'o-', linewidth=3, markersize=6, color='#e74c3c')\n",
    "    axes[0].set_title('âŒ Vanishing Gradient\\n(Sigmoid)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Couche', fontsize=12)\n",
    "    axes[0].set_ylabel('Magnitude du Gradient (log)', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=1e-10, color='red', linestyle='--', label='Trop petit!')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Exploding\n",
    "    axes[1].semilogy(layers, gradients_exploding, 's-', linewidth=3, markersize=6, color='#f39c12')\n",
    "    axes[1].set_title('âŒ Exploding Gradient\\n(Mauvaise Init)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Couche', fontsize=12)\n",
    "    axes[1].set_ylabel('Magnitude du Gradient (log)', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=1e10, color='red', linestyle='--', label='Trop grand!')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Bon\n",
    "    axes[2].semilogy(layers, gradients_good, '^-', linewidth=3, markersize=6, color='#2ecc71')\n",
    "    axes[2].set_title('âœ… Gradient Stable\\n(ReLU + He Init)', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Couche', fontsize=12)\n",
    "    axes[2].set_ylabel('Magnitude du Gradient (log)', fontsize=12)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].axhline(y=1.0, color='green', linestyle='--', label='Stable!')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“Š Observations:\")\n",
    "    print(f\"   Vanishing: {gradients_sigmoid[-1]:.2e} (trop petit!)\")\n",
    "    print(f\"   Exploding: {gradients_exploding[-1]:.2e} (trop grand!)\")\n",
    "    print(f\"   Stable: {gradients_good[-1]:.2e} (juste bien!)\")\n",
    "\n",
    "simulate_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Checklist de Debugging\n",
    "\n",
    "### ğŸ” Avant l'EntraÃ®nement\n",
    "\n",
    "- [ ] **Overfit sur un petit batch** (2-10 exemples)\n",
    "  - Si Ã§a marche pas â†’ problÃ¨me d'implÃ©mentation\n",
    "  - Si Ã§a marche â†’ problÃ¨me de rÃ©gularisation/donnÃ©es\n",
    "\n",
    "- [ ] **VÃ©rifier les shapes** de tous les tenseurs\n",
    "  ```python\n",
    "  print(f\"X: {X.shape}, W: {W.shape}, output: {output.shape}\")\n",
    "  ```\n",
    "\n",
    "- [ ] **Gradient checking** sur un mini rÃ©seau\n",
    "\n",
    "- [ ] **Initialisation** appropriÃ©e (He pour ReLU)\n",
    "\n",
    "- [ ] **Learning rate** raisonnable (commencer petit: 0.001)\n",
    "\n",
    "### ğŸ” Pendant l'EntraÃ®nement\n",
    "\n",
    "- [ ] **Loss diminue** rÃ©guliÃ¨rement\n",
    "  - Si stagne â†’ LR trop petit ou problÃ¨me d'optimisation\n",
    "  - Si explose â†’ LR trop grand\n",
    "\n",
    "- [ ] **Train/Val accuracy** augmentent\n",
    "  - Si train Ã©levÃ© mais val faible â†’ overfitting\n",
    "  - Si les deux faibles â†’ underfitting\n",
    "\n",
    "- [ ] **Magnitude des gradients** raisonnable (1e-5 Ã  1e-1)\n",
    "\n",
    "- [ ] **Pas de NaN/Inf** dans les activations/gradients\n",
    "\n",
    "### ğŸ› ProblÃ¨mes FrÃ©quents\n",
    "\n",
    "| SymptÃ´me | Cause Probable | Solution |\n",
    "|----------|---------------|----------|\n",
    "| Loss = NaN | Learning rate trop Ã©levÃ© | RÃ©duire LR |\n",
    "| Loss ne bouge pas | LR trop petit ou mauvaise init | Augmenter LR ou rÃ©init |\n",
    "| Loss oscille | Batch size trop petit | Augmenter batch size |\n",
    "| Accuracy alÃ©atoire | Forward ou backprop incorrecte | Debug Ã©tape par Ã©tape |\n",
    "| Train OK, val mauvais | Overfitting | RÃ©gularisation, dropout |\n",
    "| Les deux mauvais | Underfitting ou bug | Plus de capacitÃ© ou debug |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Outils de Debugging Pratiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugNetwork:\n",
    "    \"\"\"\n",
    "    RÃ©seau avec outils de debugging intÃ©grÃ©s\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        self.has_nan = False\n",
    "    \n",
    "    def check_nan_inf(self, tensor, name):\n",
    "        \"\"\"VÃ©rifie NaN/Inf\"\"\"\n",
    "        if np.any(np.isnan(tensor)):\n",
    "            print(f\"âŒ NaN dÃ©tectÃ© dans {name}!\")\n",
    "            self.has_nan = True\n",
    "        if np.any(np.isinf(tensor)):\n",
    "            print(f\"âŒ Inf dÃ©tectÃ© dans {name}!\")\n",
    "            self.has_nan = True\n",
    "    \n",
    "    def check_gradient_magnitude(self, grad, name, threshold_low=1e-8, threshold_high=1e2):\n",
    "        \"\"\"VÃ©rifie la magnitude des gradients\"\"\"\n",
    "        magnitude = np.linalg.norm(grad)\n",
    "        \n",
    "        if magnitude < threshold_low:\n",
    "            print(f\"âš ï¸ Gradient de {name} trop petit: {magnitude:.2e} (vanishing?)\")\n",
    "        elif magnitude > threshold_high:\n",
    "            print(f\"âš ï¸ Gradient de {name} trop grand: {magnitude:.2e} (exploding?)\")\n",
    "        else:\n",
    "            print(f\"âœ… Gradient de {name}: {magnitude:.2e} (OK)\")\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Affiche un rÃ©sumÃ© de debug\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ” DEBUG SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nğŸ“Š Activations:\")\n",
    "        for name, act in self.activations.items():\n",
    "            print(f\"   {name}: shape={act.shape}, mean={act.mean():.4f}, std={act.std():.4f}\")\n",
    "        \n",
    "        print(\"\\nğŸ“Š Gradients:\")\n",
    "        for name, grad in self.gradients.items():\n",
    "            print(f\"   {name}: shape={grad.shape}, norm={np.linalg.norm(grad):.2e}\")\n",
    "        \n",
    "        if not self.has_nan:\n",
    "            print(\"\\nâœ… Pas de NaN/Inf dÃ©tectÃ©\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "debug_net = DebugNetwork()\n",
    "\n",
    "# Simuler quelques tenseurs\n",
    "debug_net.activations['layer1'] = np.random.randn(10, 128)\n",
    "debug_net.gradients['dW1'] = np.random.randn(784, 128) * 0.01\n",
    "\n",
    "debug_net.check_nan_inf(debug_net.activations['layer1'], 'layer1')\n",
    "debug_net.check_gradient_magnitude(debug_net.gradients['dW1'], 'dW1')\n",
    "debug_net.summary()\n",
    "\n",
    "print(\"\\nâœ… Outils de debugging crÃ©Ã©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ RÃ©capitulatif\n",
    "\n",
    "### âœ… Ce que tu as appris\n",
    "\n",
    "1. **Gradient Checking**\n",
    "   - Compare backprop vs approximation numÃ©rique\n",
    "   - Essentiel pour vÃ©rifier ton implÃ©mentation\n",
    "   - DiffÃ©rence < 1e-7 = excellent\n",
    "\n",
    "2. **ProblÃ¨mes de Gradients**\n",
    "   - Vanishing: gradients â†’ 0 (sigmoid, rÃ©seaux profonds)\n",
    "   - Exploding: gradients â†’ âˆ (mauvaise init, LR trop grand)\n",
    "   - Solutions: ReLU, He init, BatchNorm, gradient clipping\n",
    "\n",
    "3. **Checklist de Debugging**\n",
    "   - Overfit sur petit batch d'abord\n",
    "   - VÃ©rifier shapes et NaN/Inf\n",
    "   - Monitorer loss et accuracy\n",
    "   - Magnitude des gradients\n",
    "\n",
    "4. **Outils Pratiques**\n",
    "   - Fonctions de vÃ©rification automatiques\n",
    "   - Logging des activations et gradients\n",
    "   - Debug summary\n",
    "\n",
    "### ğŸ’¡ Points clÃ©s\n",
    "\n",
    "- ğŸ› **Debug tÃ´t** : Gradient checking dÃ¨s le dÃ©but\n",
    "- ğŸ“Š **Monitor** : Loss, accuracy, gradients\n",
    "- ğŸ” **Overfit d'abord** : Si Ã§a marche pas sur 10 exemples, Ã§a marchera pas sur 60k\n",
    "- âš ï¸ **Signes d'alerte** : NaN, loss qui explose, gradients extrÃªmes\n",
    "\n",
    "### ğŸš€ Prochaines Ã‰tapes\n",
    "\n",
    "Maintenant que tu sais debugger :\n",
    "- InterprÃ©ter ce que le rÃ©seau a appris\n",
    "- Visualiser les features\n",
    "- Comprendre les erreurs\n",
    "\n",
    "---\n",
    "\n",
    "**Le debugging est une compÃ©tence qui te sauvera des heures de frustration ! ğŸ“**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
