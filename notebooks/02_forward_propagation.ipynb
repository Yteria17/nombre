{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â¡ï¸ Forward Propagation - Comment un RÃ©seau Fait des PrÃ©dictions\n",
    "\n",
    "Dans ce notebook, nous allons comprendre en profondeur comment un rÃ©seau de neurones fait des **prÃ©dictions**.\n",
    "\n",
    "## ğŸ¯ Objectifs\n",
    "\n",
    "1. **Comprendre la forward propagation** pas Ã  pas\n",
    "2. **ImplÃ©menter** chaque Ã©tape en Python\n",
    "3. **Visualiser** ce qui se passe dans le rÃ©seau\n",
    "4. **Tester** avec de vraies donnÃ©es MNIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Fixer la seed pour la reproductibilitÃ©\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Imports rÃ©ussis !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Rappel : Architecture du RÃ©seau\n",
    "\n",
    "Nous allons construire un rÃ©seau simple pour la classification MNIST :\n",
    "\n",
    "```\n",
    "Input Layer       Hidden Layer      Output Layer\n",
    "   (784)    â†’       (128)      â†’       (10)\n",
    "    \n",
    "    x        â†’   W1, b1, ReLU  â†’    W2, b2, Softmax  â†’  probabilities\n",
    "```\n",
    "\n",
    "### ğŸ”‘ Composants clÃ©s :\n",
    "\n",
    "- **Input** : 784 features (28Ã—28 pixels)\n",
    "- **Hidden Layer** : 128 neurones avec activation ReLU\n",
    "- **Output Layer** : 10 neurones (un par chiffre) avec activation Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Les OpÃ©rations MathÃ©matiques\n",
    "\n",
    "La forward propagation consiste en une sÃ©rie d'opÃ©rations matricielles simples !\n",
    "\n",
    "### Pour chaque couche :\n",
    "\n",
    "#### Ã‰tape 1: Multiplication matricielle + biais\n",
    "$$Z = X \\cdot W + b$$\n",
    "\n",
    "#### Ã‰tape 2: Fonction d'activation\n",
    "$$A = f(Z)$$\n",
    "\n",
    "### Pour notre rÃ©seau complet :\n",
    "\n",
    "```\n",
    "Layer 1 (Hidden):\n",
    "   Z1 = X Â· W1 + b1\n",
    "   A1 = ReLU(Z1)\n",
    "\n",
    "Layer 2 (Output):\n",
    "   Z2 = A1 Â· W2 + b2\n",
    "   A2 = Softmax(Z2)\n",
    "   \n",
    "Output: A2 = probabilitÃ©s pour chaque classe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ImplÃ©mentons les Fonctions d'Activation\n",
    "\n",
    "CommenÃ§ons par implÃ©menter les fonctions d'activation que nous allons utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit)\n",
    "    \n",
    "    Formule: f(x) = max(0, x)\n",
    "    \n",
    "    Pourquoi ReLU ?\n",
    "    - Simple et rapide Ã  calculer\n",
    "    - Ã‰vite le problÃ¨me du gradient qui disparaÃ®t\n",
    "    - TrÃ¨s efficace en pratique\n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Softmax - Transforme des scores en probabilitÃ©s\n",
    "    \n",
    "    Formule: softmax(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "    \n",
    "    PropriÃ©tÃ©s:\n",
    "    - Sortie entre 0 et 1 pour chaque classe\n",
    "    - La somme de toutes les sorties = 1\n",
    "    - IdÃ©al pour la classification multi-classes\n",
    "    \n",
    "    Astuce: On soustrait le max pour la stabilitÃ© numÃ©rique\n",
    "    \"\"\"\n",
    "    # Soustraire le max pour Ã©viter l'overflow (astuce numÃ©rique)\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "# Test des fonctions\n",
    "print(\"ğŸ§ª Test de ReLU:\")\n",
    "test_relu = np.array([-2, -1, 0, 1, 2])\n",
    "print(f\"   Input:  {test_relu}\")\n",
    "print(f\"   Output: {relu(test_relu)}\")\n",
    "print(\"   â¡ï¸ Les valeurs nÃ©gatives deviennent 0, les positives restent inchangÃ©es\\n\")\n",
    "\n",
    "print(\"ğŸ§ª Test de Softmax:\")\n",
    "test_softmax = np.array([[1, 2, 3, 4, 5]])\n",
    "result_softmax = softmax(test_softmax)\n",
    "print(f\"   Input:  {test_softmax[0]}\")\n",
    "print(f\"   Output: {result_softmax[0]}\")\n",
    "print(f\"   Somme:  {result_softmax.sum():.4f} (doit Ãªtre 1.0)\")\n",
    "print(\"   â¡ï¸ Chaque valeur est une probabilitÃ©, somme = 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visualisons ces Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations():\n",
    "    \"\"\"\n",
    "    Visualise ReLU et Softmax\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # ReLU\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y_relu = relu(x)\n",
    "    \n",
    "    ax1.plot(x, y_relu, linewidth=3, color='#e74c3c', label='ReLU(x)')\n",
    "    ax1.plot(x, x, '--', linewidth=2, color='gray', alpha=0.5, label='y = x (rÃ©fÃ©rence)')\n",
    "    ax1.axhline(y=0, color='black', linewidth=0.8)\n",
    "    ax1.axvline(x=0, color='black', linewidth=0.8)\n",
    "    ax1.fill_between(x, 0, y_relu, alpha=0.3, color='#e74c3c')\n",
    "    ax1.set_xlabel('Input (Z)', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Output', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('âš¡ Fonction ReLU', fontsize=16, fontweight='bold')\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotations\n",
    "    ax1.annotate('Zone morte\\n(gradient = 0)', xy=(-2, 0), xytext=(-2.5, 0.8),\n",
    "                fontsize=11, ha='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    ax1.annotate('Zone active\\n(gradient = 1)', xy=(2, 2), xytext=(1.5, 2.8),\n",
    "                fontsize=11, ha='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    \n",
    "    # Softmax - exemple avec 5 classes\n",
    "    scores = np.array([[1, 2, 3, 4, 5],\n",
    "                       [5, 4, 3, 2, 1],\n",
    "                       [3, 3, 3, 3, 3],\n",
    "                       [1, 5, 1, 1, 1]])\n",
    "    probabilities = softmax(scores)\n",
    "    \n",
    "    x_pos = np.arange(4)\n",
    "    width = 0.15\n",
    "    colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\n",
    "    \n",
    "    for class_idx in range(5):\n",
    "        ax2.bar(x_pos + class_idx * width, probabilities[:, class_idx], \n",
    "               width, label=f'Classe {class_idx}', color=colors[class_idx],\n",
    "               edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax2.set_xlabel('Exemple', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('ProbabilitÃ©', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('ğŸ¯ Fonction Softmax (5 classes)', fontsize=16, fontweight='bold')\n",
    "    ax2.set_xticks(x_pos + width * 2)\n",
    "    ax2.set_xticklabels(['Ex 1', 'Ex 2', 'Ex 3', 'Ex 4'])\n",
    "    ax2.legend(fontsize=10, loc='upper right')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Annotations\n",
    "    ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "    ax2.text(3.5, 1.02, 'Somme = 1.0 pour chaque exemple', \n",
    "            fontsize=11, ha='right', style='italic', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Initialisation des Poids\n",
    "\n",
    "Avant de faire des prÃ©dictions, il faut initialiser les poids du rÃ©seau.\n",
    "\n",
    "### ğŸ² StratÃ©gies d'initialisation :\n",
    "\n",
    "- **AlÃ©atoire simple** : Mauvaise idÃ©e (gradients peuvent exploser/disparaÃ®tre)\n",
    "- **Xavier/Glorot** : Bon pour sigmoid/tanh\n",
    "- **He initialization** : â­ IdÃ©al pour ReLU (ce que nous utilisons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialise les poids et biais du rÃ©seau\n",
    "    \n",
    "    Args:\n",
    "        layer_dims: liste des dimensions de chaque couche\n",
    "                   ex: [784, 128, 10] pour notre rÃ©seau\n",
    "    \n",
    "    Returns:\n",
    "        parameters: dictionnaire contenant W1, b1, W2, b2, etc.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # nombre de couches\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        # He initialization: multiplier par sqrt(2/n)\n",
    "        # OptimisÃ© pour ReLU\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l-1], layer_dims[l]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        \n",
    "        # Biais initialisÃ©s Ã  zÃ©ro\n",
    "        parameters[f'b{l}'] = np.zeros((1, layer_dims[l]))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# CrÃ©er notre architecture\n",
    "layer_dimensions = [784, 128, 10]\n",
    "params = initialize_parameters(layer_dimensions)\n",
    "\n",
    "print(\"ğŸ—ï¸ Architecture du rÃ©seau crÃ©Ã©e !\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š PARAMÃˆTRES DU RÃ‰SEAU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for key, value in params.items():\n",
    "    print(f\"\\n{key}: shape = {value.shape}\")\n",
    "    if 'W' in key:\n",
    "        print(f\"   - Nombre de paramÃ¨tres: {value.size:,}\")\n",
    "        print(f\"   - Moyenne: {value.mean():.6f}\")\n",
    "        print(f\"   - Std: {value.std():.6f}\")\n",
    "\n",
    "total_params = sum(p.size for p in params.values())\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ¨ Nombre total de paramÃ¨tres: {total_params:,}\")\n",
    "print(f\"   (C'est ce que le rÃ©seau va apprendre !)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visualisons la Distribution des Poids Initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weight_distribution(parameters):\n",
    "    \"\"\"\n",
    "    Visualise la distribution des poids aprÃ¨s initialisation\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('ğŸ“Š Distribution des Poids Initiaux (He Initialization)', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # W1\n",
    "    axes[0].hist(parameters['W1'].flatten(), bins=50, color='#3498db', \n",
    "                edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0].set_xlabel('Valeur du poids', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylabel('FrÃ©quence', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_title(f'W1: {parameters[\"W1\"].shape}', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Stats\n",
    "    mean_w1 = parameters['W1'].mean()\n",
    "    std_w1 = parameters['W1'].std()\n",
    "    axes[0].text(0.95, 0.95, f'Î¼ = {mean_w1:.4f}\\nÏƒ = {std_w1:.4f}',\n",
    "                transform=axes[0].transAxes, fontsize=12,\n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # W2\n",
    "    axes[1].hist(parameters['W2'].flatten(), bins=50, color='#2ecc71',\n",
    "                edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1].set_xlabel('Valeur du poids', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_ylabel('FrÃ©quence', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_title(f'W2: {parameters[\"W2\"].shape}', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Stats\n",
    "    mean_w2 = parameters['W2'].mean()\n",
    "    std_w2 = parameters['W2'].std()\n",
    "    axes[1].text(0.95, 0.95, f'Î¼ = {mean_w2:.4f}\\nÏƒ = {std_w2:.4f}',\n",
    "                transform=axes[1].transAxes, fontsize=12,\n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Pourquoi cette distribution ?\")\n",
    "    print(\"   â€¢ CentrÃ©e sur 0 (Î¼ â‰ˆ 0)\")\n",
    "    print(\"   â€¢ Variance calculÃ©e pour Ã©viter explosion/disparition du gradient\")\n",
    "    print(\"   â€¢ He initialization: optimale pour ReLU\")\n",
    "\n",
    "visualize_weight_distribution(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Forward Propagation - ImplÃ©mentation ComplÃ¨te\n",
    "\n",
    "Maintenant, implÃ©mentons la forward propagation complÃ¨te !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, verbose=False):\n",
    "    \"\"\"\n",
    "    Effectue la forward propagation\n",
    "    \n",
    "    Args:\n",
    "        X: donnÃ©es d'entrÃ©e (n_samples, 784)\n",
    "        parameters: poids et biais du rÃ©seau\n",
    "        verbose: afficher les dÃ©tails Ã©tape par Ã©tape\n",
    "    \n",
    "    Returns:\n",
    "        A2: prÃ©dictions finales (probabilitÃ©s)\n",
    "        cache: valeurs intermÃ©diaires (utiles pour la backprop)\n",
    "    \"\"\"\n",
    "    # RÃ©cupÃ©rer les paramÃ¨tres\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸš€ FORWARD PROPAGATION - Ã‰TAPE PAR Ã‰TAPE\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nğŸ“¥ Input: X.shape = {X.shape}\")\n",
    "    \n",
    "    # ===== COUCHE 1 (Hidden Layer) =====\n",
    "    # Ã‰tape 1a: Multiplication matricielle + biais\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ”¸ Layer 1 - Linear Transformation:\")\n",
    "        print(f\"   Z1 = X Â· W1 + b1\")\n",
    "        print(f\"   Z1.shape = {Z1.shape}\")\n",
    "        print(f\"   Z1 range: [{Z1.min():.3f}, {Z1.max():.3f}]\")\n",
    "    \n",
    "    # Ã‰tape 1b: Activation ReLU\n",
    "    A1 = relu(Z1)\n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ”¸ Layer 1 - Activation (ReLU):\")\n",
    "        print(f\"   A1 = ReLU(Z1)\")\n",
    "        print(f\"   A1.shape = {A1.shape}\")\n",
    "        print(f\"   A1 range: [{A1.min():.3f}, {A1.max():.3f}]\")\n",
    "        print(f\"   Neurons activated: {np.sum(A1 > 0)} / {A1.size} ({np.sum(A1 > 0)/A1.size*100:.1f}%)\")\n",
    "    \n",
    "    # ===== COUCHE 2 (Output Layer) =====\n",
    "    # Ã‰tape 2a: Multiplication matricielle + biais\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ”¸ Layer 2 - Linear Transformation:\")\n",
    "        print(f\"   Z2 = A1 Â· W2 + b2\")\n",
    "        print(f\"   Z2.shape = {Z2.shape}\")\n",
    "        print(f\"   Z2 range: [{Z2.min():.3f}, {Z2.max():.3f}]\")\n",
    "    \n",
    "    # Ã‰tape 2b: Activation Softmax\n",
    "    A2 = softmax(Z2)\n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ”¸ Layer 2 - Activation (Softmax):\")\n",
    "        print(f\"   A2 = Softmax(Z2)\")\n",
    "        print(f\"   A2.shape = {A2.shape}\")\n",
    "        print(f\"   A2 range: [{A2.min():.3f}, {A2.max():.3f}]\")\n",
    "        print(f\"   Sum of probabilities: {A2[0].sum():.6f} (should be 1.0)\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ… FORWARD PROPAGATION COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # Sauvegarder les valeurs pour la backpropagation\n",
    "    cache = {\n",
    "        'Z1': Z1, 'A1': A1,\n",
    "        'Z2': Z2, 'A2': A2\n",
    "    }\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "print(\"âœ… Fonction forward_propagation crÃ©Ã©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Test avec des DonnÃ©es RÃ©elles\n",
    "\n",
    "Testons notre forward propagation avec de vraies images MNIST !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger MNIST\n",
    "from src.utils import load_mnist_data\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist_data()\n",
    "\n",
    "# Prendre un seul exemple\n",
    "single_example = X_train[0:1]  # Shape: (1, 784)\n",
    "true_label = y_train[0]\n",
    "\n",
    "print(f\"\\nğŸ–¼ï¸ Test avec une image du chiffre: {true_label}\")\n",
    "\n",
    "# Forward propagation avec mode verbose\n",
    "predictions, cache = forward_propagation(single_example, params, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visualisons la PrÃ©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(image, predictions, true_label):\n",
    "    \"\"\"\n",
    "    Visualise l'image et les prÃ©dictions du rÃ©seau\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Image originale\n",
    "    ax1.imshow(image.reshape(28, 28), cmap='gray_r')\n",
    "    ax1.set_title(f'ğŸ–¼ï¸ Image du chiffre: {true_label}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # PrÃ©dictions\n",
    "    classes = range(10)\n",
    "    probs = predictions[0]\n",
    "    predicted_class = np.argmax(probs)\n",
    "    \n",
    "    colors = ['green' if i == predicted_class else 'lightblue' for i in classes]\n",
    "    colors[true_label] = 'orange' if true_label != predicted_class else 'green'\n",
    "    \n",
    "    bars = ax2.bar(classes, probs, color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax2.set_xlabel('Classe (Chiffre)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('ProbabilitÃ©', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('ğŸ¯ PrÃ©dictions du RÃ©seau (non entraÃ®nÃ©)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax2.set_xticks(classes)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for i, (bar, prob) in enumerate(zip(bars, probs)):\n",
    "        height = bar.get_height()\n",
    "        if prob > 0.01:  # Afficher seulement si > 1%\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{prob:.1%}',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # LÃ©gende\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', label='PrÃ©diction (et vrai label)' if predicted_class == true_label else 'PrÃ©diction'),\n",
    "        Patch(facecolor='orange', label='Vrai label'),\n",
    "        Patch(facecolor='lightblue', label='Autres classes')\n",
    "    ]\n",
    "    if predicted_class == true_label:\n",
    "        legend_elements = legend_elements[:1] + legend_elements[2:]\n",
    "    ax2.legend(handles=legend_elements, fontsize=11, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # RÃ©sultat\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š RÃ‰SULTAT DE LA PRÃ‰DICTION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nğŸ¯ Vrai label: {true_label}\")\n",
    "    print(f\"ğŸ¤– PrÃ©diction: {predicted_class}\")\n",
    "    print(f\"ğŸ“ˆ Confiance: {probs[predicted_class]:.2%}\")\n",
    "    \n",
    "    if predicted_class == true_label:\n",
    "        print(f\"\\nâœ… CORRECT ! (mais c'est par hasard, le rÃ©seau n'est pas encore entraÃ®nÃ©)\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ INCORRECT (normal, le rÃ©seau n'est pas encore entraÃ®nÃ©)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Top 3 prÃ©dictions:\")\n",
    "    top3_indices = np.argsort(probs)[::-1][:3]\n",
    "    for i, idx in enumerate(top3_indices, 1):\n",
    "        print(f\"   {i}. Chiffre {idx}: {probs[idx]:.2%}\")\n",
    "\n",
    "visualize_prediction(single_example, predictions, true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Testons avec Plusieurs Exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_predictions(X, y, parameters, n_samples=10):\n",
    "    \"\"\"\n",
    "    Teste le rÃ©seau sur plusieurs exemples\n",
    "    \"\"\"\n",
    "    # SÃ©lectionner des exemples alÃ©atoires\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    \n",
    "    # Forward propagation\n",
    "    predictions, _ = forward_propagation(X[indices], parameters)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = y[indices]\n",
    "    \n",
    "    # Visualiser\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(16, 7))\n",
    "    fig.suptitle('ğŸ² PrÃ©dictions sur 10 Exemples AlÃ©atoires (RÃ©seau Non EntraÃ®nÃ©)', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        image = X[indices[idx]].reshape(28, 28)\n",
    "        true_label = true_labels[idx]\n",
    "        pred_label = predicted_labels[idx]\n",
    "        confidence = predictions[idx][pred_label]\n",
    "        \n",
    "        ax.imshow(image, cmap='gray_r')\n",
    "        \n",
    "        # Couleur selon correct/incorrect\n",
    "        color = 'green' if pred_label == true_label else 'red'\n",
    "        title = f'Vrai: {true_label} | Pred: {pred_label}\\nConf: {confidence:.1%}'\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold', color=color)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Bordure\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(color)\n",
    "            spine.set_linewidth(3)\n",
    "            spine.set_visible(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š STATISTIQUES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nğŸ¯ Accuracy: {accuracy:.1%} ({np.sum(predicted_labels == true_labels)}/{n_samples})\")\n",
    "    print(f\"\\nğŸ’¡ Note: Le rÃ©seau n'est PAS encore entraÃ®nÃ© !\")\n",
    "    print(f\"   - Performance alÃ©atoire attendue: ~10% (1/10 classes)\")\n",
    "    print(f\"   - Performance actuelle: {accuracy:.1%}\")\n",
    "    if accuracy > 0.15:\n",
    "        print(f\"   â¡ï¸ Un peu de chance ! ğŸ€\")\n",
    "    else:\n",
    "        print(f\"   â¡ï¸ Normal pour un rÃ©seau non entraÃ®nÃ©\")\n",
    "\n",
    "test_multiple_predictions(X_test, y_test, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Visualisons l'ActivitÃ© des Neurones\n",
    "\n",
    "Regardons ce qui se passe dans la couche cachÃ©e !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hidden_layer_activity(X, y, parameters, sample_idx=0):\n",
    "    \"\"\"\n",
    "    Visualise l'activitÃ© de la couche cachÃ©e\n",
    "    \"\"\"\n",
    "    # Forward propagation\n",
    "    single_input = X[sample_idx:sample_idx+1]\n",
    "    _, cache = forward_propagation(single_input, parameters)\n",
    "    \n",
    "    A1 = cache['A1'][0]  # Activations de la couche cachÃ©e (128 neurones)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'ğŸ”¬ Analyse de la Couche CachÃ©e - Chiffre: {y[sample_idx]}', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. Image originale\n",
    "    axes[0, 0].imshow(X[sample_idx].reshape(28, 28), cmap='gray_r')\n",
    "    axes[0, 0].set_title('ğŸ–¼ï¸ Image d\\'entrÃ©e', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # 2. Activations des neurones\n",
    "    axes[0, 1].bar(range(128), A1, color='steelblue', edgecolor='black', linewidth=0.5, alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Neurone', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Activation', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title(f'âš¡ Activations des 128 Neurones', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Distribution des activations\n",
    "    axes[1, 0].hist(A1, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='ZÃ©ro (ReLU coupure)')\n",
    "    axes[1, 0].set_xlabel('Valeur d\\'activation', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('FrÃ©quence', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('ğŸ“Š Distribution des Activations', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Statistiques\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    active_neurons = np.sum(A1 > 0)\n",
    "    inactive_neurons = np.sum(A1 == 0)\n",
    "    stats_text = f\"\"\"\n",
    "    ğŸ“ˆ STATISTIQUES DE LA COUCHE CACHÃ‰E\n",
    "    {'='*50}\n",
    "    \n",
    "    ğŸ”¹ Neurones actifs: {active_neurons} / 128 ({active_neurons/128*100:.1f}%)\n",
    "    ğŸ”¹ Neurones inactifs: {inactive_neurons} / 128 ({inactive_neurons/128*100:.1f}%)\n",
    "    \n",
    "    ğŸ”¹ Valeurs:\n",
    "       â€¢ Minimum: {A1.min():.4f}\n",
    "       â€¢ Maximum: {A1.max():.4f}\n",
    "       â€¢ Moyenne: {A1.mean():.4f}\n",
    "       â€¢ MÃ©diane: {np.median(A1):.4f}\n",
    "       â€¢ Ã‰cart-type: {A1.std():.4f}\n",
    "    \n",
    "    ğŸ”¹ Top 5 neurones les plus actifs:\n",
    "    \"\"\"\n",
    "    \n",
    "    top5_indices = np.argsort(A1)[::-1][:5]\n",
    "    for i, idx in enumerate(top5_indices, 1):\n",
    "        stats_text += f\"\\n       {i}. Neurone {idx}: {A1[idx]:.4f}\"\n",
    "    \n",
    "    stats_text += \"\\n\\n    ğŸ’¡ Note: Ces activations sont alÃ©atoires car\"\n",
    "    stats_text += \"\\n       le rÃ©seau n'est pas encore entraÃ®nÃ© !\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, verticalalignment='center', family='monospace',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualiser pour un exemple alÃ©atoire\n",
    "random_idx = np.random.randint(0, 100)\n",
    "visualize_hidden_layer_activity(X_test, y_test, params, random_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ RÃ©capitulatif\n",
    "\n",
    "Dans ce notebook, nous avons appris la **Forward Propagation** en profondeur !\n",
    "\n",
    "### âœ… Ce que nous avons couvert\n",
    "\n",
    "1. **Les opÃ©rations mathÃ©matiques** :\n",
    "   - Multiplication matricielle: `Z = X Â· W + b`\n",
    "   - Fonctions d'activation: ReLU et Softmax\n",
    "\n",
    "2. **L'initialisation des poids** :\n",
    "   - He initialization pour ReLU\n",
    "   - Importance de la bonne initialisation\n",
    "\n",
    "3. **L'implÃ©mentation complÃ¨te** :\n",
    "   - Forward propagation Ã©tape par Ã©tape\n",
    "   - Test avec de vraies donnÃ©es MNIST\n",
    "\n",
    "4. **La visualisation** :\n",
    "   - PrÃ©dictions du rÃ©seau\n",
    "   - ActivitÃ© des neurones cachÃ©s\n",
    "   - Distribution des activations\n",
    "\n",
    "### ğŸ§  Points clÃ©s\n",
    "\n",
    "- ğŸ“Š **Forward propagation** = comment le rÃ©seau fait des prÃ©dictions\n",
    "- ğŸ² **Poids alÃ©atoires** = prÃ©dictions alÃ©atoires (~10% accuracy)\n",
    "- ğŸ“ˆ **ReLU** met Ã  zÃ©ro ~50% des neurones (c'est normal !)\n",
    "- ğŸ¯ **Softmax** transforme les scores en probabilitÃ©s (somme = 1)\n",
    "\n",
    "### ğŸ’¡ Prochaine Ã‰tape\n",
    "\n",
    "Maintenant nous savons comment le rÃ©seau fait des prÃ©dictions.\n",
    "Mais comment **apprend-il** ?\n",
    "\n",
    "**â¡ï¸ Prochain notebook: `03_backpropagation.ipynb`**\n",
    "\n",
    "Nous allons dÃ©couvrir la **backpropagation** - l'algorithme qui permet au rÃ©seau d'apprendre ! ğŸ“\n",
    "\n",
    "---\n",
    "\n",
    "**Super travail ! Continue comme Ã§a ! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
