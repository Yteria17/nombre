{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚¨ÖÔ∏è Backpropagation - Comment un R√©seau Apprend\n",
    "\n",
    "Bienvenue dans le notebook le plus important : la **backpropagation** !\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "1. **Comprendre l'intuition** derri√®re la backpropagation\n",
    "2. **Voir les math√©matiques** (mais simplement !)\n",
    "3. **Impl√©menter** l'algorithme √©tape par √©tape\n",
    "4. **Visualiser** comment le r√©seau s'am√©liore\n",
    "\n",
    "---\n",
    "\n",
    "## üí° L'Intuition en 3 Questions\n",
    "\n",
    "### 1. Le r√©seau fait une erreur. Pourquoi ?\n",
    "**R√©ponse** : Parce que les poids ne sont pas bons.\n",
    "\n",
    "### 2. Comment savoir quels poids sont responsables ?\n",
    "**R√©ponse** : En propageant l'erreur **en arri√®re** dans le r√©seau.\n",
    "\n",
    "### 3. Comment corriger les poids ?\n",
    "**R√©ponse** : En les ajustant dans la direction qui **r√©duit l'erreur**.\n",
    "\n",
    "C'est √ßa, la backpropagation ! üéì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Pr√™t √† apprendre la backpropagation !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Analogie : Apprendre √† Tirer au But\n",
    "\n",
    "Imagine que tu apprends √† tirer au football :\n",
    "\n",
    "1. üéØ **Tu tires** ‚Üí Le ballon va √† gauche du but\n",
    "2. ü§î **Tu analyses** ‚Üí \"J'ai tir√© trop √† gauche\"\n",
    "3. üîß **Tu ajustes** ‚Üí \"La prochaine fois, je vise plus √† droite\"\n",
    "4. üîÑ **Tu recommences** ‚Üí Tu t'am√©liores !\n",
    "\n",
    "### La backpropagation fait EXACTEMENT pareil :\n",
    "\n",
    "1. üéØ **Forward pass** ‚Üí Le r√©seau fait une pr√©diction\n",
    "2. üìä **Calcul de l'erreur** ‚Üí \"Voil√† √† quel point je me suis tromp√©\"\n",
    "3. ‚¨ÖÔ∏è **Backward pass** ‚Üí \"Ces poids sont responsables de l'erreur\"\n",
    "4. üîß **Mise √† jour** ‚Üí \"Je les ajuste pour faire mieux\"\n",
    "5. üîÑ **R√©p√®te** ‚Üí Le r√©seau s'am√©liore !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ La Fonction de Co√ªt (Loss Function)\n",
    "\n",
    "Avant de corriger, il faut **mesurer l'erreur** !\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "Pour la classification, on utilise la **cross-entropy** :\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=0}^{9} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "O√π :\n",
    "- $y$ = vrai label (one-hot encoded)\n",
    "- $\\hat{y}$ = pr√©diction (probabilit√©s)\n",
    "\n",
    "### Pourquoi cette formule ?\n",
    "\n",
    "- Si pr√©diction **correcte** (ex: 95% sur la bonne classe) ‚Üí Loss **petite** ‚úÖ\n",
    "- Si pr√©diction **incorrecte** (ex: 5% sur la bonne classe) ‚Üí Loss **grande** ‚ùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcule la cross-entropy loss\n",
    "    \n",
    "    Args:\n",
    "        y_true: vrais labels (one-hot encoded) - shape: (n_samples, n_classes)\n",
    "        y_pred: pr√©dictions (probabilit√©s) - shape: (n_samples, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "        loss: erreur moyenne\n",
    "    \"\"\"\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # √âviter log(0) en ajoutant une petite valeur\n",
    "    epsilon = 1e-7\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Cross-entropy\n",
    "    loss = -np.sum(y_true * np.log(y_pred)) / n_samples\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def one_hot_encode(y, n_classes=10):\n",
    "    \"\"\"\n",
    "    Transforme les labels en one-hot encoding\n",
    "    \n",
    "    Exemple: 3 ‚Üí [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((y.shape[0], n_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Test\n",
    "print(\"üß™ Test de la fonction de co√ªt:\\n\")\n",
    "\n",
    "# Cas 1: Bonne pr√©diction\n",
    "y_true_1 = one_hot_encode(np.array([3]))\n",
    "y_pred_1 = np.array([[0.01, 0.01, 0.01, 0.95, 0.01, 0.01, 0.00, 0.00, 0.00, 0.00]])\n",
    "loss_1 = cross_entropy_loss(y_true_1, y_pred_1)\n",
    "print(f\"‚úÖ Bonne pr√©diction (95% sur classe 3):\")\n",
    "print(f\"   Loss = {loss_1:.4f} (petite!)\\n\")\n",
    "\n",
    "# Cas 2: Mauvaise pr√©diction\n",
    "y_true_2 = one_hot_encode(np.array([3]))\n",
    "y_pred_2 = np.array([[0.05, 0.05, 0.05, 0.10, 0.25, 0.15, 0.10, 0.10, 0.10, 0.05]])\n",
    "loss_2 = cross_entropy_loss(y_true_2, y_pred_2)\n",
    "print(f\"‚ùå Mauvaise pr√©diction (10% sur classe 3):\")\n",
    "print(f\"   Loss = {loss_2:.4f} (grande!)\\n\")\n",
    "\n",
    "print(f\"üí° La loss est {loss_2/loss_1:.1f}x plus grande quand la pr√©diction est mauvaise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualisons la Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss_function():\n",
    "    \"\"\"\n",
    "    Montre comment varie la loss selon la confiance de la pr√©diction\n",
    "    \"\"\"\n",
    "    # Probabilit√©s pr√©dites pour la bonne classe\n",
    "    probabilities = np.linspace(0.01, 0.99, 100)\n",
    "    losses = -np.log(probabilities)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Graphique 1: Loss vs Probabilit√©\n",
    "    ax1.plot(probabilities, losses, linewidth=3, color='#e74c3c')\n",
    "    ax1.fill_between(probabilities, 0, losses, alpha=0.3, color='#e74c3c')\n",
    "    ax1.set_xlabel('Probabilit√© pr√©dite pour la VRAIE classe', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss (Cross-Entropy)', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('üìà Comment la Loss P√©nalise les Mauvaises Pr√©dictions', \n",
    "                 fontsize=15, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotations\n",
    "    ax1.annotate('Tr√®s mauvaise\\npr√©diction\\n(Loss √©lev√©e)', \n",
    "                xy=(0.1, -np.log(0.1)), xytext=(0.3, 3),\n",
    "                fontsize=11, ha='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "    \n",
    "    ax1.annotate('Bonne\\npr√©diction\\n(Loss faible)', \n",
    "                xy=(0.9, -np.log(0.9)), xytext=(0.7, 1),\n",
    "                fontsize=11, ha='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='green', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n",
    "    \n",
    "    # Graphique 2: Exemples concrets\n",
    "    scenarios = ['Pr√©diction\\nparfaite\\n(99%)', 'Bonne\\npr√©diction\\n(80%)', \n",
    "                'Pr√©diction\\nmoyenne\\n(50%)', 'Mauvaise\\npr√©diction\\n(20%)', \n",
    "                'Tr√®s mauvaise\\npr√©diction\\n(5%)']\n",
    "    probs = [0.99, 0.80, 0.50, 0.20, 0.05]\n",
    "    colors_bar = ['darkgreen', 'lightgreen', 'yellow', 'orange', 'red']\n",
    "    losses_scenarios = [-np.log(p) for p in probs]\n",
    "    \n",
    "    bars = ax2.bar(range(5), losses_scenarios, color=colors_bar, \n",
    "                   edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax2.set_xlabel('Sc√©nario', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('üéØ Loss pour Diff√©rents Sc√©narios', fontsize=15, fontweight='bold')\n",
    "    ax2.set_xticks(range(5))\n",
    "    ax2.set_xticklabels(scenarios, fontsize=10)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Valeurs sur les barres\n",
    "    for i, (bar, loss, prob) in enumerate(zip(bars, losses_scenarios, probs)):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{loss:.3f}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° √Ä retenir:\")\n",
    "    print(\"   ‚Ä¢ Plus la pr√©diction est confiante et CORRECTE ‚Üí Loss faible\")\n",
    "    print(\"   ‚Ä¢ Plus la pr√©diction est mauvaise ‚Üí Loss √©lev√©e\")\n",
    "    print(\"   ‚Ä¢ La loss 'explose' quand la pr√©diction est tr√®s mauvaise\")\n",
    "    print(\"   ‚Ä¢ L'objectif de l'entra√Ænement: MINIMISER cette loss !\")\n",
    "\n",
    "visualize_loss_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Le Gradient : La Direction de la Descente\n",
    "\n",
    "### üéø Analogie : Descendre une Montagne\n",
    "\n",
    "Imagine que tu es sur une montagne dans le brouillard :\n",
    "- ‚ùì Tu ne vois pas o√π est le bas\n",
    "- üîç Mais tu peux sentir la pente sous tes pieds\n",
    "- üö∂ Tu marches dans la direction de la **plus grande pente**\n",
    "- üéØ Petit √† petit, tu arrives en bas !\n",
    "\n",
    "### C'est exactement la descente de gradient !\n",
    "\n",
    "- üèîÔ∏è **Montagne** = Surface de la loss (fonction √† minimiser)\n",
    "- üìç **Ta position** = Valeurs actuelles des poids\n",
    "- üìê **La pente** = Le **gradient** (d√©riv√©e de la loss)\n",
    "- üö∂ **Marcher** = Ajuster les poids\n",
    "- üéØ **En bas** = Loss minimale = bon r√©seau !\n",
    "\n",
    "### Formule de mise √† jour :\n",
    "\n",
    "$$\n",
    "W_{\\text{nouveau}} = W_{\\text{ancien}} - \\alpha \\cdot \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "O√π :\n",
    "- $\\alpha$ = **learning rate** (taille du pas)\n",
    "- $\\frac{\\partial L}{\\partial W}$ = **gradient** (direction de la pente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_descent():\n",
    "    \"\"\"\n",
    "    Visualise la descente de gradient sur une fonction simple\n",
    "    \"\"\"\n",
    "    # Fonction simple: f(x) = x^2\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = x ** 2\n",
    "    \n",
    "    # Simulation de descente de gradient\n",
    "    learning_rates = [0.1, 0.3, 0.5]\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('üéø Descente de Gradient avec Diff√©rents Learning Rates', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    \n",
    "    for ax, lr, color in zip(axes, learning_rates, colors):\n",
    "        # Tracer la fonction\n",
    "        ax.plot(x, y, 'k-', linewidth=2, alpha=0.3, label='Loss = x¬≤')\n",
    "        \n",
    "        # Descente de gradient\n",
    "        x_current = 2.5  # Point de d√©part\n",
    "        trajectory_x = [x_current]\n",
    "        trajectory_y = [x_current ** 2]\n",
    "        \n",
    "        for i in range(15):\n",
    "            gradient = 2 * x_current  # D√©riv√©e de x^2 = 2x\n",
    "            x_current = x_current - lr * gradient\n",
    "            trajectory_x.append(x_current)\n",
    "            trajectory_y.append(x_current ** 2)\n",
    "        \n",
    "        # Tracer la trajectoire\n",
    "        ax.plot(trajectory_x, trajectory_y, 'o-', color=color, \n",
    "               linewidth=2, markersize=8, label=f'Trajectoire (Œ±={lr})')\n",
    "        \n",
    "        # Point de d√©part et d'arriv√©e\n",
    "        ax.plot(trajectory_x[0], trajectory_y[0], 'ro', markersize=15, \n",
    "               label='D√©part', zorder=5)\n",
    "        ax.plot(trajectory_x[-1], trajectory_y[-1], 'go', markersize=15, \n",
    "               label='Arriv√©e', zorder=5)\n",
    "        \n",
    "        ax.set_xlabel('Poids (W)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'Learning Rate Œ± = {lr}', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(-0.5, 7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéì Observations:\")\n",
    "    print(\"\\n   üìâ Learning Rate Œ± = 0.1 (petit):\")\n",
    "    print(\"      ‚Ä¢ Progression lente mais s√ªre\")\n",
    "    print(\"      ‚Ä¢ Converge doucement vers le minimum\")\n",
    "    print(\"      ‚úÖ S√ªr mais peut √™tre lent\\n\")\n",
    "    \n",
    "    print(\"   üìâ Learning Rate Œ± = 0.3 (moyen):\")\n",
    "    print(\"      ‚Ä¢ Progression rapide\")\n",
    "    print(\"      ‚Ä¢ Converge efficacement\")\n",
    "    print(\"      ‚úÖ Bon √©quilibre!\\n\")\n",
    "    \n",
    "    print(\"   üìâ Learning Rate Œ± = 0.5 (grand):\")\n",
    "    print(\"      ‚Ä¢ Progression tr√®s rapide\")\n",
    "    print(\"      ‚Ä¢ Peut osciller autour du minimum\")\n",
    "    print(\"      ‚ö†Ô∏è Risque d'instabilit√©\\n\")\n",
    "    \n",
    "    print(\"üí° Conclusion: Le learning rate doit √™tre bien choisi!\")\n",
    "\n",
    "visualize_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Backpropagation : Les Math√©matiques\n",
    "\n",
    "Maintenant, voyons comment calculer ces gradients !\n",
    "\n",
    "### Rappel de notre r√©seau :\n",
    "\n",
    "```\n",
    "Forward:\n",
    "  X ‚Üí [W1, b1, ReLU] ‚Üí A1 ‚Üí [W2, b2, Softmax] ‚Üí A2 ‚Üí Loss\n",
    "\n",
    "Backward (on inverse!):\n",
    "  ‚àÇLoss/‚àÇW1 ‚Üê [backprop] ‚Üê ‚àÇLoss/‚àÇA1 ‚Üê [backprop] ‚Üê ‚àÇLoss/‚àÇA2 ‚Üê Loss\n",
    "```\n",
    "\n",
    "### Les gradients √† calculer :\n",
    "\n",
    "#### Couche de sortie (Layer 2) :\n",
    "$$\\frac{\\partial L}{\\partial Z_2} = A_2 - Y \\quad \\text{(pour Softmax + Cross-Entropy)}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_2} = A_1^T \\cdot \\frac{\\partial L}{\\partial Z_2}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_2} = \\sum \\frac{\\partial L}{\\partial Z_2}$$\n",
    "\n",
    "#### Couche cach√©e (Layer 1) :\n",
    "$$\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} \\cdot W_2^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\text{ReLU}'(Z_1) \\quad \\text{(o√π ReLU' = 1 si Z>0, 0 sinon)}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_1} = X^T \\cdot \\frac{\\partial L}{\\partial Z_1}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_1} = \\sum \\frac{\\partial L}{\\partial Z_1}$$\n",
    "\n",
    "### üí° Pas de panique !\n",
    "Ces formules peuvent sembler compliqu√©es, mais ce sont juste des multiplications matricielles ! \n",
    "Regardons le code, c'est plus clair :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D'abord, r√©cup√©rons les fonctions du notebook pr√©c√©dent\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1 = parameters['W1'], parameters['b1']\n",
    "    W2, b2 = parameters['W2'], parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    return A2, cache\n",
    "\n",
    "def backward_propagation(X, Y, parameters, cache, verbose=False):\n",
    "    \"\"\"\n",
    "    Calcule les gradients par backpropagation\n",
    "    \n",
    "    Args:\n",
    "        X: donn√©es d'entr√©e (n_samples, 784)\n",
    "        Y: vrais labels (n_samples, 10) - one-hot encoded\n",
    "        parameters: poids et biais actuels\n",
    "        cache: valeurs de la forward propagation\n",
    "        verbose: afficher les d√©tails\n",
    "    \n",
    "    Returns:\n",
    "        gradients: dictionnaire avec dW1, db1, dW2, db2\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # R√©cup√©rer les valeurs du cache\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    Z1 = cache['Z1']\n",
    "    \n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚¨ÖÔ∏è BACKPROPAGATION - √âTAPE PAR √âTAPE\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # ===== COUCHE 2 (Output) =====\n",
    "    # Gradient de la loss par rapport √† Z2\n",
    "    # Pour Softmax + Cross-Entropy, c'est super simple!\n",
    "    dZ2 = A2 - Y\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüî∏ Layer 2 - Gradient par rapport √† Z2:\")\n",
    "        print(f\"   dZ2 = A2 - Y (simplifi√© pour Softmax+CrossEntropy)\")\n",
    "        print(f\"   dZ2.shape = {dZ2.shape}\")\n",
    "        print(f\"   dZ2 range: [{dZ2.min():.3f}, {dZ2.max():.3f}]\")\n",
    "    \n",
    "    # Gradient par rapport √† W2 et b2\n",
    "    dW2 = np.dot(A1.T, dZ2) / n_samples\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / n_samples\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüî∏ Layer 2 - Gradients des param√®tres:\")\n",
    "        print(f\"   dW2 = A1.T ¬∑ dZ2 / n_samples\")\n",
    "        print(f\"   dW2.shape = {dW2.shape}\")\n",
    "        print(f\"   db2.shape = {db2.shape}\")\n",
    "    \n",
    "    # ===== COUCHE 1 (Hidden) =====\n",
    "    # Propager le gradient vers A1\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüî∏ Layer 1 - Propagation du gradient:\")\n",
    "        print(f\"   dA1 = dZ2 ¬∑ W2.T\")\n",
    "        print(f\"   dA1.shape = {dA1.shape}\")\n",
    "    \n",
    "    # Gradient √† travers ReLU\n",
    "    # D√©riv√©e de ReLU: 1 si Z1 > 0, 0 sinon\n",
    "    dZ1 = dA1 * (Z1 > 0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüî∏ Layer 1 - Gradient √† travers ReLU:\")\n",
    "        print(f\"   dZ1 = dA1 ‚äô ReLU'(Z1)\")\n",
    "        print(f\"   dZ1.shape = {dZ1.shape}\")\n",
    "        active = np.sum(Z1 > 0)\n",
    "        print(f\"   Gradients non-nuls: {active} / {Z1.size} ({active/Z1.size*100:.1f}%)\")\n",
    "    \n",
    "    # Gradient par rapport √† W1 et b1\n",
    "    dW1 = np.dot(X.T, dZ1) / n_samples\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / n_samples\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüî∏ Layer 1 - Gradients des param√®tres:\")\n",
    "        print(f\"   dW1 = X.T ¬∑ dZ1 / n_samples\")\n",
    "        print(f\"   dW1.shape = {dW1.shape}\")\n",
    "        print(f\"   db1.shape = {db1.shape}\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ BACKPROPAGATION COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    gradients = {\n",
    "        'dW1': dW1, 'db1': db1,\n",
    "        'dW2': dW2, 'db2': db2\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "print(\"‚úÖ Fonction backward_propagation cr√©√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Test de la Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un mini r√©seau\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l-1], layer_dims[l]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters[f'b{l}'] = np.zeros((1, layer_dims[l]))\n",
    "    return parameters\n",
    "\n",
    "# Charger quelques donn√©es\n",
    "from src.utils import load_mnist_data\n",
    "X_train, y_train, X_test, y_test = load_mnist_data()\n",
    "\n",
    "# Prendre un mini-batch\n",
    "batch_size = 5\n",
    "X_batch = X_train[:batch_size]\n",
    "Y_batch = one_hot_encode(y_train[:batch_size])\n",
    "\n",
    "# Initialiser le r√©seau\n",
    "layer_dims = [784, 128, 10]\n",
    "params = initialize_parameters(layer_dims)\n",
    "\n",
    "# Forward pass\n",
    "print(\"\\nüöÄ FORWARD PASS\")\n",
    "predictions, cache = forward_propagation(X_batch, params)\n",
    "loss = cross_entropy_loss(Y_batch, predictions)\n",
    "print(f\"\\nüìä Loss avant backprop: {loss:.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "gradients = backward_propagation(X_batch, Y_batch, params, cache, verbose=True)\n",
    "\n",
    "# Afficher les gradients\n",
    "print(\"\\n\\nüìä MAGNITUDES DES GRADIENTS:\")\n",
    "print(\"=\"*70)\n",
    "for key, grad in gradients.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"   Shape: {grad.shape}\")\n",
    "    print(f\"   Mean: {np.mean(np.abs(grad)):.6f}\")\n",
    "    print(f\"   Max: {np.max(np.abs(grad)):.6f}\")\n",
    "    print(f\"   Min: {np.min(np.abs(grad)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Mise √† Jour des Poids\n",
    "\n",
    "Maintenant qu'on a les gradients, on peut mettre √† jour les poids !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Met √† jour les param√®tres avec la descente de gradient\n",
    "    \n",
    "    Formule: W = W - Œ± * dW\n",
    "    \"\"\"\n",
    "    parameters_updated = {}\n",
    "    \n",
    "    for key in parameters.keys():\n",
    "        grad_key = 'd' + key\n",
    "        parameters_updated[key] = parameters[key] - learning_rate * gradients[grad_key]\n",
    "    \n",
    "    return parameters_updated\n",
    "\n",
    "# Test\n",
    "learning_rate = 0.01\n",
    "params_before = {k: v.copy() for k, v in params.items()}\n",
    "params_updated = update_parameters(params, gradients, learning_rate)\n",
    "\n",
    "print(\"\\nüîß MISE √Ä JOUR DES POIDS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLearning rate: {learning_rate}\")\n",
    "\n",
    "# Calculer les changements\n",
    "for key in params_before.keys():\n",
    "    change = params_updated[key] - params_before[key]\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"   Changement moyen: {np.mean(np.abs(change)):.6f}\")\n",
    "    print(f\"   Changement max: {np.max(np.abs(change)):.6f}\")\n",
    "\n",
    "# V√©rifier si la loss a diminu√©\n",
    "predictions_after, _ = forward_propagation(X_batch, params_updated)\n",
    "loss_after = cross_entropy_loss(Y_batch, predictions_after)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä R√âSULTAT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLoss avant:  {loss:.6f}\")\n",
    "print(f\"Loss apr√®s:  {loss_after:.6f}\")\n",
    "print(f\"Diff√©rence:  {loss - loss_after:.6f}\")\n",
    "\n",
    "if loss_after < loss:\n",
    "    print(\"\\n‚úÖ SUCCESS! La loss a diminu√©! Le r√©seau s'am√©liore!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è La loss a augment√© (peut arriver avec un batch trop petit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Cycle Complet d'Entra√Ænement\n",
    "\n",
    "Mettons tout ensemble pour voir l'entra√Ænement en action !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(X, Y, parameters, learning_rate, batch_size=32):\n",
    "    \"\"\"\n",
    "    Entra√Æne le r√©seau sur une √©poque compl√®te\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        # Mini-batch\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X[start:end]\n",
    "        Y_batch = Y[start:end]\n",
    "        \n",
    "        # Forward\n",
    "        predictions, cache = forward_propagation(X_batch, parameters)\n",
    "        \n",
    "        # Loss\n",
    "        loss = cross_entropy_loss(Y_batch, predictions)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Backward\n",
    "        gradients = backward_propagation(X_batch, Y_batch, parameters, cache)\n",
    "        \n",
    "        # Update\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    avg_loss = total_loss / n_batches\n",
    "    return parameters, avg_loss\n",
    "\n",
    "def compute_accuracy(X, y, parameters):\n",
    "    \"\"\"\n",
    "    Calcule l'accuracy du r√©seau\n",
    "    \"\"\"\n",
    "    predictions, _ = forward_propagation(X, parameters)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == y)\n",
    "    return accuracy\n",
    "\n",
    "# Entra√Æner sur quelques √©poques\n",
    "print(\"\\nüéì ENTRA√éNEMENT DU R√âSEAU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Param√®tres\n",
    "n_epochs = 5\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "# Utiliser un sous-ensemble pour aller plus vite\n",
    "n_train = 10000\n",
    "X_train_subset = X_train[:n_train]\n",
    "y_train_subset = y_train[:n_train]\n",
    "Y_train_one_hot = one_hot_encode(y_train_subset)\n",
    "\n",
    "# R√©initialiser le r√©seau\n",
    "params = initialize_parameters([784, 128, 10])\n",
    "\n",
    "# Stocker l'historique\n",
    "history = {'loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  ‚Ä¢ √âpoques: {n_epochs}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {learning_rate}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {batch_size}\")\n",
    "print(f\"  ‚Ä¢ Exemples d'entra√Ænement: {n_train:,}\")\n",
    "print(f\"\\nD√©but de l'entra√Ænement...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Entra√Æner\n",
    "    params, avg_loss = train_one_epoch(X_train_subset, Y_train_one_hot, \n",
    "                                       params, learning_rate, batch_size)\n",
    "    \n",
    "    # Calculer l'accuracy\n",
    "    train_acc = compute_accuracy(X_train_subset, y_train_subset, params)\n",
    "    test_acc = compute_accuracy(X_test[:1000], y_test[:1000], params)\n",
    "    \n",
    "    # Sauvegarder\n",
    "    history['loss'].append(avg_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    # Afficher\n",
    "    print(f\"√âpoque {epoch+1}/{n_epochs} - \"\n",
    "          f\"Loss: {avg_loss:.4f} - \"\n",
    "          f\"Train Acc: {train_acc:.2%} - \"\n",
    "          f\"Test Acc: {test_acc:.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Visualisons l'Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Visualise l'√©volution de la loss et de l'accuracy\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('üìà √âvolution de l\\'Entra√Ænement', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(epochs, history['loss'], 'o-', linewidth=3, markersize=10, \n",
    "            color='#e74c3c', label='Loss')\n",
    "    ax1.set_xlabel('√âpoque', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('üìâ √âvolution de la Loss', fontsize=15, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(fontsize=12)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax2.plot(epochs, [acc*100 for acc in history['train_acc']], \n",
    "            'o-', linewidth=3, markersize=10, color='#2ecc71', label='Train Accuracy')\n",
    "    ax2.plot(epochs, [acc*100 for acc in history['test_acc']], \n",
    "            's-', linewidth=3, markersize=10, color='#3498db', label='Test Accuracy')\n",
    "    ax2.set_xlabel('√âpoque', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('üìà √âvolution de l\\'Accuracy', fontsize=15, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(fontsize=12)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques finales\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä STATISTIQUES FINALES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüéØ Loss:\")\n",
    "    print(f\"   ‚Ä¢ Initiale: {history['loss'][0]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Finale: {history['loss'][-1]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ R√©duction: {(1 - history['loss'][-1]/history['loss'][0])*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ Train Accuracy:\")\n",
    "    print(f\"   ‚Ä¢ Initiale: {history['train_acc'][0]:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Finale: {history['train_acc'][-1]:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Am√©lioration: +{(history['train_acc'][-1] - history['train_acc'][0])*100:.1f} points\")\n",
    "    \n",
    "    print(f\"\\nüéØ Test Accuracy:\")\n",
    "    print(f\"   ‚Ä¢ Initiale: {history['test_acc'][0]:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Finale: {history['test_acc'][-1]:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Am√©lioration: +{(history['test_acc'][-1] - history['test_acc'][0])*100:.1f} points\")\n",
    "    \n",
    "    print(\"\\nüí° Le r√©seau a appris ! üéâ\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ R√©capitulatif\n",
    "\n",
    "F√©licitations ! Tu as compris la **backpropagation** ! üéâ\n",
    "\n",
    "### ‚úÖ Ce que nous avons appris\n",
    "\n",
    "1. **L'intuition** :\n",
    "   - Le r√©seau fait une erreur\n",
    "   - On propage cette erreur en arri√®re\n",
    "   - On ajuste les poids pour r√©duire l'erreur\n",
    "\n",
    "2. **La fonction de co√ªt** :\n",
    "   - Cross-entropy pour mesurer l'erreur\n",
    "   - Plus la pr√©diction est mauvaise, plus la loss est grande\n",
    "\n",
    "3. **Le gradient** :\n",
    "   - Direction pour minimiser la loss\n",
    "   - Descente de gradient pour optimiser\n",
    "   - Learning rate contr√¥le la taille des pas\n",
    "\n",
    "4. **L'impl√©mentation** :\n",
    "   - Calcul des gradients couche par couche\n",
    "   - Mise √† jour des poids: W = W - Œ±¬∑dW\n",
    "   - Le r√©seau s'am√©liore √† chaque it√©ration !\n",
    "\n",
    "### üß† Points cl√©s\n",
    "\n",
    "- üìä **Backprop** = algorithme pour calculer les gradients efficacement\n",
    "- üéØ **Loss** diminue ‚Üí le r√©seau s'am√©liore\n",
    "- üìà **Accuracy** augmente ‚Üí le r√©seau apprend !\n",
    "- üîß **Learning rate** : crucial pour la convergence\n",
    "\n",
    "### üöÄ Prochaine √âtape\n",
    "\n",
    "Maintenant qu'on comprend tout le processus, construisons un r√©seau complet from scratch !\n",
    "\n",
    "**‚û°Ô∏è Prochain notebook: `04_building_complete_network.ipynb`**\n",
    "\n",
    "On va cr√©er un syst√®me d'entra√Ænement complet avec :\n",
    "- Architecture modulaire\n",
    "- Entra√Ænement sur tout MNIST\n",
    "- Visualisation des r√©sultats\n",
    "- Analyse des performances\n",
    "\n",
    "---\n",
    "\n",
    "**Bravo pour avoir ma√Ætris√© la backpropagation ! C'est le concept le plus important en deep learning ! üèÜ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
