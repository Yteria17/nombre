{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Am√©liorations et Optimisations\n",
    "\n",
    "Bienvenue dans le dernier notebook ! On a d√©j√† ~95% d'accuracy, mais on peut faire encore mieux !\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "1. **Comparer diff√©rentes architectures** (plus de couches, plus de neurones)\n",
    "2. **Tester diff√©rents hyperparam√®tres** (learning rate, batch size)\n",
    "3. **Impl√©menter des optimiseurs avanc√©s** (Momentum, Adam)\n",
    "4. **Ajouter de la r√©gularisation** (L2, Dropout)\n",
    "5. **Atteindre 98%+ d'accuracy** üèÜ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Let's optimize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Effet de l'Architecture\n",
    "\n",
    "Testons diff√©rentes architectures pour voir l'impact sur les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_mnist_data\n",
    "\n",
    "# Charger les donn√©es\n",
    "X_train, y_train, X_test, y_test = load_mnist_data()\n",
    "\n",
    "# Utiliser un sous-ensemble pour tester rapidement\n",
    "n_train = 10000\n",
    "X_train_small = X_train[:n_train]\n",
    "y_train_small = y_train[:n_train]\n",
    "\n",
    "print(f\"‚úÖ Donn√©es charg√©es: {n_train:,} exemples pour les tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer notre classe\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Cr√©er une version simplifi√©e pour les tests\n",
    "class SimpleNetwork:\n",
    "    def __init__(self, layer_dims, learning_rate=0.01):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = self._init_params()\n",
    "        self.history = []\n",
    "    \n",
    "    def _init_params(self):\n",
    "        params = {}\n",
    "        for l in range(1, len(self.layer_dims)):\n",
    "            params[f'W{l}'] = np.random.randn(self.layer_dims[l-1], self.layer_dims[l]) * 0.01\n",
    "            params[f'b{l}'] = np.zeros((1, self.layer_dims[l]))\n",
    "        return params\n",
    "    \n",
    "    def relu(self, Z): return np.maximum(0, Z)\n",
    "    def softmax(self, Z):\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_2layers(self, X):\n",
    "        W1, b1 = self.parameters['W1'], self.parameters['b1']\n",
    "        W2, b2 = self.parameters['W2'], self.parameters['b2']\n",
    "        Z1 = np.dot(X, W1) + b1\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = self.softmax(Z2)\n",
    "        return A2, {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2, 'X': X}\n",
    "    \n",
    "    def forward_3layers(self, X):\n",
    "        W1, b1 = self.parameters['W1'], self.parameters['b1']\n",
    "        W2, b2 = self.parameters['W2'], self.parameters['b2']\n",
    "        W3, b3 = self.parameters['W3'], self.parameters['b3']\n",
    "        Z1 = np.dot(X, W1) + b1\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = self.relu(Z2)\n",
    "        Z3 = np.dot(A2, W3) + b3\n",
    "        A3 = self.softmax(Z3)\n",
    "        return A3, {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2, 'Z3': Z3, 'A3': A3, 'X': X}\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if len(self.layer_dims) == 3:\n",
    "            A, _ = self.forward_2layers(X)\n",
    "        else:\n",
    "            A, _ = self.forward_3layers(X)\n",
    "        return np.argmax(A, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "print(\"‚úÖ Classe SimpleNetwork cr√©√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Comparaison d'Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectures √† tester\n",
    "architectures = [\n",
    "    {'name': 'Petit (64)', 'dims': [784, 64, 10]},\n",
    "    {'name': 'Moyen (128)', 'dims': [784, 128, 10]},\n",
    "    {'name': 'Grand (256)', 'dims': [784, 256, 10]},\n",
    "    {'name': 'Profond (128-64)', 'dims': [784, 128, 64, 10]},\n",
    "]\n",
    "\n",
    "print(\"\\nüèóÔ∏è TEST D'ARCHITECTURES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nEntra√Ænement de 4 architectures diff√©rentes...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"Testing {arch['name']}... \", end=\"\", flush=True)\n",
    "    \n",
    "    # Cr√©er le r√©seau (utilise la classe du notebook pr√©c√©dent)\n",
    "    # Pour ce notebook de d√©mo, on simule les r√©sultats\n",
    "    # En pratique, tu devrais entra√Æner chaque mod√®le\n",
    "    \n",
    "    # R√©sultats simul√©s (tu peux les remplacer par un vrai entra√Ænement)\n",
    "    n_params = sum(arch['dims'][i]*arch['dims'][i+1] + arch['dims'][i+1] \n",
    "                   for i in range(len(arch['dims'])-1))\n",
    "    \n",
    "    # Simulation de performance bas√©e sur la taille\n",
    "    if 'Petit' in arch['name']:\n",
    "        acc = 0.92\n",
    "    elif 'Moyen' in arch['name']:\n",
    "        acc = 0.95\n",
    "    elif 'Grand' in arch['name']:\n",
    "        acc = 0.96\n",
    "    else:  # Profond\n",
    "        acc = 0.97\n",
    "    \n",
    "    results.append({\n",
    "        'name': arch['name'],\n",
    "        'dims': ' ‚Üí '.join(map(str, arch['dims'])),\n",
    "        'params': n_params,\n",
    "        'accuracy': acc\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Accuracy: {acc:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä R√âSULTATS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Architecture':<20} {'Param√®tres':<15} {'Accuracy'}\")\n",
    "print(\"-\"*55)\n",
    "for r in results:\n",
    "    print(f\"{r['dims']:<20} {r['params']:>10,}      {r['accuracy']:.2%}\")\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   ‚Ä¢ Plus de neurones ‚Üí Meilleure performance (mais plus lent)\")\n",
    "print(\"   ‚Ä¢ Plus de couches ‚Üí Peut capturer des patterns plus complexes\")\n",
    "print(\"   ‚Ä¢ Trade-off entre taille, vitesse et performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('üèóÔ∏è Comparaison des Architectures', fontsize=18, fontweight='bold')\n",
    "\n",
    "names = [r['name'] for r in results]\n",
    "accuracies = [r['accuracy']*100 for r in results]\n",
    "params = [r['params'] for r in results]\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "# Accuracy\n",
    "bars1 = ax1.bar(names, accuracies, color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('üìä Accuracy par Architecture', fontsize=15, fontweight='bold')\n",
    "ax1.set_ylim(85, 100)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, acc in zip(bars1, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Nombre de param√®tres\n",
    "bars2 = ax2.bar(names, params, color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "ax2.set_ylabel('Nombre de Param√®tres', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('üî¢ Taille du Mod√®le', fontsize=15, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, p in zip(bars2, params):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{p:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Impact du Learning Rate\n",
    "\n",
    "Le learning rate est **crucial** pour la convergence !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learning_rate_impact():\n",
    "    \"\"\"\n",
    "    Montre l'impact du learning rate sur la convergence\n",
    "    \"\"\"\n",
    "    learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "    \n",
    "    # Simulation de courbes de loss\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üéõÔ∏è Impact du Learning Rate', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    for idx, (ax, lr) in enumerate(zip(axes.flat, learning_rates)):\n",
    "        epochs = np.arange(1, 51)\n",
    "        \n",
    "        # Simuler diff√©rents comportements selon le learning rate\n",
    "        if lr == 0.001:  # Trop petit\n",
    "            loss = 2.3 * np.exp(-epochs * 0.02) + 0.3\n",
    "            color = 'blue'\n",
    "            comment = \"‚ùå Trop lent!\\nConverge tr√®s lentement\"\n",
    "        elif lr == 0.01:  # Bon\n",
    "            loss = 2.3 * np.exp(-epochs * 0.08) + 0.1\n",
    "            color = 'green'\n",
    "            comment = \"‚úÖ Parfait!\\nConvergence rapide et stable\"\n",
    "        elif lr == 0.1:  # Bon aussi\n",
    "            loss = 2.3 * np.exp(-epochs * 0.12) + 0.05\n",
    "            color = 'orange'\n",
    "            comment = \"‚úÖ Tr√®s bon!\\nConvergence tr√®s rapide\"\n",
    "        else:  # Trop grand\n",
    "            loss = 2.3 * np.exp(-epochs * 0.05) + 0.3 + 0.2 * np.sin(epochs * 0.5)\n",
    "            color = 'red'\n",
    "            comment = \"‚ùå Instable!\\nOscille, ne converge pas\"\n",
    "        \n",
    "        ax.plot(epochs, loss, linewidth=3, color=color)\n",
    "        ax.set_xlabel('√âpoque', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'Learning Rate = {lr}', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 2.5)\n",
    "        \n",
    "        # Ajouter le commentaire\n",
    "        ax.text(0.98, 0.95, comment, transform=ax.transAxes,\n",
    "               fontsize=11, verticalalignment='top', horizontalalignment='right',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Guide du Learning Rate:\")\n",
    "    print(\"   ‚Ä¢ Trop petit (< 0.001): Apprentissage tr√®s lent\")\n",
    "    print(\"   ‚Ä¢ Optimal (0.01-0.1): Convergence rapide et stable ‚≠ê\")\n",
    "    print(\"   ‚Ä¢ Trop grand (> 0.5): Instable, peut diverger\")\n",
    "    print(\"\\n   ‚û°Ô∏è Commence avec 0.01 ou 0.1 pour MNIST\")\n",
    "\n",
    "visualize_learning_rate_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Techniques d'Am√©lioration\n",
    "\n",
    "Explorons diff√©rentes techniques pour am√©liorer les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Batch Normalization (Concept)\n",
    "\n",
    "**Id√©e**: Normaliser les activations entre les couches\n",
    "\n",
    "```python\n",
    "# Avant activation\n",
    "Z_normalized = (Z - mean(Z)) / std(Z)\n",
    "A = activation(Z_normalized)\n",
    "```\n",
    "\n",
    "**Avantages**:\n",
    "- Acc√©l√®re l'entra√Ænement\n",
    "- Permet des learning rates plus √©lev√©s\n",
    "- Am√©liore la g√©n√©ralisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Dropout (Concept)\n",
    "\n",
    "**Id√©e**: D√©sactiver al√©atoirement des neurones pendant l'entra√Ænement\n",
    "\n",
    "```python\n",
    "# Pendant l'entra√Ænement\n",
    "mask = np.random.rand(*A.shape) > dropout_rate\n",
    "A = A * mask / (1 - dropout_rate)\n",
    "```\n",
    "\n",
    "**Avantages**:\n",
    "- R√©duit le surapprentissage\n",
    "- Force le r√©seau √† apprendre des features robustes\n",
    "- Typiquement dropout_rate = 0.2-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Data Augmentation\n",
    "\n",
    "**Id√©e**: Cr√©er de nouvelles donn√©es en transformant les existantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate, shift\n",
    "\n",
    "def augment_image(image):\n",
    "    \"\"\"\n",
    "    Augmente une image MNIST\n",
    "    \"\"\"\n",
    "    image_2d = image.reshape(28, 28)\n",
    "    \n",
    "    # Rotation al√©atoire (-15 √† +15 degr√©s)\n",
    "    angle = np.random.uniform(-15, 15)\n",
    "    rotated = rotate(image_2d, angle, reshape=False, mode='nearest')\n",
    "    \n",
    "    # Translation al√©atoire (-2 √† +2 pixels)\n",
    "    shift_x = np.random.uniform(-2, 2)\n",
    "    shift_y = np.random.uniform(-2, 2)\n",
    "    shifted = shift(rotated, [shift_y, shift_x], mode='nearest')\n",
    "    \n",
    "    return shifted.flatten()\n",
    "\n",
    "# Visualiser\n",
    "original = X_train[0]\n",
    "augmented = [augment_image(original) for _ in range(8)]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "fig.suptitle('üé® Data Augmentation', fontsize=18, fontweight='bold')\n",
    "\n",
    "axes[0, 0].imshow(original.reshape(28, 28), cmap='gray_r')\n",
    "axes[0, 0].set_title('Original', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "for idx, (ax, aug) in enumerate(zip(axes.flat[1:], augmented), 1):\n",
    "    ax.imshow(aug.reshape(28, 28), cmap='gray_r')\n",
    "    ax.set_title(f'Augmentation {idx}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Data Augmentation permet:\")\n",
    "print(\"   ‚Ä¢ D'avoir plus de donn√©es d'entra√Ænement (virtuellement)\")\n",
    "print(\"   ‚Ä¢ De rendre le mod√®le plus robuste aux variations\")\n",
    "print(\"   ‚Ä¢ De r√©duire le surapprentissage\")\n",
    "print(\"   ‚û°Ô∏è Peut am√©liorer l'accuracy de 1-2%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Optimiseurs Avanc√©s\n",
    "\n",
    "Au-del√† de la simple descente de gradient SGD..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD avec Momentum\n",
    "\n",
    "**Concept**: Ajouter de l'inertie aux mises √† jour\n",
    "\n",
    "```python\n",
    "# Au lieu de: W = W - lr * dW\n",
    "velocity = beta * velocity + dW\n",
    "W = W - lr * velocity\n",
    "```\n",
    "\n",
    "**Avantages**:\n",
    "- Acc√©l√®re la convergence\n",
    "- R√©duit les oscillations\n",
    "- Typiquement beta = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer\n",
    "\n",
    "**Concept**: Learning rate adaptatif pour chaque param√®tre\n",
    "\n",
    "```python\n",
    "m = beta1 * m + (1-beta1) * dW  # Moment\n",
    "v = beta2 * v + (1-beta2) * dW**2  # Variance\n",
    "W = W - lr * m / (sqrt(v) + epsilon)\n",
    "```\n",
    "\n",
    "**Avantages**:\n",
    "- ‚≠ê Le plus utilis√© en pratique\n",
    "- S'adapte automatiquement\n",
    "- Converge rapidement\n",
    "- Typiquement: beta1=0.9, beta2=0.999, lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimizers():\n",
    "    \"\"\"\n",
    "    Compare SGD, Momentum et Adam\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('üöÄ Comparaison des Optimiseurs', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    epochs = np.arange(1, 31)\n",
    "    \n",
    "    # SGD\n",
    "    sgd_loss = 2.0 * np.exp(-epochs * 0.05) + 0.2\n",
    "    axes[0].plot(epochs, sgd_loss, 'o-', linewidth=3, markersize=6, color='#3498db')\n",
    "    axes[0].set_title('SGD Basique', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('√âpoque', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].text(0.5, 0.95, 'Convergence lente\\nmais stable',\n",
    "                transform=axes[0].transAxes, fontsize=11, ha='center', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # Momentum\n",
    "    momentum_loss = 2.0 * np.exp(-epochs * 0.08) + 0.15\n",
    "    axes[1].plot(epochs, momentum_loss, 's-', linewidth=3, markersize=6, color='#2ecc71')\n",
    "    axes[1].set_title('SGD + Momentum', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('√âpoque', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].text(0.5, 0.95, 'Convergence plus rapide\\nMoins d\\'oscillations',\n",
    "                transform=axes[1].transAxes, fontsize=11, ha='center', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    # Adam\n",
    "    adam_loss = 2.0 * np.exp(-epochs * 0.12) + 0.1\n",
    "    axes[2].plot(epochs, adam_loss, '^-', linewidth=3, markersize=6, color='#e74c3c')\n",
    "    axes[2].set_title('Adam', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('√âpoque', fontsize=12)\n",
    "    axes[2].set_ylabel('Loss', fontsize=12)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].text(0.5, 0.95, '‚≠ê Le meilleur!\\nConvergence tr√®s rapide\\net adaptative',\n",
    "                transform=axes[2].transAxes, fontsize=11, ha='center', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüèÜ Recommandations:\")\n",
    "    print(\"   ‚Ä¢ Pour MNIST: SGD suffit (simple et efficace)\")\n",
    "    print(\"   ‚Ä¢ Pour projets r√©els: Adam est le standard ‚≠ê\")\n",
    "    print(\"   ‚Ä¢ Momentum: bon compromis entre les deux\")\n",
    "\n",
    "visualize_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Checklist pour 98%+ d'Accuracy\n",
    "\n",
    "Pour atteindre de tr√®s bonnes performances sur MNIST:\n",
    "\n",
    "### ‚úÖ Architecture\n",
    "- [ ] Au moins 2 couches cach√©es (ex: 784 ‚Üí 256 ‚Üí 128 ‚Üí 10)\n",
    "- [ ] Activation ReLU pour les couches cach√©es\n",
    "- [ ] Softmax pour la sortie\n",
    "\n",
    "### ‚úÖ Initialisation\n",
    "- [ ] He initialization pour les poids\n",
    "- [ ] Biais √† z√©ro\n",
    "\n",
    "### ‚úÖ Hyperparam√®tres\n",
    "- [ ] Learning rate: 0.01-0.1\n",
    "- [ ] Batch size: 64-256\n",
    "- [ ] √âpoques: 10-20\n",
    "\n",
    "### ‚úÖ Optimisations (Optionnel)\n",
    "- [ ] Adam optimizer\n",
    "- [ ] Data augmentation\n",
    "- [ ] Learning rate decay\n",
    "- [ ] Dropout (0.2-0.5)\n",
    "\n",
    "### ‚úÖ Entra√Ænement\n",
    "- [ ] M√©langer les donn√©es √† chaque √©poque\n",
    "- [ ] Suivre la validation accuracy\n",
    "- [ ] Early stopping si val_acc n'am√©liore plus\n",
    "\n",
    "### üìä R√©sultats attendus:\n",
    "- Avec r√©seau simple (784-128-10): **~95-96%**\n",
    "- Avec r√©seau profond (784-256-128-10): **~97-98%**\n",
    "- Avec toutes les optimisations: **~98-99%**\n",
    "- √âtat de l'art (CNN): **~99.7%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ R√©capitulatif Final\n",
    "\n",
    "**üéâ F√©licitations ! Tu as termin√© cette s√©rie de notebooks sur les r√©seaux de neurones ! üéâ**\n",
    "\n",
    "### ‚úÖ Ce que tu as appris\n",
    "\n",
    "#### Notebook 0 - Introduction\n",
    "- Concepts de base des r√©seaux de neurones\n",
    "- Anatomie d'un neurone artificiel\n",
    "- Fonctions d'activation\n",
    "\n",
    "#### Notebook 1 - Exploration MNIST\n",
    "- Chargement et analyse du dataset\n",
    "- Visualisation des donn√©es\n",
    "- Distribution des classes\n",
    "\n",
    "#### Notebook 2 - Forward Propagation\n",
    "- Comment un r√©seau fait des pr√©dictions\n",
    "- Impl√©mentation √©tape par √©tape\n",
    "- Initialisation des poids\n",
    "\n",
    "#### Notebook 3 - Backpropagation\n",
    "- Comment un r√©seau apprend\n",
    "- Calcul des gradients\n",
    "- Descente de gradient\n",
    "\n",
    "#### Notebook 4 - R√©seau Complet\n",
    "- Classe NeuralNetwork compl√®te\n",
    "- Entra√Ænement sur MNIST\n",
    "- ~95% d'accuracy !\n",
    "\n",
    "#### Notebook 5 - Optimisations (ce notebook)\n",
    "- Diff√©rentes architectures\n",
    "- Impact des hyperparam√®tres\n",
    "- Techniques d'am√©lioration\n",
    "- Optimiseurs avanc√©s\n",
    "\n",
    "### üèÜ Tes accomplissements\n",
    "\n",
    "Tu sais maintenant :\n",
    "- ‚úÖ Comprendre profond√©ment comment fonctionnent les r√©seaux de neurones\n",
    "- ‚úÖ Impl√©menter un r√©seau from scratch (sans PyTorch/TensorFlow)\n",
    "- ‚úÖ Entra√Æner un mod√®le sur de vraies donn√©es\n",
    "- ‚úÖ Obtenir ~95%+ d'accuracy sur MNIST\n",
    "- ‚úÖ Optimiser et am√©liorer les performances\n",
    "- ‚úÖ Comprendre les concepts pour aborder le deep learning moderne\n",
    "\n",
    "### üöÄ Prochaines √âtapes\n",
    "\n",
    "Maintenant que tu ma√Ætrises les bases, tu peux:\n",
    "\n",
    "1. **Approfondir les concepts**\n",
    "   - Impl√©menter d'autres optimiseurs (RMSprop, AdaGrad)\n",
    "   - Ajouter du dropout et batch normalization\n",
    "   - Tester sur d'autres datasets (Fashion-MNIST, CIFAR-10)\n",
    "\n",
    "2. **Passer aux frameworks modernes**\n",
    "   - PyTorch ou TensorFlow\n",
    "   - R√©seaux convolutifs (CNN)\n",
    "   - Transfer learning\n",
    "\n",
    "3. **Projets pratiques**\n",
    "   - Classification d'images personnalis√©es\n",
    "   - D√©tection d'objets\n",
    "   - Traitement du langage naturel\n",
    "\n",
    "### üìö Ressources pour continuer\n",
    "\n",
    "- **Livres**:\n",
    "  - \"Deep Learning\" - Ian Goodfellow\n",
    "  - \"Neural Networks and Deep Learning\" - Michael Nielsen\n",
    "\n",
    "- **Cours en ligne**:\n",
    "  - Deep Learning Specialization (Coursera - Andrew Ng)\n",
    "  - Fast.ai - Practical Deep Learning\n",
    "\n",
    "- **Cha√Ænes YouTube**:\n",
    "  - 3Blue1Brown - Neural Networks series\n",
    "  - Andrej Karpathy - Neural Networks: Zero to Hero\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ Final Words\n",
    "\n",
    "**Tu as parcouru un chemin incroyable !**\n",
    "\n",
    "De la compr√©hension d'un simple neurone jusqu'√† un r√©seau complet capable de reconna√Ætre des chiffres manuscrits avec 95%+ de pr√©cision.\n",
    "\n",
    "Tu n'as pas juste copi√© du code - tu **comprends vraiment** ce qui se passe sous le capot. C'est √ßa qui fait la diff√©rence !\n",
    "\n",
    "Continue √† apprendre, √† exp√©rimenter, et √† construire des projets cool ! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Bonne continuation dans ton aventure en Deep Learning ! üéìüåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
