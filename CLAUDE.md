# CLAUDE.md - Documentation du Projet de Reconnaissance de Chiffres

## ğŸ¯ Objectif du Projet

Ce projet est un exercice d'apprentissage pour comprendre en profondeur le fonctionnement des rÃ©seaux de neurones en implÃ©mentant un systÃ¨me de reconnaissance de chiffres manuscrits **from scratch** (sans utiliser PyTorch, TensorFlow ou Keras).

L'objectif est de comprendre :
- Comment fonctionne la propagation avant (forward propagation)
- Comment fonctionne la rÃ©tropropagation (backpropagation)
- Comment les rÃ©seaux de neurones apprennent Ã  partir de donnÃ©es
- Comment optimiser et amÃ©liorer les performances

## ğŸ“ Structure du Projet

```
nombre/
â”œâ”€â”€ CLAUDE.md              # Ce fichier - documentation dÃ©taillÃ©e
â”œâ”€â”€ README.md              # Vue d'ensemble du projet
â”œâ”€â”€ ARCHITECTURE.md        # Explications mathÃ©matiques dÃ©taillÃ©es
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”‚
â”œâ”€â”€ data/                  # Dataset MNIST
â”‚   â”œâ”€â”€ mnist_train.csv
â”‚   â””â”€â”€ mnist_test.csv
â”‚
â”œâ”€â”€ src/                   # Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ network.py         # Classe principale du rÃ©seau de neurones
â”‚   â”œâ”€â”€ layers.py          # ImplÃ©mentation des couches (Dense, etc.)
â”‚   â”œâ”€â”€ activations.py     # Fonctions d'activation (sigmoid, ReLU, softmax)
â”‚   â”œâ”€â”€ losses.py          # Fonctions de coÃ»t (MSE, cross-entropy)
â”‚   â”œâ”€â”€ optimizers.py      # Algorithmes d'optimisation (SGD, Adam)
â”‚   â”œâ”€â”€ utils.py           # Utilitaires (chargement donnÃ©es, normalisation)
â”‚   â”œâ”€â”€ visualize.py       # Fonctions de visualisation
â”‚   â””â”€â”€ metrics.py         # Calcul des mÃ©triques (accuracy, confusion matrix)
â”‚
â”œâ”€â”€ notebooks/             # Notebooks Jupyter pour expÃ©rimentation
â”‚   â”œâ”€â”€ 01_exploration.ipynb      # Exploration du dataset MNIST
â”‚   â”œâ”€â”€ 02_simple_network.ipynb   # RÃ©seau simple
â”‚   â””â”€â”€ 03_improvements.ipynb     # AmÃ©liorations et optimisations
â”‚
â”œâ”€â”€ models/                # ModÃ¨les entraÃ®nÃ©s sauvegardÃ©s
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ tests/                 # Tests unitaires
â”‚   â”œâ”€â”€ test_activations.py
â”‚   â”œâ”€â”€ test_layers.py
â”‚   â””â”€â”€ test_network.py
â”‚
â”œâ”€â”€ train.py               # Script d'entraÃ®nement principal
â”œâ”€â”€ evaluate.py            # Script d'Ã©valuation
â””â”€â”€ draw_interface.py      # Interface pour dessiner et tester
```

## ğŸ§  Concepts ImplÃ©mentÃ©s

### 1. Architecture du RÃ©seau de Neurones

#### Phase 1 : RÃ©seau Simple (MLP - Multi-Layer Perceptron)
```
Input Layer (784 neurones)
    â†“
Hidden Layer 1 (128 neurones) + ReLU
    â†“
Hidden Layer 2 (64 neurones) + ReLU
    â†“
Output Layer (10 neurones) + Softmax
```

#### Phase 2 : AmÃ©liorations
- Plus de couches cachÃ©es
- Dropout pour Ã©viter le surapprentissage
- Batch normalization
- DiffÃ©rentes fonctions d'activation

### 2. Fonctions d'Activation

| Fonction | Ã‰quation | Usage |
|----------|----------|-------|
| **Sigmoid** | Ïƒ(x) = 1/(1+e^(-x)) | Couches cachÃ©es (historique) |
| **ReLU** | f(x) = max(0, x) | Couches cachÃ©es (moderne) |
| **Softmax** | f(x)áµ¢ = e^(xáµ¢) / Î£e^(xâ±¼) | Couche de sortie (classification) |
| **Tanh** | f(x) = (e^x - e^(-x))/(e^x + e^(-x)) | Alternative Ã  sigmoid |

### 3. Fonctions de CoÃ»t

- **Cross-Entropy** : Pour la classification multi-classes
  ```
  L = -Î£ yáµ¢ log(Å·áµ¢)
  ```
- **MSE (Mean Squared Error)** : Alternative plus simple
  ```
  L = (1/n) Î£(y - Å·)Â²
  ```

### 4. Algorithmes d'Optimisation

- **SGD (Stochastic Gradient Descent)** : Basique
- **SGD avec Momentum** : AccÃ©lÃ©ration de la convergence
- **Adam** : Adaptatif et performant

## ğŸš€ FonctionnalitÃ©s

### âœ… FonctionnalitÃ©s Principales

1. **EntraÃ®nement du modÃ¨le**
   - Chargement automatique de MNIST
   - EntraÃ®nement avec diffÃ©rentes configurations
   - Sauvegarde des poids entraÃ®nÃ©s

2. **Ã‰valuation et mÃ©triques**
   - Accuracy globale et par classe
   - Matrice de confusion
   - Courbes d'apprentissage (loss/accuracy)

3. **Interface de test interactive**
   - Dessiner des chiffres Ã  la main
   - PrÃ©diction en temps rÃ©el
   - Affichage des probabilitÃ©s

### âœ… FonctionnalitÃ©s AvancÃ©es

4. **Visualisation**
   - Visualisation des poids de la premiÃ¨re couche
   - Exemples mal classifiÃ©s
   - Ã‰volution des mÃ©triques pendant l'entraÃ®nement

5. **Comparaison de modÃ¨les**
   - Tester diffÃ©rentes architectures
   - Comparer les performances
   - Sauvegarder les rÃ©sultats

6. **Data Augmentation**
   - Rotation lÃ©gÃ¨re
   - Translation
   - Zoom

7. **Tests unitaires**
   - Validation des calculs de gradient
   - Tests des fonctions d'activation
   - Tests de backpropagation

8. **Documentation explicative**
   - Notebooks avec explications pas Ã  pas
   - Commentaires dÃ©taillÃ©s dans le code
   - Explications mathÃ©matiques

## ğŸ“Š Dataset MNIST

- **Taille** : 60,000 images d'entraÃ®nement + 10,000 images de test
- **Format** : Images en niveaux de gris 28Ã—28 pixels
- **Classes** : Chiffres de 0 Ã  9
- **PrÃ©traitement** : Normalisation des pixels [0, 255] â†’ [0, 1]

## ğŸ”§ Utilisation

### Installation

```bash
# Cloner le dÃ©pÃ´t
git clone <repo-url>
cd nombre

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### EntraÃ®nement

```bash
# EntraÃ®nement avec configuration par dÃ©faut
python train.py

# EntraÃ®nement avec paramÃ¨tres personnalisÃ©s
python train.py --epochs 20 --batch-size 64 --learning-rate 0.001
```

### Ã‰valuation

```bash
# Ã‰valuer un modÃ¨le entraÃ®nÃ©
python evaluate.py --model-path models/best_model.pkl
```

### Interface de dessin

```bash
# Lancer l'interface graphique
python draw_interface.py
```

## ğŸ“ Apprentissage Progressif

### Ã‰tape 1 : Comprendre les bases
- Lire `ARCHITECTURE.md` pour comprendre les mathÃ©matiques
- Explorer le notebook `01_exploration.ipynb`
- Comprendre le dataset MNIST

### Ã‰tape 2 : ImplÃ©menter le rÃ©seau simple
- ImplÃ©menter les fonctions d'activation
- ImplÃ©menter la propagation avant
- ImplÃ©menter la rÃ©tropropagation
- Tester avec `02_simple_network.ipynb`

### Ã‰tape 3 : EntraÃ®ner et Ã©valuer
- EntraÃ®ner un premier modÃ¨le
- Analyser les rÃ©sultats
- Identifier les faiblesses

### Ã‰tape 4 : AmÃ©liorer
- Ajouter des couches
- Tester diffÃ©rentes fonctions d'activation
- Optimiser les hyperparamÃ¨tres
- ImplÃ©menter des techniques avancÃ©es

### Ã‰tape 5 : ExpÃ©rimenter
- Data augmentation
- Visualisation des poids
- Analyse des erreurs

## ğŸ› Debugging et Validation

### VÃ©rification du Gradient

Pour s'assurer que la backpropagation est correctement implÃ©mentÃ©e :

```python
# Gradient checking (comparaison numÃ©rique vs analytique)
python -m tests.test_network
```

### Validation de l'Apprentissage

Signes d'un apprentissage correct :
- âœ… Loss qui diminue progressivement
- âœ… Accuracy qui augmente sur le train et le test
- âœ… Pas de divergence (loss qui explose)

Signes de problÃ¨mes :
- âŒ Loss qui stagne immÃ©diatement â†’ Learning rate trop faible
- âŒ Loss qui explose â†’ Learning rate trop Ã©levÃ©
- âŒ Train accuracy Ã©levÃ©e mais test accuracy faible â†’ Surapprentissage

## ğŸ“ˆ RÃ©sultats Attendus

### RÃ©seau Simple (MLP)
- **Accuracy attendue** : ~95-97% sur le test set
- **Temps d'entraÃ®nement** : 5-10 minutes sur CPU

### RÃ©seau AmÃ©liorÃ©
- **Accuracy attendue** : ~98-99% sur le test set
- **Temps d'entraÃ®nement** : 10-20 minutes sur CPU

## ğŸ”¬ ExpÃ©rimentations SuggÃ©rÃ©es

1. **Impact du learning rate** : Tester 0.001, 0.01, 0.1
2. **Nombre de couches** : 1 vs 2 vs 3 couches cachÃ©es
3. **Taille des couches** : 32, 64, 128, 256 neurones
4. **Fonctions d'activation** : ReLU vs Sigmoid vs Tanh
5. **Batch size** : 16, 32, 64, 128
6. **Optimisateurs** : SGD vs Momentum vs Adam

## ğŸ“š Ressources ComplÃ©mentaires

- [ARCHITECTURE.md](./ARCHITECTURE.md) - DÃ©tails mathÃ©matiques
- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Michael Nielsen - Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [Andrew Ng - Machine Learning Course](https://www.coursera.org/learn/machine-learning)

## ğŸ¤ Contribution

Ce projet est un projet d'apprentissage personnel. Suggestions d'amÃ©liorations bienvenues !

## ğŸ“ Notes de DÃ©veloppement

### Version 1.0 - RÃ©seau Simple
- [ ] ImplÃ©mentation basique du MLP
- [ ] Fonctions d'activation (sigmoid, ReLU, softmax)
- [ ] Backpropagation
- [ ] EntraÃ®nement sur MNIST
- [ ] Ã‰valuation basique

### Version 2.0 - AmÃ©liorations
- [ ] Interface de dessin
- [ ] Visualisations avancÃ©es
- [ ] Sauvegarde/chargement des modÃ¨les
- [ ] Comparaison de configurations
- [ ] Data augmentation

### Version 3.0 - Optimisations
- [ ] Optimisateurs avancÃ©s (Adam)
- [ ] Batch normalization
- [ ] Dropout
- [ ] Tests unitaires complets

---

**Date de crÃ©ation** : 2025-11-07
**Auteur** : Projet d'apprentissage personnel
**Langage** : Python 3.8+
