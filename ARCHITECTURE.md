# ğŸ§  Architecture et Concepts MathÃ©matiques

Ce document explique en dÃ©tail les concepts mathÃ©matiques et l'architecture du rÃ©seau de neurones implÃ©mentÃ© dans ce projet.

## Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Le Neurone Artificiel](#le-neurone-artificiel)
3. [Propagation Avant (Forward Pass)](#propagation-avant-forward-pass)
4. [Fonctions d'Activation](#fonctions-dactivation)
5. [Fonction de CoÃ»t](#fonction-de-coÃ»t)
6. [RÃ©tropropagation (Backpropagation)](#rÃ©tropropagation-backpropagation)
7. [Optimisation](#optimisation)
8. [Architecture du RÃ©seau](#architecture-du-rÃ©seau)

---

## Vue d'ensemble

Un rÃ©seau de neurones est un modÃ¨le mathÃ©matique inspirÃ© du cerveau humain, composÃ© de **couches de neurones artificiels** connectÃ©s entre eux.

### Principe gÃ©nÃ©ral

```
Input â†’ [Couche 1] â†’ [Couche 2] â†’ ... â†’ [Couche N] â†’ Output
```

Le rÃ©seau apprend en ajustant les **poids** (weights) et **biais** (biases) de chaque connexion pour minimiser l'erreur de prÃ©diction.

---

## Le Neurone Artificiel

### Structure d'un Neurone

Un neurone reÃ§oit plusieurs entrÃ©es, les combine, et produit une sortie.

```
EntrÃ©es: xâ‚, xâ‚‚, ..., xâ‚™
Poids:   wâ‚, wâ‚‚, ..., wâ‚™
Biais:   b

Sortie:  y = f(wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b)
```

### Formule MathÃ©matique

Pour un neurone `j` :

```
z_j = Î£áµ¢ (wáµ¢â±¼ Â· xáµ¢) + bâ±¼
a_j = f(z_j)
```

OÃ¹ :
- `xáµ¢` : entrÃ©es
- `wáµ¢â±¼` : poids de la connexion de `i` vers `j`
- `bâ±¼` : biais du neurone `j`
- `z_j` : somme pondÃ©rÃ©e (activation prÃ©-activation)
- `f` : fonction d'activation
- `a_j` : activation du neurone (sortie)

### Notation Matricielle

Pour une couche entiÃ¨re :

```
Z = W Â· X + b
A = f(Z)
```

OÃ¹ :
- `X` : vecteur/matrice d'entrÃ©es
- `W` : matrice des poids
- `b` : vecteur des biais
- `Z` : activations prÃ©-activation
- `A` : activations (sorties)

---

## Propagation Avant (Forward Pass)

La propagation avant consiste Ã  calculer les sorties du rÃ©seau en propageant les donnÃ©es Ã  travers les couches.

### Pour une couche Dense

```python
# Pseudo-code
Z = np.dot(W, X) + b    # Combinaison linÃ©aire
A = activation(Z)        # Application fonction d'activation
```

### Pour un rÃ©seau complet (3 couches)

```
Couche 1:
ZÂ¹ = WÂ¹ Â· X + bÂ¹
AÂ¹ = fÂ¹(ZÂ¹)

Couche 2:
ZÂ² = WÂ² Â· AÂ¹ + bÂ²
AÂ² = fÂ²(ZÂ²)

Couche 3 (sortie):
ZÂ³ = WÂ³ Â· AÂ² + bÂ³
AÂ³ = fÂ³(ZÂ³)
```

### Exemple MNIST

Pour une image 28Ã—28 pixels :

```
Input: X = [784 valeurs] (image aplatie)
       â†“
Layer 1: WÂ¹[128Ã—784], bÂ¹[128]
       ZÂ¹ = WÂ¹ Â· X + bÂ¹         [128 valeurs]
       AÂ¹ = ReLU(ZÂ¹)            [128 valeurs]
       â†“
Layer 2: WÂ²[64Ã—128], bÂ²[64]
       ZÂ² = WÂ² Â· AÂ¹ + bÂ²        [64 valeurs]
       AÂ² = ReLU(ZÂ²)            [64 valeurs]
       â†“
Layer 3: WÂ³[10Ã—64], bÂ³[10]
       ZÂ³ = WÂ³ Â· AÂ² + bÂ³        [10 valeurs]
       AÂ³ = Softmax(ZÂ³)         [10 probabilitÃ©s]
```

---

## Fonctions d'Activation

Les fonctions d'activation introduisent de la **non-linÃ©aritÃ©** dans le rÃ©seau, permettant d'apprendre des relations complexes.

### 1. Sigmoid

**Formule** :
```
Ïƒ(x) = 1 / (1 + eâ»Ë£)
```

**DÃ©rivÃ©e** :
```
Ïƒ'(x) = Ïƒ(x) Â· (1 - Ïƒ(x))
```

**PropriÃ©tÃ©s** :
- Sortie entre 0 et 1
- UtilisÃ©e historiquement
- ProblÃ¨me : gradient vanishing pour valeurs extrÃªmes

**Graphique** :
```
  1 |     â”Œâ”€â”€â”€â”€â”€
    |    /
0.5 |   /
    |  /
  0 |â”€â”€
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    -5  0   5
```

### 2. ReLU (Rectified Linear Unit)

**Formule** :
```
ReLU(x) = max(0, x) = {
    x  si x > 0
    0  si x â‰¤ 0
}
```

**DÃ©rivÃ©e** :
```
ReLU'(x) = {
    1  si x > 0
    0  si x â‰¤ 0
}
```

**PropriÃ©tÃ©s** :
- TrÃ¨s utilisÃ©e dans les couches cachÃ©es
- Calcul rapide
- RÃ©sout le gradient vanishing
- ProblÃ¨me : "dying ReLU" (neurones morts si x < 0 toujours)

**Graphique** :
```
    |    â•±
    |   â•±
    |  â•±
    | â•±
â”€â”€â”€â”€â”¼â”€â”€â”€â”€
    |
```

### 3. Tanh (Tangente Hyperbolique)

**Formule** :
```
tanh(x) = (eË£ - eâ»Ë£) / (eË£ + eâ»Ë£)
```

**DÃ©rivÃ©e** :
```
tanh'(x) = 1 - tanhÂ²(x)
```

**PropriÃ©tÃ©s** :
- Sortie entre -1 et 1
- CentrÃ©e autour de 0 (mieux que sigmoid)
- Gradient plus fort que sigmoid

### 4. Softmax

**Formule** (pour un vecteur de sortie) :
```
softmax(xáµ¢) = e^(xáµ¢) / Î£â±¼ e^(xâ±¼)
```

**PropriÃ©tÃ©s** :
- Transforme les valeurs en probabilitÃ©s
- Î£ softmax(xáµ¢) = 1
- UtilisÃ©e pour la couche de sortie (classification multi-classes)

**Exemple** :
```
Input:  [2.0, 1.0, 0.1]
Softmax: [0.659, 0.242, 0.099]  (somme = 1.0)
```

---

## Fonction de CoÃ»t

La fonction de coÃ»t (loss) mesure l'erreur entre les prÃ©dictions et les vraies valeurs.

### 1. Cross-Entropy (Entropie CroisÃ©e)

Pour la classification multi-classes (utilisÃ©e avec Softmax).

**Formule** :
```
L = -Î£áµ¢ yáµ¢ Â· log(Å·áµ¢)
```

OÃ¹ :
- `yáµ¢` : vraie valeur (one-hot encoded)
- `Å·áµ¢` : prÃ©diction (probabilitÃ©)

**Exemple** :
```
Vraie classe: 3
y = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  (one-hot)
Å· = [0.05, 0.05, 0.10, 0.60, 0.10, 0.05, 0.02, 0.01, 0.01, 0.01]

L = -(0Â·log(0.05) + ... + 1Â·log(0.60) + ...)
  = -log(0.60)
  â‰ˆ 0.51
```

**PropriÃ©tÃ©s** :
- PÃ©nalise fortement les mauvaises prÃ©dictions confiantes
- Gradient bien dÃ©fini avec softmax

### 2. MSE (Mean Squared Error)

**Formule** :
```
L = (1/n) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²
```

**PropriÃ©tÃ©s** :
- Plus simple Ã  comprendre
- Moins adaptÃ©e pour la classification
- Souvent utilisÃ©e pour la rÃ©gression

---

## RÃ©tropropagation (Backpropagation)

La rÃ©tropropagation calcule les gradients de la fonction de coÃ»t par rapport Ã  chaque poids et biais.

### Principe

On utilise la **rÃ¨gle de la chaÃ®ne** (chain rule) pour calculer les dÃ©rivÃ©es en remontant du dernier vers le premier layer.

### Notation

- `L` : Loss (fonction de coÃ»t)
- `âˆ‚L/âˆ‚W` : Gradient de la loss par rapport aux poids W
- `âˆ‚L/âˆ‚b` : Gradient de la loss par rapport aux biais b

### Algorithme

Pour la **derniÃ¨re couche** (couche de sortie) :

```
âˆ‚L/âˆ‚ZÂ³ = AÂ³ - Y  (si softmax + cross-entropy)

âˆ‚L/âˆ‚WÂ³ = (âˆ‚L/âˆ‚ZÂ³) Â· AÂ²áµ€
âˆ‚L/âˆ‚bÂ³ = âˆ‚L/âˆ‚ZÂ³
```

Pour les **couches cachÃ©es** (backprop de la couche l) :

```
âˆ‚L/âˆ‚AË¡ = W^(l+1)áµ€ Â· âˆ‚L/âˆ‚Z^(l+1)

âˆ‚L/âˆ‚ZË¡ = (âˆ‚L/âˆ‚AË¡) âŠ™ f'(ZË¡)
         oÃ¹ âŠ™ est le produit Ã©lÃ©ment par Ã©lÃ©ment (Hadamard)

âˆ‚L/âˆ‚WË¡ = (âˆ‚L/âˆ‚ZË¡) Â· A^(l-1)áµ€
âˆ‚L/âˆ‚bË¡ = âˆ‚L/âˆ‚ZË¡
```

### Exemple Concret (RÃ©seau 3 couches)

#### Forward Pass
```
X â†’ ZÂ¹ = WÂ¹Â·X + bÂ¹ â†’ AÂ¹ = ReLU(ZÂ¹)
  â†’ ZÂ² = WÂ²Â·AÂ¹ + bÂ² â†’ AÂ² = ReLU(ZÂ²)
  â†’ ZÂ³ = WÂ³Â·AÂ² + bÂ³ â†’ AÂ³ = Softmax(ZÂ³)
  â†’ L = CrossEntropy(AÂ³, Y)
```

#### Backward Pass
```
âˆ‚L/âˆ‚ZÂ³ = AÂ³ - Y

âˆ‚L/âˆ‚WÂ³ = âˆ‚L/âˆ‚ZÂ³ Â· AÂ²áµ€
âˆ‚L/âˆ‚bÂ³ = âˆ‚L/âˆ‚ZÂ³

âˆ‚L/âˆ‚AÂ² = WÂ³áµ€ Â· âˆ‚L/âˆ‚ZÂ³
âˆ‚L/âˆ‚ZÂ² = âˆ‚L/âˆ‚AÂ² âŠ™ ReLU'(ZÂ²)
âˆ‚L/âˆ‚WÂ² = âˆ‚L/âˆ‚ZÂ² Â· AÂ¹áµ€
âˆ‚L/âˆ‚bÂ² = âˆ‚L/âˆ‚ZÂ²

âˆ‚L/âˆ‚AÂ¹ = WÂ²áµ€ Â· âˆ‚L/âˆ‚ZÂ²
âˆ‚L/âˆ‚ZÂ¹ = âˆ‚L/âˆ‚AÂ¹ âŠ™ ReLU'(ZÂ¹)
âˆ‚L/âˆ‚WÂ¹ = âˆ‚L/âˆ‚ZÂ¹ Â· Xáµ€
âˆ‚L/âˆ‚bÂ¹ = âˆ‚L/âˆ‚ZÂ¹
```

### DÃ©rivÃ©es des Fonctions d'Activation

#### ReLU
```python
def relu_derivative(Z):
    return (Z > 0).astype(float)
```

#### Sigmoid
```python
def sigmoid_derivative(A):
    return A * (1 - A)
```

#### Tanh
```python
def tanh_derivative(A):
    return 1 - A**2
```

#### Softmax + Cross-Entropy
```python
# DÃ©rivÃ©e combinÃ©e simplifiÃ©e
dZ = A - Y  # TrÃ¨s simple !
```

---

## Optimisation

### 1. Gradient Descent (Descente de Gradient)

On met Ã  jour les paramÃ¨tres dans la direction opposÃ©e au gradient.

**Formule** :
```
W := W - Î± Â· âˆ‚L/âˆ‚W
b := b - Î± Â· âˆ‚L/âˆ‚b
```

OÃ¹ `Î±` est le **learning rate** (taux d'apprentissage).

### 2. Stochastic Gradient Descent (SGD)

Au lieu de calculer le gradient sur tout le dataset, on utilise des **mini-batches**.

**Algorithme** :
```
Pour chaque epoch:
    MÃ©langer les donnÃ©es
    Pour chaque mini-batch:
        1. Forward pass sur le batch
        2. Calcul de la loss
        3. Backward pass (calcul gradients)
        4. Mise Ã  jour des paramÃ¨tres
```

### 3. SGD avec Momentum

AccÃ©lÃ¨re la convergence en accumulant les gradients prÃ©cÃ©dents.

**Formule** :
```
v := Î² Â· v + (1 - Î²) Â· âˆ‚L/âˆ‚W
W := W - Î± Â· v
```

OÃ¹ `Î²` (momentum) est gÃ©nÃ©ralement 0.9.

### 4. Adam (Adaptive Moment Estimation)

Combine momentum et adaptation du learning rate.

**Formule** :
```
m := Î²â‚ Â· m + (1 - Î²â‚) Â· âˆ‚L/âˆ‚W         (moment 1)
v := Î²â‚‚ Â· v + (1 - Î²â‚‚) Â· (âˆ‚L/âˆ‚W)Â²      (moment 2)

m_corrected := m / (1 - Î²â‚áµ—)
v_corrected := v / (1 - Î²â‚‚áµ—)

W := W - Î± Â· m_corrected / (âˆšv_corrected + Îµ)
```

**ParamÃ¨tres typiques** :
- Î± = 0.001
- Î²â‚ = 0.9
- Î²â‚‚ = 0.999
- Îµ = 10â»â¸

---

## Architecture du RÃ©seau

### RÃ©seau Simple pour MNIST

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         INPUT LAYER (784)               â”‚
â”‚   Image 28Ã—28 aplatie en vecteur        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      HIDDEN LAYER 1 (128 neurones)      â”‚
â”‚         ZÂ¹ = WÂ¹ Â· X + bÂ¹                â”‚
â”‚         AÂ¹ = ReLU(ZÂ¹)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      HIDDEN LAYER 2 (64 neurones)       â”‚
â”‚         ZÂ² = WÂ² Â· AÂ¹ + bÂ²               â”‚
â”‚         AÂ² = ReLU(ZÂ²)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      OUTPUT LAYER (10 neurones)         â”‚
â”‚         ZÂ³ = WÂ³ Â· AÂ² + bÂ³               â”‚
â”‚         AÂ³ = Softmax(ZÂ³)                â”‚
â”‚    [P(0), P(1), ..., P(9)]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dimensions des Matrices

Pour un batch de taille `B` :

| Couche | Poids W | Biais b | Input | Output |
|--------|---------|---------|-------|--------|
| Layer 1 | [128, 784] | [128, 1] | [784, B] | [128, B] |
| Layer 2 | [64, 128] | [64, 1] | [128, B] | [64, B] |
| Layer 3 | [10, 64] | [10, 1] | [64, B] | [10, B] |

### Nombre de ParamÃ¨tres

```
Layer 1: 784 Ã— 128 + 128 = 100,480 paramÃ¨tres
Layer 2: 128 Ã— 64 + 64   = 8,256 paramÃ¨tres
Layer 3: 64 Ã— 10 + 10    = 650 paramÃ¨tres

TOTAL: 109,386 paramÃ¨tres
```

---

## Processus d'EntraÃ®nement Complet

### Pseudo-code

```python
# Initialisation
W1, b1 = initialize_weights(128, 784)
W2, b2 = initialize_weights(64, 128)
W3, b3 = initialize_weights(10, 64)

learning_rate = 0.01
epochs = 20
batch_size = 64

for epoch in range(epochs):
    # MÃ©langer les donnÃ©es
    shuffle(train_data)

    for batch in get_batches(train_data, batch_size):
        X, Y = batch

        # === FORWARD PASS ===
        Z1 = W1 @ X + b1
        A1 = relu(Z1)

        Z2 = W2 @ A1 + b2
        A2 = relu(Z2)

        Z3 = W3 @ A2 + b3
        A3 = softmax(Z3)

        # Calcul de la loss
        loss = cross_entropy(A3, Y)

        # === BACKWARD PASS ===
        dZ3 = A3 - Y
        dW3 = dZ3 @ A2.T / batch_size
        db3 = np.sum(dZ3, axis=1, keepdims=True) / batch_size

        dA2 = W3.T @ dZ3
        dZ2 = dA2 * relu_derivative(Z2)
        dW2 = dZ2 @ A1.T / batch_size
        db2 = np.sum(dZ2, axis=1, keepdims=True) / batch_size

        dA1 = W2.T @ dZ2
        dZ1 = dA1 * relu_derivative(Z1)
        dW1 = dZ1 @ X.T / batch_size
        db1 = np.sum(dZ1, axis=1, keepdims=True) / batch_size

        # === UPDATE ===
        W3 -= learning_rate * dW3
        b3 -= learning_rate * db3
        W2 -= learning_rate * dW2
        b2 -= learning_rate * db2
        W1 -= learning_rate * dW1
        b1 -= learning_rate * db1

    # Ã‰valuation
    accuracy = evaluate(W1, b1, W2, b2, W3, b3, test_data)
    print(f"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.2%}")
```

---

## Techniques AvancÃ©es (Ã€ ImplÃ©menter)

### 1. Dropout

DÃ©sactive alÃ©atoirement des neurones pendant l'entraÃ®nement pour Ã©viter le surapprentissage.

```python
def dropout(A, keep_prob=0.8):
    mask = np.random.rand(*A.shape) < keep_prob
    return A * mask / keep_prob
```

### 2. Batch Normalization

Normalise les activations pour stabiliser l'apprentissage.

```python
def batch_norm(Z):
    mean = np.mean(Z, axis=0)
    std = np.std(Z, axis=0)
    return (Z - mean) / (std + 1e-8)
```

### 3. Weight Initialization (Xavier/He)

Initialisation intelligente pour Ã©viter les gradients qui explosent ou disparaissent.

```python
# Xavier (pour sigmoid, tanh)
W = np.random.randn(n_out, n_in) * np.sqrt(1 / n_in)

# He (pour ReLU)
W = np.random.randn(n_out, n_in) * np.sqrt(2 / n_in)
```

### 4. Learning Rate Decay

RÃ©duire progressivement le learning rate.

```python
learning_rate = initial_lr / (1 + decay_rate * epoch)
```

---

## DÃ©bogage et Validation

### Gradient Checking

VÃ©rifier que la backpropagation est correcte en comparant avec le gradient numÃ©rique.

```python
# Gradient numÃ©rique (approximation)
epsilon = 1e-7
grad_numeric = (loss(W + epsilon) - loss(W - epsilon)) / (2 * epsilon)

# Gradient analytique (backprop)
grad_analytic = backprop(W)

# VÃ©rification
difference = abs(grad_numeric - grad_analytic)
assert difference < 1e-7, "Gradient incorrect !"
```

### Signes d'un Bon Apprentissage

âœ… **Bon** :
- Loss qui diminue progressivement
- Accuracy qui augmente sur train ET test
- Convergence stable

âŒ **ProblÃ¨mes** :
- Loss qui explose â†’ Learning rate trop Ã©levÃ©
- Loss qui stagne â†’ Learning rate trop faible, ou modÃ¨le trop simple
- Train accuracy Ã©levÃ©e, test accuracy faible â†’ Surapprentissage (overfitting)
- Loss = NaN â†’ Gradient qui explose, mauvaise initialisation

---

## RÃ©fÃ©rences

- **Livre** : [Neural Networks and Deep Learning - Michael Nielsen](http://neuralnetworksanddeeplearning.com/)
- **VidÃ©os** : [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- **Cours** : [CS231n - Stanford](http://cs231n.stanford.edu/)
- **Paper** : [Backpropagation - Rumelhart et al. 1986](https://www.nature.com/articles/323533a0)

---

**Note** : Cette documentation est destinÃ©e Ã  l'apprentissage. Pour approfondir, consultez les ressources ci-dessus et expÃ©rimentez avec le code !
