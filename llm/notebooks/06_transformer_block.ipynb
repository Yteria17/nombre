{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Transformer Block\n",
    "\n",
    "Assembler : Attention + Feed-Forward + LayerNorm + Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Multi-head attention + residual + norm\n",
    "        attn_out = self.attention.forward(x)\n",
    "        x = self.ln1.forward(x + attn_out)\n",
    "        \n",
    "        # Feed-forward + residual + norm\n",
    "        ffn_out = self.ffn.forward(x)\n",
    "        x = self.ln2.forward(x + ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) * 0.01\n",
    "        self.W2 = np.random.randn(d_ff, d_model) * 0.01\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = np.maximum(0, np.dot(x, self.W1))  # ReLU\n",
    "        x = np.dot(x, self.W2)\n",
    "        return x\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
