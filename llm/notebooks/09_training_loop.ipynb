{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Training Loop\n",
    "\n",
    "Entra√Æner le mod√®le : Forward ‚Üí Loss ‚Üí Backward ‚Üí Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function - Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    Calcule la cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        logits: (batch_size, seq_len, vocab_size)\n",
    "        targets: (batch_size, seq_len) - indices des tokens cibles\n",
    "    \n",
    "    Returns:\n",
    "        loss: scalar\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    \n",
    "    # Reshape\n",
    "    logits = logits.reshape(-1, vocab_size)\n",
    "    targets = targets.reshape(-1)\n",
    "    \n",
    "    # Softmax\n",
    "    logits_exp = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "    probs = logits_exp / np.sum(logits_exp, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Cross-entropy\n",
    "    log_probs = -np.log(probs[np.arange(len(targets)), targets] + 1e-10)\n",
    "    loss = np.mean(log_probs)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Test\n",
    "batch_size, seq_len, vocab_size = 4, 8, 100\n",
    "logits = np.random.randn(batch_size, seq_len, vocab_size)\n",
    "targets = np.random.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "loss = cross_entropy_loss(logits, targets)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Random baseline: {-np.log(1/vocab_size):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer - Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}  # First moment\n",
    "        self.v = {}  # Second moment\n",
    "        self.t = 0   # Timestep\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        Update parameters using Adam.\n",
    "        \n",
    "        Args:\n",
    "            params: dict of parameter arrays\n",
    "            grads: dict of gradient arrays\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        for key in params.keys():\n",
    "            # Initialize moments if needed\n",
    "            if key not in self.m:\n",
    "                self.m[key] = np.zeros_like(params[key])\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            # Update biased moments\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, config):\n",
    "    \"\"\"\n",
    "    Entra√Æne le mod√®le GPT.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT model\n",
    "        train_data: Training dataset\n",
    "        val_data: Validation dataset\n",
    "        config: dict with training hyperparameters\n",
    "    \"\"\"\n",
    "    optimizer = AdamOptimizer(learning_rate=config['lr'])\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for step in range(config['steps_per_epoch']):\n",
    "            # Get batch\n",
    "            x, y = get_batch(train_data, config['block_size'], config['batch_size'])\n",
    "            \n",
    "            # Forward\n",
    "            logits = model.forward(x)\n",
    "            loss = cross_entropy_loss(logits, y)\n",
    "            \n",
    "            # Backward (simplified - assume model has backward method)\n",
    "            grads = model.backward(y)\n",
    "            \n",
    "            # Update\n",
    "            optimizer.update(model.parameters, grads)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = evaluate_model(model, val_data, config)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} - \"\n",
    "              f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, data, config):\n",
    "    \"\"\"√âvalue le mod√®le sur les donn√©es de validation.\"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(config['eval_steps']):\n",
    "        x, y = get_batch(data, config['block_size'], config['batch_size'])\n",
    "        logits = model.forward(x)\n",
    "        loss = cross_entropy_loss(logits, y)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration et Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'vocab_size': 65,\n",
    "    'd_model': 256,\n",
    "    'num_layers': 4,\n",
    "    'num_heads': 8,\n",
    "    'd_ff': 1024,\n",
    "    'max_len': 256,\n",
    "    'block_size': 64,\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 10,\n",
    "    'steps_per_epoch': 100,\n",
    "    'eval_steps': 20\n",
    "}\n",
    "\n",
    "print(\"Configuration d'entra√Ænement:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Initialiser le mod√®le\n",
    "# model = GPT(**config)\n",
    "\n",
    "# Entra√Æner\n",
    "# train_losses, val_losses = train_model(model, train_data, val_data, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler des losses pour la visualisation\n",
    "train_losses = [4.5 - 0.3*i + 0.1*np.random.randn() for i in range(10)]\n",
    "val_losses = [4.5 - 0.25*i + 0.15*np.random.randn() for i in range(10)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {val_losses[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
