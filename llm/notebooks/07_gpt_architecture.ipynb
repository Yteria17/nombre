{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– GPT Architecture ComplÃ¨te\n",
    "\n",
    "Assembler tous les composants : Embedding â†’ NÃ—Transformer â†’ Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GPT:\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_len):\n",
    "        self.token_embedding = np.random.randn(vocab_size, d_model) * 0.01\n",
    "        self.pos_encoding = positional_encoding(max_len, d_model)\n",
    "        \n",
    "        self.blocks = [TransformerBlock(d_model, num_heads, d_ff) \n",
    "                      for _ in range(num_layers)]\n",
    "        \n",
    "        self.ln_final = LayerNorm(d_model)\n",
    "        self.output_proj = np.random.randn(d_model, vocab_size) * 0.01\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        # Embeddings\n",
    "        x = self.token_embedding[token_ids]\n",
    "        x = x + self.pos_encoding[:len(token_ids)]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.ln_final.forward(x)\n",
    "        logits = np.dot(x, self.output_proj)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Example\n",
    "model = GPT(vocab_size=5000, d_model=256, num_layers=4, \n",
    "           num_heads=8, d_ff=1024, max_len=128)\n",
    "\n",
    "tokens = np.array([1, 2, 3, 4, 5])\n",
    "logits = model.forward(tokens)\n",
    "print(f\"Input shape: {tokens.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
