{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ú® Text Generation\n",
    "\n",
    "G√©n√©rer du texte : Greedy, Top-k, Top-p, Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strat√©gies de Sampling\n",
    "\n",
    "### 1. Greedy Decoding\n",
    "Toujours choisir le token le plus probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sample(logits):\n",
    "    \"\"\"\n",
    "    Greedy sampling: choisir le token avec la plus haute probabilit√©.\n",
    "    \n",
    "    Args:\n",
    "        logits: (vocab_size,)\n",
    "    \n",
    "    Returns:\n",
    "        token_id: int\n",
    "    \"\"\"\n",
    "    return np.argmax(logits)\n",
    "\n",
    "# Test\n",
    "logits = np.array([1.0, 3.5, 2.0, 0.5])\n",
    "token = greedy_sample(logits)\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Token choisi (greedy): {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Temperature Sampling\n",
    "Contr√¥ler la \"cr√©ativit√©\" du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sample(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample avec temp√©rature.\n",
    "    \n",
    "    Args:\n",
    "        logits: (vocab_size,)\n",
    "        temperature: float\n",
    "            - T < 1.0: Plus conservateur (peaked distribution)\n",
    "            - T = 1.0: Distribution normale\n",
    "            - T > 1.0: Plus cr√©atif (flatter distribution)\n",
    "    \n",
    "    Returns:\n",
    "        token_id: int\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Softmax\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    # Sample\n",
    "    token = np.random.choice(len(probs), p=probs)\n",
    "    return token\n",
    "\n",
    "# Test avec diff√©rentes temp√©ratures\n",
    "logits = np.array([1.0, 3.5, 2.0, 0.5])\n",
    "\n",
    "print(\"√âchantillonnage avec diff√©rentes temp√©ratures:\")\n",
    "for temp in [0.5, 1.0, 2.0]:\n",
    "    samples = [temperature_sample(logits, temp) for _ in range(100)]\n",
    "    print(f\"\\nT={temp}:\")\n",
    "    for i in range(4):\n",
    "        count = samples.count(i)\n",
    "        print(f\"  Token {i}: {count}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Top-k Sampling\n",
    "Ne consid√©rer que les k tokens les plus probables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sample(logits, k=10, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Top-k sampling.\n",
    "    \n",
    "    Args:\n",
    "        logits: (vocab_size,)\n",
    "        k: int - nombre de top tokens √† consid√©rer\n",
    "        temperature: float\n",
    "    \n",
    "    Returns:\n",
    "        token_id: int\n",
    "    \"\"\"\n",
    "    # Get top k indices\n",
    "    top_k_indices = np.argsort(logits)[-k:]\n",
    "    top_k_logits = logits[top_k_indices]\n",
    "    \n",
    "    # Apply temperature\n",
    "    top_k_logits = top_k_logits / temperature\n",
    "    \n",
    "    # Softmax\n",
    "    exp_logits = np.exp(top_k_logits - np.max(top_k_logits))\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    # Sample from top k\n",
    "    sampled_index = np.random.choice(len(probs), p=probs)\n",
    "    token = top_k_indices[sampled_index]\n",
    "    \n",
    "    return token\n",
    "\n",
    "# Test\n",
    "logits = np.random.randn(100)\n",
    "token = top_k_sample(logits, k=10)\n",
    "print(f\"Token choisi (top-k): {token}\")\n",
    "print(f\"Rang du token: {np.argsort(logits)[::-1].tolist().index(token) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Top-p (Nucleus) Sampling\n",
    "Consid√©rer les tokens jusqu'√† ce que la probabilit√© cumul√©e atteigne p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sample(logits, p=0.9, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Top-p (nucleus) sampling.\n",
    "    \n",
    "    Args:\n",
    "        logits: (vocab_size,)\n",
    "        p: float - probabilit√© cumul√©e cible (0 < p <= 1)\n",
    "        temperature: float\n",
    "    \n",
    "    Returns:\n",
    "        token_id: int\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Softmax\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    # Sort probabilities\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    sorted_probs = probs[sorted_indices]\n",
    "    \n",
    "    # Get cumulative probabilities\n",
    "    cumsum_probs = np.cumsum(sorted_probs)\n",
    "    \n",
    "    # Find cutoff\n",
    "    cutoff_index = np.searchsorted(cumsum_probs, p) + 1\n",
    "    \n",
    "    # Keep only top-p tokens\n",
    "    nucleus_indices = sorted_indices[:cutoff_index]\n",
    "    nucleus_probs = sorted_probs[:cutoff_index]\n",
    "    nucleus_probs = nucleus_probs / np.sum(nucleus_probs)  # Renormalize\n",
    "    \n",
    "    # Sample\n",
    "    sampled_index = np.random.choice(len(nucleus_probs), p=nucleus_probs)\n",
    "    token = nucleus_indices[sampled_index]\n",
    "    \n",
    "    return token\n",
    "\n",
    "# Test\n",
    "logits = np.random.randn(100)\n",
    "token = top_p_sample(logits, p=0.9)\n",
    "print(f\"Token choisi (top-p): {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G√©n√©ration de Texte Compl√®te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_length=100, strategy='top_p', **kwargs):\n",
    "    \"\"\"\n",
    "    G√©n√®re du texte de mani√®re autoregressive.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT model\n",
    "        prompt: str - texte de d√©part\n",
    "        max_length: int - nombre de tokens √† g√©n√©rer\n",
    "        strategy: str - 'greedy', 'temperature', 'top_k', ou 'top_p'\n",
    "        **kwargs: arguments pour la strat√©gie de sampling\n",
    "    \n",
    "    Returns:\n",
    "        generated_text: str\n",
    "    \"\"\"\n",
    "    # Encode prompt\n",
    "    context = encode(prompt)\n",
    "    \n",
    "    # Generate tokens\n",
    "    for _ in range(max_length):\n",
    "        # Get logits for next token\n",
    "        logits = model.forward(np.array([context]))\n",
    "        next_token_logits = logits[0, -1, :]  # Last token\n",
    "        \n",
    "        # Sample next token\n",
    "        if strategy == 'greedy':\n",
    "            next_token = greedy_sample(next_token_logits)\n",
    "        elif strategy == 'temperature':\n",
    "            next_token = temperature_sample(next_token_logits, **kwargs)\n",
    "        elif strategy == 'top_k':\n",
    "            next_token = top_k_sample(next_token_logits, **kwargs)\n",
    "        elif strategy == 'top_p':\n",
    "            next_token = top_p_sample(next_token_logits, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        # Append to context\n",
    "        context.append(next_token)\n",
    "        \n",
    "        # Optional: stop at end of sentence\n",
    "        if decode([next_token]) in ['.', '!', '?']:\n",
    "            break\n",
    "    \n",
    "    # Decode\n",
    "    return decode(context)\n",
    "\n",
    "# Example usage (pseudo-code)\n",
    "# generated = generate_text(model, \"To be or not to be\", \n",
    "#                          max_length=50, \n",
    "#                          strategy='top_p', \n",
    "#                          p=0.9, \n",
    "#                          temperature=0.8)\n",
    "# print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des Strat√©gies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'impact de la temp√©rature\n",
    "logits = np.array([1.0, 3.5, 2.0, 0.5, 1.5])\n",
    "temperatures = [0.5, 1.0, 2.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, temp in enumerate(temperatures):\n",
    "    scaled_logits = logits / temp\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits))\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    axes[i].bar(range(len(probs)), probs)\n",
    "    axes[i].set_title(f'Temperature = {temp}')\n",
    "    axes[i].set_xlabel('Token ID')\n",
    "    axes[i].set_ylabel('Probabilit√©')\n",
    "    axes[i].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüî• Temp√©rature basse (0.5): Plus conservateur, peaked\")\n",
    "print(\"üòê Temp√©rature normale (1.0): Distribution originale\")\n",
    "print(\"üé≤ Temp√©rature haute (2.0): Plus cr√©atif, flat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
