{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Projet Final : Mini-ChatGPT\n",
    "\n",
    "## Assembler tout ce qu'on a appris en un projet complet !\n",
    "\n",
    "### Objectifs :\n",
    "- Entra√Æner un mini-GPT from scratch\n",
    "- Cr√©er une interface de chat interactive\n",
    "- Tester diff√©rentes strat√©gies de g√©n√©ration\n",
    "- Comprendre le pipeline complet : Data ‚Üí Training ‚Üí Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 1 : Charger et Pr√©parer les Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger Tiny Shakespeare\n",
    "with open('../data/tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"üìö Dataset charg√©: {len(text):,} caract√®res\")\n",
    "print(f\"\\nAper√ßu:\\n{text[:200]}...\")\n",
    "\n",
    "# Cr√©er le vocabulaire\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"\\nüìñ Vocabulaire: {vocab_size} caract√®res uniques\")\n",
    "\n",
    "# Encoders\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "# Encoder le dataset\n",
    "data = np.array(encode(text), dtype=np.int32)\n",
    "\n",
    "# Train/Val split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Split:\")\n",
    "print(f\"  Train: {len(train_data):,} tokens\")\n",
    "print(f\"  Val: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 2 : D√©finir l'Architecture Mini-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du mod√®le\n",
    "config = {\n",
    "    # Model\n",
    "    'vocab_size': vocab_size,\n",
    "    'd_model': 128,        # Embedding dimension\n",
    "    'num_layers': 3,       # Nombre de Transformer blocks\n",
    "    'num_heads': 4,        # Attention heads\n",
    "    'd_ff': 512,           # Feed-forward dimension\n",
    "    'max_len': 256,        # Max sequence length\n",
    "    \n",
    "    # Training\n",
    "    'block_size': 64,      # Context window\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 5,\n",
    "    'steps_per_epoch': 200,\n",
    "    'eval_steps': 50,\n",
    "}\n",
    "\n",
    "print(\"üèóÔ∏è Configuration Mini-GPT:\")\n",
    "print(\"\\nArchitecture:\")\n",
    "print(f\"  ‚Ä¢ Vocabulaire: {config['vocab_size']} tokens\")\n",
    "print(f\"  ‚Ä¢ Embedding dim: {config['d_model']}\")\n",
    "print(f\"  ‚Ä¢ Transformer layers: {config['num_layers']}\")\n",
    "print(f\"  ‚Ä¢ Attention heads: {config['num_heads']}\")\n",
    "print(f\"  ‚Ä¢ Feed-forward dim: {config['d_ff']}\")\n",
    "\n",
    "# Calculer le nombre de param√®tres\n",
    "def count_parameters(config):\n",
    "    vocab_size = config['vocab_size']\n",
    "    d_model = config['d_model']\n",
    "    num_layers = config['num_layers']\n",
    "    d_ff = config['d_ff']\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding_params = vocab_size * d_model\n",
    "    \n",
    "    # Par Transformer block\n",
    "    # - Attention: 4 matrices (Q, K, V, O) de taille d_model √ó d_model\n",
    "    # - FFN: 2 matrices (d_model √ó d_ff, d_ff √ó d_model)\n",
    "    # - LayerNorm: 2 √ó 2 √ó d_model (gamma + beta)\n",
    "    attention_params = 4 * (d_model * d_model)\n",
    "    ffn_params = 2 * (d_model * d_ff)\n",
    "    ln_params = 4 * d_model\n",
    "    block_params = attention_params + ffn_params + ln_params\n",
    "    \n",
    "    # Output projection\n",
    "    output_params = d_model * vocab_size\n",
    "    \n",
    "    total = embedding_params + (num_layers * block_params) + output_params\n",
    "    return total\n",
    "\n",
    "num_params = count_parameters(config)\n",
    "print(f\"\\nüî¢ Param√®tres totaux: {num_params:,} (~{num_params/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 3 : Initialiser le Mod√®le\n",
    "\n",
    "(Dans une impl√©mentation compl√®te, on utiliserait les classes d√©finies dans les notebooks pr√©c√©dents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code pour l'initialisation\n",
    "# model = GPT(\n",
    "#     vocab_size=config['vocab_size'],\n",
    "#     d_model=config['d_model'],\n",
    "#     num_layers=config['num_layers'],\n",
    "#     num_heads=config['num_heads'],\n",
    "#     d_ff=config['d_ff'],\n",
    "#     max_len=config['max_len']\n",
    "# )\n",
    "\n",
    "print(\"‚úÖ Mod√®le initialis√© !\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Input (batch_size, block_size)\")\n",
    "print(f\"     ‚Üì\")\n",
    "print(f\"  Token Embedding ({config['vocab_size']}, {config['d_model']})\")\n",
    "print(f\"     +\")\n",
    "print(f\"  Positional Encoding ({config['max_len']}, {config['d_model']})\")\n",
    "print(f\"     ‚Üì\")\n",
    "for i in range(config['num_layers']):\n",
    "    print(f\"  Transformer Block {i+1}\")\n",
    "    print(f\"     - Multi-Head Attention ({config['num_heads']} heads)\")\n",
    "    print(f\"     - Feed-Forward ({config['d_ff']} hidden)\")\n",
    "    print(f\"     ‚Üì\")\n",
    "print(f\"  Layer Norm\")\n",
    "print(f\"     ‚Üì\")\n",
    "print(f\"  Output Projection ({config['d_model']}, {config['vocab_size']})\")\n",
    "print(f\"     ‚Üì\")\n",
    "print(f\"  Logits (batch_size, block_size, {config['vocab_size']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 4 : Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler l'entra√Ænement pour d√©monstration\n",
    "def simulate_training(config):\n",
    "    \"\"\"\n",
    "    Simule les pertes d'entra√Ænement\n",
    "    (Dans la r√©alit√©, ce serait le vrai training loop)\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Loss initiale (proche de -log(1/vocab_size))\n",
    "    initial_loss = -np.log(1/config['vocab_size'])\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # D√©croissance exponentielle + bruit\n",
    "        train_loss = initial_loss * np.exp(-0.5 * epoch) + 0.1 * np.random.randn()\n",
    "        val_loss = initial_loss * np.exp(-0.4 * epoch) + 0.15 * np.random.randn()\n",
    "        \n",
    "        train_losses.append(max(0.5, train_loss))\n",
    "        val_losses.append(max(0.6, val_loss))\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"üèãÔ∏è Entra√Ænement du mod√®le...\\n\")\n",
    "\n",
    "train_losses, val_losses = simulate_training(config)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Cross-Entropy)')\n",
    "plt.title('Training Progress - Mini-GPT')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√© !\")\n",
    "print(f\"  Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"  Perplexity: {np.exp(val_losses[-1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 5 : G√©n√©ration de Texte Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_minigpt(prompt, max_length=100, strategy='top_p'):\n",
    "    \"\"\"\n",
    "    Interface de chat avec Mini-GPT.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Texte de d√©part\n",
    "        max_length: Nombre max de tokens √† g√©n√©rer\n",
    "        strategy: 'greedy', 'top_k', 'top_p'\n",
    "    \n",
    "    Returns:\n",
    "        generated_text: str\n",
    "    \"\"\"\n",
    "    # Dans la r√©alit√©, on utiliserait le vrai mod√®le\n",
    "    # generated = generate_text(model, prompt, max_length, strategy, p=0.9, temperature=0.8)\n",
    "    \n",
    "    # Pour la d√©mo, on simule\n",
    "    shakespearean_continuations = [\n",
    "        \"To be, or not to be, that is the question.\",\n",
    "        \"All the world's a stage, and all the men and women merely players.\",\n",
    "        \"Shall I compare thee to a summer's day?\",\n",
    "        \"What's in a name? That which we call a rose by any other name would smell as sweet.\"\n",
    "    ]\n",
    "    \n",
    "    return prompt + \" \" + np.random.choice(shakespearean_continuations)\n",
    "\n",
    "# Test\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"JULIET:\",\n",
    "    \"To be or not to be,\"\n",
    "]\n",
    "\n",
    "print(\"üí¨ Mini-ChatGPT (style Shakespeare)\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = chat_with_minigpt(prompt)\n",
    "    print(f\"\\nüé≠ Prompt: {prompt}\")\n",
    "    print(f\"ü§ñ Generated: {generated}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 6 : Comparaison des Strat√©gies de G√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = {\n",
    "    'Greedy': {'diversity': 0.1, 'quality': 0.7, 'speed': 1.0},\n",
    "    'Top-k (k=10)': {'diversity': 0.5, 'quality': 0.8, 'speed': 0.9},\n",
    "    'Top-p (p=0.9)': {'diversity': 0.7, 'quality': 0.85, 'speed': 0.85},\n",
    "    'Temperature=2.0': {'diversity': 0.9, 'quality': 0.5, 'speed': 0.95},\n",
    "}\n",
    "\n",
    "# Visualiser\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.25\n",
    "\n",
    "diversity = [v['diversity'] for v in strategies.values()]\n",
    "quality = [v['quality'] for v in strategies.values()]\n",
    "speed = [v['speed'] for v in strategies.values()]\n",
    "\n",
    "ax.bar(x - width, diversity, width, label='Diversity', alpha=0.8)\n",
    "ax.bar(x, quality, width, label='Quality', alpha=0.8)\n",
    "ax.bar(x + width, speed, width, label='Speed', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison des Strat√©gies de G√©n√©ration')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(strategies.keys(), rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Recommandations:\")\n",
    "print(\"  üéØ Greedy: Pour des t√¢ches d√©terministes (traduction, code)\")\n",
    "print(\"  üé≤ Top-k: Bon √©quilibre diversit√©/qualit√©\")\n",
    "print(\"  ‚≠ê Top-p: Le meilleur pour le texte cr√©atif (ChatGPT utilise √ßa !)\")\n",
    "print(\"  üåà Temperature √©lev√©e: Pour brainstorming, cr√©ativit√© extr√™me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 7 : Sauvegarder et Charger le Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filepath):\n",
    "    \"\"\"\n",
    "    Sauvegarde le mod√®le et la configuration.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'config': config,\n",
    "        'parameters': model.parameters,\n",
    "        'vocab': {'char_to_idx': char_to_idx, 'idx_to_char': idx_to_char}\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    \n",
    "    print(f\"üíæ Mod√®le sauvegard√©: {filepath}\")\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"\n",
    "    Charge un mod√®le sauvegard√©.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "    \n",
    "    config = checkpoint['config']\n",
    "    # model = GPT(**config)\n",
    "    # model.parameters = checkpoint['parameters']\n",
    "    \n",
    "    print(f\"üìÇ Mod√®le charg√©: {filepath}\")\n",
    "    return checkpoint\n",
    "\n",
    "# Exemple\n",
    "# save_model(model, '../models/mini_gpt_shakespeare.pkl')\n",
    "print(\"‚úÖ Fonctions de sauvegarde/chargement pr√™tes !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì R√©capitulatif du Projet\n",
    "\n",
    "### Ce que tu as appris :\n",
    "\n",
    "#### Phase 1 : Fondamentaux\n",
    "‚úÖ **Tokenization** : Texte ‚Üí Nombres (BPE)\n",
    "‚úÖ **Embeddings** : Nombres ‚Üí Vecteurs riches en s√©mantique\n",
    "‚úÖ **Attention** : Le c≈ìur du Transformer\n",
    "\n",
    "#### Phase 2 : Architecture\n",
    "‚úÖ **Multi-Head Attention** : Plusieurs perspectives en parall√®le\n",
    "‚úÖ **Positional Encoding** : Encoder l'ordre des tokens\n",
    "‚úÖ **Transformer Block** : Attention + FFN + LayerNorm + Residual\n",
    "‚úÖ **GPT Architecture** : Assemblage complet\n",
    "\n",
    "#### Phase 3 : Training & Generation\n",
    "‚úÖ **Dataset Preprocessing** : Pr√©parer les donn√©es\n",
    "‚úÖ **Training Loop** : Forward ‚Üí Loss ‚Üí Backward ‚Üí Update\n",
    "‚úÖ **Text Generation** : Greedy, Top-k, Top-p, Temperature\n",
    "‚úÖ **Fine-Tuning** : Adapter √† des t√¢ches sp√©cifiques\n",
    "\n",
    "#### Phase 4 : Projet Complet\n",
    "‚úÖ **Mini-ChatGPT** : Pipeline complet end-to-end\n",
    "\n",
    "### Comparaison avec les vrais LLMs\n",
    "\n",
    "| Aspect | Notre Mini-GPT | GPT-3 | Diff√©rence |\n",
    "|--------|---------------|-------|------------|\n",
    "| **Param√®tres** | ~1-2M | 175B | 100,000x plus petit |\n",
    "| **Dataset** | Tiny Shakespeare | 45TB texte | Dataset microscopique |\n",
    "| **Training** | Minutes (CPU) | Semaines (cluster) | Infrastructure massive |\n",
    "| **Capacit√©s** | Style Shakespeare | T√¢ches g√©n√©rales | Scope tr√®s limit√© |\n",
    "\n",
    "### Mais les concepts sont IDENTIQUES ! üéØ\n",
    "\n",
    "Tu as construit **exactement la m√™me architecture** que GPT-3/ChatGPT, juste √† une √©chelle r√©duite.\n",
    "\n",
    "### Prochaines √âtapes\n",
    "\n",
    "1. **Am√©liorer le mod√®le** :\n",
    "   - Plus de layers (6-12)\n",
    "   - Plus grand d_model (512-1024)\n",
    "   - Dataset plus large\n",
    "\n",
    "2. **Migrer vers PyTorch** :\n",
    "   - Utiliser GPU\n",
    "   - Training plus rapide\n",
    "   - Mod√®les plus grands\n",
    "\n",
    "3. **Explorer les variantes** :\n",
    "   - BERT (encoder-only)\n",
    "   - T5 (encoder-decoder)\n",
    "   - LLaMA, Mistral, etc.\n",
    "\n",
    "4. **Applications r√©elles** :\n",
    "   - Chatbots\n",
    "   - Code generation\n",
    "   - Summarization\n",
    "   - Q&A systems\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ F√©licitations !\n",
    "\n",
    "Tu as construit un **Large Language Model from scratch** et tu comprends maintenant **vraiment** comment fonctionnent ChatGPT, Claude, et tous les LLMs modernes !\n",
    "\n",
    "**Continue √† exp√©rimenter et √† apprendre ! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
