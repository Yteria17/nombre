{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ Multi-Head Attention\n",
    "\n",
    "Plusieurs tÃªtes d'attention en parallÃ¨le pour capturer diffÃ©rentes relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Projections\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.01\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Projections Q, K, V\n",
    "        Q = np.dot(x, self.W_q)\n",
    "        K = np.dot(x, self.W_k)\n",
    "        V = np.dot(x, self.W_v)\n",
    "        \n",
    "        # Split en tÃªtes multiples\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        # Attention pour chaque tÃªte\n",
    "        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(self.d_k)\n",
    "        attention = self.softmax(scores)\n",
    "        output = np.matmul(attention, V)\n",
    "        \n",
    "        # Concat\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, self.d_model)\n",
    "        output = np.dot(output, self.W_o)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
