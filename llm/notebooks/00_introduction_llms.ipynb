{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Introduction aux Large Language Models (LLMs)\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "Dans ce notebook, nous allons d√©couvrir :\n",
    "\n",
    "- üß† **Qu'est-ce qu'un LLM** ? (ChatGPT, Claude, GPT-4)\n",
    "- üèóÔ∏è **L'architecture Transformer** - La r√©volution de 2017\n",
    "- üî§ **Comment les LLMs \"comprennent\" le texte**\n",
    "- üìä **GPT vs BERT vs T5** - Diff√©rentes approches\n",
    "- üõ£Ô∏è **Notre parcours d'apprentissage** - Ce qu'on va construire\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Qu'est-ce qu'un Large Language Model ?\n",
    "\n",
    "### D√©finition Simple\n",
    "\n",
    "Un **LLM** est un r√©seau de neurones entra√Æn√© sur **√©norm√©ment de texte** pour :\n",
    "\n",
    "1. **Comprendre** le langage naturel\n",
    "2. **G√©n√©rer** du texte coh√©rent\n",
    "3. **Accomplir** des t√¢ches vari√©es (traduction, r√©sum√©, code, etc.)\n",
    "\n",
    "### Exemples Concrets\n",
    "\n",
    "| Mod√®le | Entreprise | Param√®tres | Release |\n",
    "|--------|-----------|------------|----------|\n",
    "| **GPT-4** | OpenAI | ~1.76T | 2023 |\n",
    "| **Claude 3** | Anthropic | Non divulgu√© | 2024 |\n",
    "| **Gemini** | Google | ~1.5T | 2024 |\n",
    "| **LLaMA 2** | Meta | 70B | 2023 |\n",
    "| **Mistral** | Mistral AI | 7B | 2023 |\n",
    "\n",
    "**Notre mini-LLM** : ~10 millions de param√®tres (√©ducatif)\n",
    "\n",
    "---\n",
    "\n",
    "## üèõÔ∏è Histoire : L'√âvolution vers les LLMs\n",
    "\n",
    "### Ligne du Temps\n",
    "\n",
    "```\n",
    "1950s  : Premiers syst√®mes bas√©s sur des r√®gles\n",
    "1980s  : R√©seaux de neurones r√©currents (RNN)\n",
    "1997   : LSTM (Long Short-Term Memory)\n",
    "2013   : Word2Vec (embeddings)\n",
    "2017   : üî• TRANSFORMER (r√©volution)\n",
    "2018   : BERT, GPT-1\n",
    "2019   : GPT-2\n",
    "2020   : GPT-3 (emergence)\n",
    "2022   : ChatGPT (explosion)\n",
    "2023   : GPT-4, LLaMA, Claude 2\n",
    "2024   : Claude 3, Gemini, mod√®les open-source\n",
    "```\n",
    "\n",
    "### Le Moment Charni√®re : 2017\n",
    "\n",
    "üìú **Paper** : \"Attention Is All You Need\" (Vaswani et al.)\n",
    "\n",
    "**Innovation** : L'architecture **Transformer** remplace les RNN/LSTM\n",
    "\n",
    "**Pourquoi c'est r√©volutionnaire ?**\n",
    "\n",
    "| RNN/LSTM | Transformer |\n",
    "|----------|-------------|\n",
    "| ‚ùå S√©quentiel (lent) | ‚úÖ Parall√®le (rapide) |\n",
    "| ‚ùå M√©moire limit√©e | ‚úÖ Attention sur tout le contexte |\n",
    "| ‚ùå Difficile √† entra√Æner | ‚úÖ Plus stable |\n",
    "| ‚ùå Max ~1000 tokens | ‚úÖ Jusqu'√† 128k tokens (GPT-4) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports pour ce notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Pr√™t √† explorer les LLMs !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Comment Fonctionne un LLM ? (Vue d'Ensemble)\n",
    "\n",
    "### Le Pipeline Complet\n",
    "\n",
    "```\n",
    "INPUT: \"Le chat mange une\"\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  1Ô∏è‚É£ TOKENIZATION                                    ‚îÇ\n",
    "‚îÇ     Texte ‚Üí Nombres                                 ‚îÇ\n",
    "‚îÇ     \"Le chat mange une\" ‚Üí [142, 2304, 15673, 1739]  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  2Ô∏è‚É£ EMBEDDINGS                                      ‚îÇ\n",
    "‚îÇ     Nombres ‚Üí Vecteurs denses                       ‚îÇ\n",
    "‚îÇ     142 ‚Üí [0.23, -0.45, 0.67, ...] (256 dimensions) ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  3Ô∏è‚É£ POSITIONAL ENCODING                            ‚îÇ\n",
    "‚îÇ     Ajouter l'info de position                      ‚îÇ\n",
    "‚îÇ     \"Le\"(pos=0), \"chat\"(pos=1), ...                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  4Ô∏è‚É£ TRANSFORMER LAYERS (√óN)                        ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ\n",
    "‚îÇ     ‚îÇ  Multi-Head Self-Attention       ‚îÇ ‚Üê MAGIE !  ‚îÇ\n",
    "‚îÇ     ‚îÇ  (contexte complet)              ‚îÇ            ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ\n",
    "‚îÇ     ‚îÇ  Feed-Forward Network            ‚îÇ            ‚îÇ\n",
    "‚îÇ     ‚îÇ  (transformations)               ‚îÇ            ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ\n",
    "‚îÇ     (R√©p√©t√© 4-96 fois selon le mod√®le)              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  5Ô∏è‚É£ OUTPUT PROJECTION                              ‚îÇ\n",
    "‚îÇ     Vecteurs ‚Üí Probabilit√©s sur vocabulaire         ‚îÇ\n",
    "‚îÇ     [0.45, 0.23, 0.12, ...] (50k probabilit√©s)      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  6Ô∏è‚É£ SAMPLING                                        ‚îÇ\n",
    "‚îÇ     Choisir le prochain token                       ‚îÇ\n",
    "‚îÇ     Probabilit√© max ‚Üí \"souris\" (token 4523)         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "OUTPUT: \"Le chat mange une souris\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Le Concept Cl√© : ATTENTION\n",
    "\n",
    "### Analogie Simple\n",
    "\n",
    "Imagine que tu lis cette phrase :\n",
    "\n",
    "> \"Le chat de Marie est mignon. **Il** aime jouer.\"\n",
    "\n",
    "**Question** : √Ä quoi \"Il\" fait-il r√©f√©rence ?\n",
    "\n",
    "**Humain** : Je regarde le contexte ‚Üí \"Il\" = \"le chat\"\n",
    "\n",
    "**LLM avec Attention** : Calcule l'importance de chaque mot pr√©c√©dent pour comprendre \"Il\"\n",
    "\n",
    "### Visualisation de l'Attention\n",
    "\n",
    "```\n",
    "Mot actuel : \"Il\"\n",
    "\n",
    "Regarde le contexte :\n",
    "\"Le\"     ‚Üí  5%  (peu important)\n",
    "\"chat\"   ‚Üí 80%  (tr√®s important) ‚úì\n",
    "\"de\"     ‚Üí  2%  (peu important)\n",
    "\"Marie\"  ‚Üí 10%  (un peu important)\n",
    "\"est\"    ‚Üí  1%  (peu important)\n",
    "\"mignon\" ‚Üí  2%  (peu important)\n",
    "```\n",
    "\n",
    "Le mod√®le \"attend\" (attend to) principalement sur \"chat\" !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration simplifi√©e de l'attention\n",
    "\n",
    "# Phrase exemple\n",
    "words = [\"Le\", \"chat\", \"de\", \"Marie\", \"est\", \"mignon\", \"Il\"]\n",
    "\n",
    "# Scores d'attention (simul√©s) pour le mot \"Il\" regardant les mots pr√©c√©dents\n",
    "# En r√©alit√©, ces scores sont calcul√©s par le mod√®le\n",
    "attention_scores = np.array([0.05, 0.80, 0.02, 0.10, 0.01, 0.02])\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['lightblue' if score < 0.5 else 'salmon' for score in attention_scores]\n",
    "bars = ax.barh(words[:-1], attention_scores, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for i, (bar, score) in enumerate(zip(bars, attention_scores)):\n",
    "    ax.text(score + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "            f'{score:.0%}', \n",
    "            va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Score d\\'Attention', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mots du Contexte', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Attention du mot \"Il\" sur le contexte pr√©c√©dent', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Annotation\n",
    "ax.annotate('\"Il\" se concentre surtout sur \"chat\" !', \n",
    "            xy=(0.80, 1), xytext=(0.5, 3),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=12, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° C'est √ßa l'ATTENTION : le mod√®le apprend automatiquement\")\n",
    "print(\"   sur quels mots se concentrer pour comprendre le contexte !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìê Architectures de Transformers : GPT vs BERT vs T5\n",
    "\n",
    "### Les 3 Grandes Familles\n",
    "\n",
    "#### 1Ô∏è‚É£ **Decoder-Only** (GPT family)\n",
    "\n",
    "```\n",
    "Structure : Decoder seulement\n",
    "Task     : G√©n√©ration de texte (autor√©gressif)\n",
    "Masking  : Causal (ne voit que le pass√©)\n",
    "\n",
    "Exemples : GPT-2, GPT-3, GPT-4, LLaMA, Mistral\n",
    "\n",
    "Usage    : Chatbots, g√©n√©ration cr√©ative, completion\n",
    "```\n",
    "\n",
    "**Notre projet** : On va construire un mod√®le **GPT-style** ! üéØ\n",
    "\n",
    "#### 2Ô∏è‚É£ **Encoder-Only** (BERT family)\n",
    "\n",
    "```\n",
    "Structure : Encoder seulement\n",
    "Task     : Compr√©hension (bidirectionnel)\n",
    "Masking  : Bidirectionnel (voit pass√© ET futur)\n",
    "\n",
    "Exemples : BERT, RoBERTa, ALBERT\n",
    "\n",
    "Usage    : Classification, NER, Q&A\n",
    "```\n",
    "\n",
    "#### 3Ô∏è‚É£ **Encoder-Decoder** (T5 family)\n",
    "\n",
    "```\n",
    "Structure : Encoder + Decoder\n",
    "Task     : Traduction, r√©sum√©\n",
    "\n",
    "Exemples : T5, BART, mT5\n",
    "\n",
    "Usage    : Traduction, r√©sum√©, reformulation\n",
    "```\n",
    "\n",
    "### Comparaison Visuelle\n",
    "\n",
    "```\n",
    "GPT (Decoder-Only)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Input:  \"Le chat mange\"\n",
    "        ‚Üì\n",
    "     [Decoder] ‚Üí Masked Self-Attention (causal)\n",
    "        ‚Üì\n",
    "Output: \"une souris\" (g√©n√©ration)\n",
    "\n",
    "\n",
    "BERT (Encoder-Only)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Input:  \"Le [MASK] mange une souris\"\n",
    "        ‚Üì\n",
    "     [Encoder] ‚Üí Self-Attention (bidirectionnel)\n",
    "        ‚Üì\n",
    "Output: \"chat\" (pr√©diction du masque)\n",
    "\n",
    "\n",
    "T5 (Encoder-Decoder)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Input:  \"Traduire en anglais: Le chat mange\"\n",
    "        ‚Üì\n",
    "     [Encoder] ‚Üí Context\n",
    "        ‚Üì\n",
    "     [Decoder] ‚Üí Generation\n",
    "        ‚Üì\n",
    "Output: \"The cat eats\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Pourquoi GPT pour notre Projet ?\n",
    "\n",
    "### Raisons P√©dagogiques\n",
    "\n",
    "1. ‚úÖ **Plus simple** : Une seule composante (decoder)\n",
    "2. ‚úÖ **Plus intuitif** : G√©n√©ration de texte est facile √† comprendre\n",
    "3. ‚úÖ **Plus populaire** : ChatGPT, Claude = GPT-style\n",
    "4. ‚úÖ **Plus fun** : On peut discuter avec notre mod√®le !\n",
    "\n",
    "### Architecture GPT Simplifi√©e\n",
    "\n",
    "```python\n",
    "class MiniGPT:\n",
    "    def __init__(self):\n",
    "        self.token_embedding = Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEncoding(max_len, d_model)\n",
    "\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(d_model, n_heads)  # ‚Üê On va construire √ßa !\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.output_projection = Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Embeddings\n",
    "        x = self.token_embedding(x) + self.position_embedding(x)\n",
    "\n",
    "        # 2. Transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)  # ‚Üê Magie de l'attention ici !\n",
    "\n",
    "        # 3. Projection\n",
    "        logits = self.output_projection(x)\n",
    "\n",
    "        return logits\n",
    "```\n",
    "\n",
    "**C'est ce qu'on va impl√©menter from scratch !**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Notre Parcours d'Apprentissage\n",
    "\n",
    "### √âtape par √âtape\n",
    "\n",
    "#### Phase 1 : Fondamentaux (Notebooks 00-03)\n",
    "\n",
    "1. **00 - Introduction** ‚Üê Tu es ici !\n",
    "2. **01 - Tokenization** : Texte ‚Üí Nombres\n",
    "   - BPE (Byte-Pair Encoding)\n",
    "   - Construction du vocabulaire\n",
    "   - Encode/Decode\n",
    "\n",
    "3. **02 - Embeddings** : Nombres ‚Üí Vecteurs riches\n",
    "   - Word2Vec concepts\n",
    "   - Embedding layers\n",
    "   - Similarit√© s√©mantique\n",
    "\n",
    "4. **03 - Attention** : Le c≈ìur du Transformer\n",
    "   - Queries, Keys, Values\n",
    "   - Scaled Dot-Product Attention\n",
    "   - Impl√©mentation from scratch\n",
    "\n",
    "#### Phase 2 : Architecture (Notebooks 04-07)\n",
    "\n",
    "5. **04 - Multi-Head Attention** : Plusieurs perspectives\n",
    "6. **05 - Positional Encoding** : Ordre des mots\n",
    "7. **06 - Transformer Block** : Assembler les pi√®ces\n",
    "8. **07 - GPT Complet** : Le mod√®le final\n",
    "\n",
    "#### Phase 3 : Training & Generation (Notebooks 08-11)\n",
    "\n",
    "9. **08 - Dataset** : Pr√©parer les donn√©es\n",
    "10. **09 - Training** : Entra√Æner le mod√®le\n",
    "11. **10 - Generation** : G√©n√©rer du texte\n",
    "12. **11 - Fine-tuning** : Adapter √† des t√¢ches\n",
    "\n",
    "#### Phase 4 : Projet Final (Notebook 12)\n",
    "\n",
    "13. **12 - Mini-ChatGPT** : Projet complet end-to-end\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comparaison : Notre Mini-GPT vs GPT-3\n",
    "\n",
    "### Sp√©cifications\n",
    "\n",
    "| Param√®tre | Notre Mini-GPT | GPT-3 | GPT-4 |\n",
    "|-----------|----------------|-------|-------|\n",
    "| **Vocabulaire** | ~5,000 tokens | 50,257 tokens | ~100k tokens |\n",
    "| **Embedding dim** | 256 | 12,288 | ~18,000 |\n",
    "| **Layers** | 4-6 | 96 | ~120 |\n",
    "| **Attention heads** | 8 | 96 | ~128 |\n",
    "| **Context window** | 128 tokens | 2,048 tokens | 128k tokens |\n",
    "| **Param√®tres** | **~10M** | **175B** | **~1.76T** |\n",
    "| **Dataset** | 1MB texte | 45TB texte | Inconnu |\n",
    "| **Co√ªt d'entra√Ænement** | ~0‚Ç¨ (CPU) | ~$4.6M | ~$100M |\n",
    "| **Temps d'entra√Ænement** | Minutes/Heures | Semaines | Mois |\n",
    "\n",
    "### Objectifs R√©alistes\n",
    "\n",
    "**Notre mini-GPT pourra** :\n",
    "- ‚úÖ G√©n√©rer du texte dans le style de l'entra√Ænement (ex: Shakespeare)\n",
    "- ‚úÖ Compl√©ter des phrases\n",
    "- ‚úÖ Suivre des patterns simples\n",
    "- ‚úÖ D√©montrer la compr√©hension de l'attention\n",
    "\n",
    "**Il NE pourra PAS** :\n",
    "- ‚ùå Rivaliser avec ChatGPT\n",
    "- ‚ùå Raisonner de mani√®re complexe\n",
    "- ‚ùå Suivre des instructions vari√©es\n",
    "- ‚ùå Connaissances du monde r√©el √©tendues\n",
    "\n",
    "**C'est un projet P√âDAGOGIQUE** - L'objectif est la **compr√©hension** ! üéì\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : √âchelle des param√®tres\n",
    "\n",
    "models = ['Notre\\nMini-GPT', 'GPT-2\\nSmall', 'GPT-2\\nLarge', 'GPT-3', 'GPT-4\\n(estim√©)']\n",
    "params = [10e6, 117e6, 1.5e9, 175e9, 1760e9]  # En param√®tres\n",
    "params_billions = [p / 1e9 for p in params]  # Convertir en milliards\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['lightgreen', 'skyblue', 'orange', 'salmon', 'red']\n",
    "bars = ax.bar(models, params_billions, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "# √âchelle log pour mieux voir\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Nombre de Param√®tres (Milliards)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaison de la Taille des Mod√®les', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for bar, p in zip(bars, params_billions):\n",
    "    height = bar.get_height()\n",
    "    if p < 1:\n",
    "        label = f'{p*1000:.0f}M'\n",
    "    else:\n",
    "        label = f'{p:.0f}B'\n",
    "\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            label,\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notre mod√®le est 17,500√ó plus petit que GPT-3 !\")\n",
    "print(\"   Mais il nous permet de COMPRENDRE les concepts fondamentaux.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Exemple Concret : Comment GPT G√©n√®re du Texte\n",
    "\n",
    "### Le Processus Autor√©gressif\n",
    "\n",
    "GPT g√©n√®re un token √† la fois, en utilisant tous les tokens pr√©c√©dents comme contexte.\n",
    "\n",
    "```python\n",
    "# Pseudo-code simplifi√©\n",
    "\n",
    "prompt = \"Il √©tait une fois\"\n",
    "generated = prompt\n",
    "\n",
    "for _ in range(50):  # G√©n√©rer 50 tokens\n",
    "    # 1. Encoder le texte actuel\n",
    "    tokens = tokenize(generated)\n",
    "\n",
    "    # 2. Forward pass dans le mod√®le\n",
    "    logits = model(tokens)\n",
    "\n",
    "    # 3. Obtenir les probabilit√©s du prochain token\n",
    "    probs = softmax(logits[-1])  # Dernier token\n",
    "\n",
    "    # 4. √âchantillonner (sample) le prochain token\n",
    "    next_token = sample(probs, temperature=0.8)\n",
    "\n",
    "    # 5. Ajouter au texte g√©n√©r√©\n",
    "    generated += decode(next_token)\n",
    "```\n",
    "\n",
    "### Exemple Pas √† Pas\n",
    "\n",
    "```\n",
    "Prompt initial : \"Il √©tait une fois\"\n",
    "\n",
    "It√©ration 1:\n",
    "  Contexte : \"Il √©tait une fois\"\n",
    "  Pr√©dictions : {\"un\": 0.35, \"une\": 0.25, \"le\": 0.15, ...}\n",
    "  Choisi : \"un\"\n",
    "  Texte : \"Il √©tait une fois un\"\n",
    "\n",
    "It√©ration 2:\n",
    "  Contexte : \"Il √©tait une fois un\"\n",
    "  Pr√©dictions : {\"roi\": 0.40, \"prince\": 0.20, \"dragon\": 0.10, ...}\n",
    "  Choisi : \"roi\"\n",
    "  Texte : \"Il √©tait une fois un roi\"\n",
    "\n",
    "It√©ration 3:\n",
    "  Contexte : \"Il √©tait une fois un roi\"\n",
    "  Pr√©dictions : {\"qui\": 0.50, \",\": 0.20, \"nomm√©\": 0.15, ...}\n",
    "  Choisi : \"qui\"\n",
    "  Texte : \"Il √©tait une fois un roi qui\"\n",
    "\n",
    "... et ainsi de suite !\n",
    "```\n",
    "\n",
    "**Key insight** : Le mod√®le utilise TOUT le contexte pr√©c√©dent gr√¢ce √† l'attention ! üéØ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Pourquoi Construire from Scratch ?\n",
    "\n",
    "### On pourrait juste utiliser Hugging Face...\n",
    "\n",
    "```python\n",
    "# 2 lignes et c'est fait !\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "```\n",
    "\n",
    "### Mais construire from scratch permet de :\n",
    "\n",
    "1. ‚úÖ **Comprendre en profondeur** comment √ßa marche\n",
    "2. ‚úÖ **D√©mystifier la \"magie\"** de l'attention\n",
    "3. ‚úÖ **Pouvoir d√©boguer** et am√©liorer\n",
    "4. ‚úÖ **Innover** sur de nouvelles architectures\n",
    "5. ‚úÖ **Impressionner** en entretien technique üòé\n",
    "\n",
    "### Philosophie de ce Cours\n",
    "\n",
    "> \"Si tu ne peux pas le construire from scratch,  \n",
    "> tu ne le comprends pas vraiment.\"\n",
    "\n",
    "**Notre approche** :\n",
    "1. Phase 1-3 : **NumPy** (from scratch complet)\n",
    "2. Phase 4 : **PyTorch** (scaling up)\n",
    "3. Comparaison : Notre impl√©mentation vs **Transformers library**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Ressources Compl√©mentaires\n",
    "\n",
    "### Papers Fondamentaux\n",
    "\n",
    "1. **\"Attention Is All You Need\"** (Vaswani et al., 2017)\n",
    "   - Le paper original du Transformer\n",
    "   - https://arxiv.org/abs/1706.03762\n",
    "\n",
    "2. **\"Improving Language Understanding by Generative Pre-Training\"** (Radford et al., 2018)\n",
    "   - GPT-1\n",
    "   - Introduction du pretraining + fine-tuning\n",
    "\n",
    "3. **\"Language Models are Unsupervised Multitask Learners\"** (Radford et al., 2019)\n",
    "   - GPT-2\n",
    "   - Zero-shot learning\n",
    "\n",
    "4. **\"Language Models are Few-Shot Learners\"** (Brown et al., 2020)\n",
    "   - GPT-3\n",
    "   - Emergence de capacit√©s\n",
    "\n",
    "### Tutoriels et Blogs\n",
    "\n",
    "- **The Illustrated Transformer** (Jay Alammar)\n",
    "  - http://jalammar.github.io/illustrated-transformer/\n",
    "  - Visualisations excellentes\n",
    "\n",
    "- **The Annotated Transformer** (Harvard NLP)\n",
    "  - http://nlp.seas.harvard.edu/annotated-transformer/\n",
    "  - Code ligne par ligne\n",
    "\n",
    "- **Andrej Karpathy - \"Let's build GPT\"**\n",
    "  - YouTube : Construction step-by-step\n",
    "\n",
    "### Livres\n",
    "\n",
    "- **\"Speech and Language Processing\"** (Jurafsky & Martin)\n",
    "- **\"Natural Language Processing with Transformers\"** (Tunstall et al.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ R√©sum√© et Prochaines √âtapes\n",
    "\n",
    "### Ce qu'on a appris\n",
    "\n",
    "‚úÖ **Qu'est-ce qu'un LLM** : R√©seau de neurones entra√Æn√© sur du texte  \n",
    "‚úÖ **L'architecture Transformer** : R√©volution de 2017  \n",
    "‚úÖ **Le concept d'Attention** : Regarder le contexte pertinent  \n",
    "‚úÖ **GPT vs BERT vs T5** : Diff√©rentes approches  \n",
    "‚úÖ **Notre objectif** : Construire un mini-GPT from scratch  \n",
    "\n",
    "### Concepts Cl√©s √† Retenir\n",
    "\n",
    "1. üîë **Attention** : Le c≈ìur du Transformer\n",
    "2. üéØ **Autor√©gressif** : G√©n√®re un token √† la fois\n",
    "3. üß± **Composantes** : Embeddings ‚Üí Transformer ‚Üí Projection\n",
    "4. üìä **√âchelle** : Des millions aux billions de param√®tres\n",
    "\n",
    "### Prochaine √âtape\n",
    "\n",
    "**Notebook 01 - Tokenization** üéØ\n",
    "\n",
    "On va apprendre comment transformer du texte en nombres !\n",
    "\n",
    "```\n",
    "\"Bonjour le monde\" ‚Üí [145, 298, 1023]\n",
    "```\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- Les r√©seaux de neurones ne comprennent que les nombres\n",
    "- Le choix du tokenizer impacte les performances\n",
    "- GPT utilise BPE (Byte-Pair Encoding)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Exercices (Optionnels)\n",
    "\n",
    "1. **Recherche** : Trouve la taille (en param√®tres) de LLaMA 2, Mistral 7B, Claude 3\n",
    "2. **R√©flexion** : Pourquoi GPT-4 a besoin de 128k tokens de contexte ?\n",
    "3. **Exploration** : Essaye ChatGPT et observe comment il g√©n√®re du texte token par token\n",
    "4. **Lecture** : Lis \"The Illustrated Transformer\" de Jay Alammar\n",
    "\n",
    "---\n",
    "\n",
    "**Pr√™t pour la suite ? Let's go ! üöÄ**\n",
    "\n",
    "**‚Üí Notebook suivant : `01_tokenization.ipynb`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
