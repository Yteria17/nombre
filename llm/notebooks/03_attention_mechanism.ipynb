{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Attention Mechanism - Le CÅ“ur du Transformer\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre le mÃ©canisme d'attention (CLÃ‰S du Transformer)\n",
    "- Queries, Keys, Values (Q, K, V)\n",
    "- Scaled Dot-Product Attention\n",
    "- ImplÃ©mentation from scratch\n",
    "- Visualisation des scores d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V\n",
    "```\n",
    "\n",
    "- Q (Query): Ce qu'on cherche\n",
    "- K (Key): Les indices\n",
    "- V (Value): Les valeurs Ã  retourner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    d_k = K.shape[-1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    \n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de l'Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple\n",
    "seq_len, d_model = 4, 8\n",
    "\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shape: {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nAttention weights sum: {weights.sum(axis=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Voyons comment chaque token \"regarde\" les autres tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les poids d'attention\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(weights, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            xticklabels=['Token 0', 'Token 1', 'Token 2', 'Token 3'],\n",
    "            yticklabels=['Token 0', 'Token 1', 'Token 2', 'Token 3'])\n",
    "plt.title('Attention Weights Matrix')\n",
    "plt.ylabel('Query (from)')\n",
    "plt.xlabel('Key (to)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
