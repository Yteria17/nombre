{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Embeddings - ReprÃ©sentations Vectorielles du Langage\n",
    "\n",
    "## ðŸŽ¯ Objectifs\n",
    "\n",
    "Dans ce notebook, nous allons dÃ©couvrir :\n",
    "\n",
    "- ðŸ”¢ **Le problÃ¨me des IDs** - Pourquoi les nombres ne suffisent pas\n",
    "- ðŸŽ¨ **Word Embeddings** - ReprÃ©sentations vectorielles denses\n",
    "- ðŸ“ **SimilaritÃ© sÃ©mantique** - Mots similaires = vecteurs proches\n",
    "- ðŸ§® **ImplÃ©mentation** - Embedding layer from scratch\n",
    "- ðŸ” **Word2Vec concepts** - Skip-gram et CBOW\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤” Le ProblÃ¨me des IDs Bruts\n",
    "\n",
    "### Rappel : Tokenization\n",
    "\n",
    "Dans le notebook prÃ©cÃ©dent, on a transformÃ© du texte en IDs :\n",
    "\n",
    "```\n",
    "\"Le chat mange\" â†’ [145, 298, 1023]\n",
    "```\n",
    "\n",
    "### ProblÃ¨me : Les IDs ne capturent PAS la sÃ©mantique\n",
    "\n",
    "```python\n",
    "# Ces IDs sont arbitraires\n",
    "\"chat\"  â†’ ID 145\n",
    "\"chien\" â†’ ID 298\n",
    "\"souris\" â†’ ID 1023\n",
    "\n",
    "# Questions :\n",
    "# - \"chat\" et \"chien\" sont similaires (animaux domestiques)\n",
    "# - Mais 145 et 298 sont aussi Ã©loignÃ©s que 145 et 1023 !\n",
    "# - Comment capturer cette similaritÃ© ?\n",
    "```\n",
    "\n",
    "### Solution : Embeddings !\n",
    "\n",
    "Transformer chaque ID en un **vecteur dense** qui capture le sens :\n",
    "\n",
    "```python\n",
    "\"chat\"  â†’ [0.2, -0.5, 0.8, 0.1, ...]  # 256 dimensions\n",
    "\"chien\" â†’ [0.3, -0.4, 0.7, 0.2, ...]  # Proche de \"chat\" !\n",
    "\"souris\" â†’ [-0.8, 0.9, -0.3, 0.5, ...] # DiffÃ©rent\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"âœ“ PrÃªt pour les embeddings !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Qu'est-ce qu'un Embedding ?\n",
    "\n",
    "### DÃ©finition\n",
    "\n",
    "Un **embedding** est une reprÃ©sentation vectorielle **dense** et **continue** d'un token.\n",
    "\n",
    "### Comparaison : One-Hot vs Embedding\n",
    "\n",
    "#### 1ï¸âƒ£ One-Hot Encoding (NaÃ¯f)\n",
    "\n",
    "```python\n",
    "vocab_size = 50000\n",
    "\n",
    "\"chat\" (ID 145) â†’ [0, 0, 0, ..., 1, ..., 0]  # 1 Ã  la position 145\n",
    "#                  â†‘ 50,000 dimensions !\n",
    "```\n",
    "\n",
    "**ProblÃ¨mes** :\n",
    "- âŒ Vecteurs trÃ¨s grands (vocab_size dimensions)\n",
    "- âŒ Sparse (que des 0 et un seul 1)\n",
    "- âŒ Aucune similaritÃ© (tous orthogonaux)\n",
    "- âŒ Ã‰normÃ©ment de paramÃ¨tres\n",
    "\n",
    "#### 2ï¸âƒ£ Dense Embedding (Moderne) âœ…\n",
    "\n",
    "```python\n",
    "embedding_dim = 256\n",
    "\n",
    "\"chat\" (ID 145) â†’ [0.23, -0.45, 0.67, ..., 0.12]  # 256 dimensions\n",
    "#                  â†‘ Dense (toutes les valeurs utilisÃ©es)\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- âœ… Compact (256 vs 50,000 dimensions)\n",
    "- âœ… Dense (toutes les dimensions informatives)\n",
    "- âœ… Capture la similaritÃ©\n",
    "- âœ… Appris automatiquement\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DÃ©monstration : One-Hot vs Embedding\n",
    "\n",
    "vocab_size = 10\n",
    "embedding_dim = 4\n",
    "\n",
    "# One-Hot\n",
    "def one_hot(token_id, vocab_size):\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[token_id] = 1\n",
    "    return vec\n",
    "\n",
    "# Dense Embedding (alÃ©atoire pour l'exemple)\n",
    "embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
    "\n",
    "def embed(token_id):\n",
    "    return embedding_matrix[token_id]\n",
    "\n",
    "# Comparer\n",
    "token_id = 3\n",
    "\n",
    "print(\"ðŸ”¢ One-Hot Encoding:\")\n",
    "oh = one_hot(token_id, vocab_size)\n",
    "print(f\"   Token {token_id}: {oh}\")\n",
    "print(f\"   Taille: {len(oh)} dimensions\")\n",
    "print(f\"   Sparse: {(oh == 0).sum()} zÃ©ros / {len(oh)}\\n\")\n",
    "\n",
    "print(\"ðŸŽ¨ Dense Embedding:\")\n",
    "emb = embed(token_id)\n",
    "print(f\"   Token {token_id}: {emb}\")\n",
    "print(f\"   Taille: {len(emb)} dimensions\")\n",
    "print(f\"   Dense: Toutes les valeurs utilisÃ©es\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Ã‰conomie de mÃ©moire: {vocab_size / embedding_dim:.1f}Ã— plus compact !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ SimilaritÃ© SÃ©mantique\n",
    "\n",
    "### Concept ClÃ©\n",
    "\n",
    "**Mots similaires â†’ Vecteurs proches dans l'espace**\n",
    "\n",
    "### Mesure : Cosine Similarity\n",
    "\n",
    "```\n",
    "similarity(A, B) = (A Â· B) / (||A|| Ã— ||B||)\n",
    "```\n",
    "\n",
    "- Valeur entre -1 et 1\n",
    "- 1 = TrÃ¨s similaires\n",
    "- 0 = Orthogonaux (pas de relation)\n",
    "- -1 = OpposÃ©s\n",
    "\n",
    "### Exemples Attendus\n",
    "\n",
    "```python\n",
    "similarity(\"roi\", \"reine\")     â‰ˆ 0.85  # TrÃ¨s similaires\n",
    "similarity(\"roi\", \"homme\")     â‰ˆ 0.45  # LiÃ©s\n",
    "similarity(\"roi\", \"pomme\")     â‰ˆ 0.05  # Pas liÃ©s\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calcule la similaritÃ© cosinus entre deux vecteurs\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Test avec des vecteurs simulÃ©s\n",
    "# (En rÃ©alitÃ©, ces embeddings seraient appris)\n",
    "\n",
    "# Simuler des embeddings de mots\n",
    "embeddings = {\n",
    "    'roi': np.array([0.8, 0.6, 0.2, -0.1]),\n",
    "    'reine': np.array([0.75, 0.65, 0.15, -0.05]),  # Proche de roi\n",
    "    'homme': np.array([0.5, 0.3, 0.4, 0.1]),\n",
    "    'femme': np.array([0.45, 0.35, 0.38, 0.12]),\n",
    "    'pomme': np.array([-0.2, -0.8, 0.5, 0.9]),     # TrÃ¨s diffÃ©rent\n",
    "}\n",
    "\n",
    "print(\"ðŸ” SimilaritÃ©s Cosinus:\\n\")\n",
    "\n",
    "pairs = [\n",
    "    ('roi', 'reine'),\n",
    "    ('roi', 'homme'),\n",
    "    ('homme', 'femme'),\n",
    "    ('roi', 'pomme'),\n",
    "    ('reine', 'pomme'),\n",
    "]\n",
    "\n",
    "for word1, word2 in pairs:\n",
    "    sim = cosine_similarity(embeddings[word1], embeddings[word2])\n",
    "    bar = 'â–ˆ' * int(sim * 20) if sim > 0 else ''\n",
    "    print(f\"{word1:<10} â†” {word2:<10} : {sim:+.3f}  {bar}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Plus les mots sont sÃ©mantiquement proches, plus la similaritÃ© est Ã©levÃ©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§® ImplÃ©mentation : Embedding Layer\n",
    "\n",
    "### Principe\n",
    "\n",
    "Un **Embedding Layer** est simplement une **lookup table** (matrice) :\n",
    "\n",
    "```\n",
    "Embedding Matrix : (vocab_size, embedding_dim)\n",
    "\n",
    "Token ID â†’ Ligne de la matrice\n",
    "```\n",
    "\n",
    "### Exemple\n",
    "\n",
    "```python\n",
    "vocab_size = 1000\n",
    "embedding_dim = 256\n",
    "\n",
    "# Matrice d'embeddings\n",
    "E = np.random.randn(1000, 256)\n",
    "\n",
    "# Obtenir l'embedding du token 145\n",
    "token_145 = E[145]  # Vecteur de 256 dimensions\n",
    "```\n",
    "\n",
    "### Initialisation\n",
    "\n",
    "Les embeddings sont **appris** pendant l'entraÃ®nement, mais on les initialise alÃ©atoirement :\n",
    "\n",
    "```python\n",
    "# Initialisation petite et alÃ©atoire\n",
    "E = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer:\n",
    "    \"\"\"\n",
    "    Couche d'embedding simple\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Taille du vocabulaire\n",
    "            embedding_dim: Dimension des vecteurs d'embedding\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Initialisation alÃ©atoire petite\n",
    "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        Forward pass : token IDs â†’ embeddings\n",
    "\n",
    "        Args:\n",
    "            token_ids: Array d'IDs de tokens\n",
    "                       Shape: (batch_size,) ou (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Embeddings correspondants\n",
    "            Shape: (batch_size, embedding_dim) ou (batch_size, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embeddings[token_ids]\n",
    "\n",
    "    def backward(self, token_ids, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass : calcule les gradients pour les embeddings\n",
    "\n",
    "        Args:\n",
    "            token_ids: IDs des tokens\n",
    "            grad_output: Gradient de la loss par rapport aux embeddings\n",
    "\n",
    "        Returns:\n",
    "            Gradients pour la matrice d'embeddings\n",
    "        \"\"\"\n",
    "        grad_embeddings = np.zeros_like(self.embeddings)\n",
    "\n",
    "        # Accumuler les gradients pour chaque token\n",
    "        # Note: plusieurs tokens peuvent partager le mÃªme ID\n",
    "        np.add.at(grad_embeddings, token_ids, grad_output)\n",
    "\n",
    "        return grad_embeddings\n",
    "\n",
    "    def update(self, grad_embeddings, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Met Ã  jour les embeddings avec gradient descent\n",
    "        \"\"\"\n",
    "        self.embeddings -= learning_rate * grad_embeddings\n",
    "\n",
    "print(\"âœ“ Classe EmbeddingLayer dÃ©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de l'embedding layer\n",
    "\n",
    "vocab_size = 100\n",
    "embedding_dim = 16\n",
    "\n",
    "# CrÃ©er la couche\n",
    "embed_layer = EmbeddingLayer(vocab_size, embedding_dim)\n",
    "\n",
    "print(f\"ðŸ“Š Embedding Layer:\")\n",
    "print(f\"   Vocabulaire: {vocab_size} tokens\")\n",
    "print(f\"   Dimension: {embedding_dim}\")\n",
    "print(f\"   ParamÃ¨tres: {vocab_size * embedding_dim:,}\\n\")\n",
    "\n",
    "# Test 1: Un seul token\n",
    "token_id = 42\n",
    "embedding = embed_layer.forward(token_id)\n",
    "\n",
    "print(f\"ðŸ”¢ Token {token_id}:\")\n",
    "print(f\"   Embedding: {embedding[:8]}... (montrÃ© 8/{embedding_dim} dims)\")\n",
    "print(f\"   Shape: {embedding.shape}\\n\")\n",
    "\n",
    "# Test 2: SÃ©quence de tokens\n",
    "sequence = np.array([5, 12, 8, 42, 7])\n",
    "sequence_embeddings = embed_layer.forward(sequence)\n",
    "\n",
    "print(f\"ðŸ“ SÃ©quence {sequence}:\")\n",
    "print(f\"   Embeddings shape: {sequence_embeddings.shape}\")\n",
    "print(f\"   = (seq_length={len(sequence)}, embedding_dim={embedding_dim})\\n\")\n",
    "\n",
    "# Test 3: Batch de sÃ©quences\n",
    "batch = np.array([\n",
    "    [5, 12, 8],\n",
    "    [42, 7, 15],\n",
    "])\n",
    "\n",
    "batch_embeddings = embed_layer.forward(batch)\n",
    "\n",
    "print(f\"ðŸ“¦ Batch {batch.shape}:\")\n",
    "print(f\"   Embeddings shape: {batch_embeddings.shape}\")\n",
    "print(f\"   = (batch_size={batch.shape[0]}, seq_length={batch.shape[1]}, embedding_dim={embedding_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ” Word2Vec : Comment Apprendre des Embeddings\n",
    "\n",
    "### Le ProblÃ¨me\n",
    "\n",
    "Comment apprendre des embeddings qui capturent la sÃ©mantique ?\n",
    "\n",
    "### La Solution : Word2Vec (2013)\n",
    "\n",
    "**IdÃ©e** : \"You shall know a word by the company it keeps\"\n",
    "\n",
    "â†’ Les mots qui apparaissent dans des contextes similaires ont des sens similaires\n",
    "\n",
    "### Deux Architectures\n",
    "\n",
    "#### 1ï¸âƒ£ **Skip-Gram** : PrÃ©dire le contexte Ã  partir du mot\n",
    "\n",
    "```\n",
    "Phrase : \"Le chat mange une souris\"\n",
    "\n",
    "TÃ¢che : Ã‰tant donnÃ© \"chat\", prÃ©dire [\"Le\", \"mange\"]\n",
    "\n",
    "Input:  \"chat\"\n",
    "Output: \"Le\", \"mange\" (mots autour)\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ **CBOW** (Continuous Bag of Words) : Inverse\n",
    "\n",
    "```\n",
    "TÃ¢che : Ã‰tant donnÃ© [\"Le\", \"mange\"], prÃ©dire \"chat\"\n",
    "\n",
    "Input:  \"Le\", \"mange\" (contexte)\n",
    "Output: \"chat\" (mot central)\n",
    "```\n",
    "\n",
    "### EntraÃ®nement\n",
    "\n",
    "En forÃ§ant le modÃ¨le Ã  prÃ©dire les contextes, les embeddings **apprennent** la sÃ©mantique !\n",
    "\n",
    "```python\n",
    "# AprÃ¨s entraÃ®nement Word2Vec\n",
    "\n",
    "# SimilaritÃ©s apprises\n",
    "most_similar(\"roi\") â†’ [\"reine\", \"prince\", \"monarque\", ...]\n",
    "most_similar(\"Paris\") â†’ [\"France\", \"Londres\", \"capitale\", ...]\n",
    "\n",
    "# Analogies cÃ©lÃ¨bres !\n",
    "king - man + woman â‰ˆ queen\n",
    "Paris - France + Italy â‰ˆ Rome\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Embeddings dans les LLMs\n",
    "\n",
    "### GPT vs Word2Vec\n",
    "\n",
    "| Aspect | Word2Vec | GPT |\n",
    "|--------|----------|-----|\n",
    "| **Objectif** | Apprendre embeddings fixes | GÃ©nÃ©ration de texte |\n",
    "| **Embeddings** | Statiques (1 vecteur/mot) | Contextuels (varient selon contexte) |\n",
    "| **EntraÃ®nement** | PrÃ©diction de contexte | Next token prediction |\n",
    "| **Usage** | Features pour autres tÃ¢ches | ModÃ¨le complet |\n",
    "\n",
    "### Embeddings Contextuels\n",
    "\n",
    "Dans GPT, l'embedding **change** selon le contexte :\n",
    "\n",
    "```python\n",
    "# Phrase 1\n",
    "\"La banque est fermÃ©e\"  # banque = institution financiÃ¨re\n",
    "embedding_banque_1 = [0.5, -0.2, 0.8, ...]\n",
    "\n",
    "# Phrase 2\n",
    "\"Je m'assois sur la banque\"  # banque = siÃ¨ge\n",
    "embedding_banque_2 = [-0.3, 0.7, -0.1, ...]  # DiffÃ©rent !\n",
    "```\n",
    "\n",
    "**Comment ?** GrÃ¢ce au **mÃ©canisme d'attention** (prochain notebook !) ðŸŽ¯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Visualisation d'Embeddings\n",
    "\n",
    "### ProblÃ¨me : 256 dimensions\n",
    "\n",
    "On ne peut pas visualiser 256 dimensions ! ðŸ˜…\n",
    "\n",
    "### Solution : RÃ©duction de dimensionnalitÃ©\n",
    "\n",
    "- **PCA** : Projection linÃ©aire en 2D/3D\n",
    "- **t-SNE** : PrÃ©serve les distances locales (meilleur pour viz)\n",
    "\n",
    "```\n",
    "256D â†’ PCA/t-SNE â†’ 2D â†’ On peut visualiser !\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des embeddings simulÃ©s pour des mots\n",
    "# (En rÃ©alitÃ©, on utiliserait des vrais embeddings prÃ©-entraÃ®nÃ©s)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# CatÃ©gories de mots\n",
    "words = {\n",
    "    # Animaux\n",
    "    'chat': np.array([0.8, 0.6, 0.2, -0.1, 0.5, 0.3, -0.2, 0.4]),\n",
    "    'chien': np.array([0.75, 0.65, 0.15, -0.05, 0.45, 0.35, -0.15, 0.35]),\n",
    "    'souris': np.array([0.7, 0.5, 0.25, -0.15, 0.4, 0.25, -0.25, 0.45]),\n",
    "    \n",
    "    # RoyautÃ©\n",
    "    'roi': np.array([0.1, 0.2, 0.8, 0.7, -0.3, -0.4, 0.6, 0.5]),\n",
    "    'reine': np.array([0.15, 0.25, 0.75, 0.65, -0.25, -0.35, 0.65, 0.55]),\n",
    "    'prince': np.array([0.05, 0.15, 0.85, 0.75, -0.35, -0.45, 0.55, 0.45]),\n",
    "    \n",
    "    # Fruits\n",
    "    'pomme': np.array([-0.5, -0.8, 0.1, 0.2, 0.7, 0.9, -0.6, -0.4]),\n",
    "    'orange': np.array([-0.45, -0.75, 0.15, 0.25, 0.65, 0.85, -0.55, -0.35]),\n",
    "    'banane': np.array([-0.55, -0.85, 0.05, 0.15, 0.75, 0.95, -0.65, -0.45]),\n",
    "}\n",
    "\n",
    "# Convertir en matrice\n",
    "word_list = list(words.keys())\n",
    "embeddings_matrix = np.array([words[w] for w in word_list])\n",
    "\n",
    "print(f\"ðŸ“Š Embeddings crÃ©Ã©s:\")\n",
    "print(f\"   {len(word_list)} mots\")\n",
    "print(f\"   {embeddings_matrix.shape[1]} dimensions\")\n",
    "\n",
    "# RÃ©duire Ã  2D avec PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings_matrix)\n",
    "\n",
    "print(f\"\\nâœ“ RÃ©duction PCA: {embeddings_matrix.shape[1]}D â†’ 2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les embeddings\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Couleurs par catÃ©gorie\n",
    "categories = {\n",
    "    'animaux': ['chat', 'chien', 'souris'],\n",
    "    'royautÃ©': ['roi', 'reine', 'prince'],\n",
    "    'fruits': ['pomme', 'orange', 'banane'],\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'animaux': 'steelblue',\n",
    "    'royautÃ©': 'purple',\n",
    "    'fruits': 'orange',\n",
    "}\n",
    "\n",
    "# Plot par catÃ©gorie\n",
    "for category, word_group in categories.items():\n",
    "    indices = [word_list.index(w) for w in word_group]\n",
    "    points = embeddings_2d[indices]\n",
    "    \n",
    "    ax.scatter(points[:, 0], points[:, 1], \n",
    "              s=200, c=colors[category], alpha=0.6, \n",
    "              edgecolors='black', linewidth=2,\n",
    "              label=category.capitalize())\n",
    "    \n",
    "    # Annoter\n",
    "    for i, word in enumerate(word_group):\n",
    "        ax.annotate(word, (points[i, 0], points[i, 1]),\n",
    "                   xytext=(5, 5), textcoords='offset points',\n",
    "                   fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Composante Principale 1', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Composante Principale 2', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Visualisation des Embeddings (PCA 2D)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Observation:\")\n",
    "print(\"   - Les mots de la mÃªme catÃ©gorie sont PROCHES dans l'espace\")\n",
    "print(\"   - Les catÃ©gories diffÃ©rentes sont Ã‰LOIGNÃ‰ES\")\n",
    "print(\"   - C'est la MAGIE des embeddings ! ðŸŽ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ RÃ©sumÃ© et Concepts ClÃ©s\n",
    "\n",
    "### Ce qu'on a appris\n",
    "\n",
    "âœ… **ProblÃ¨me des IDs** : Ne capturent pas la sÃ©mantique  \n",
    "âœ… **Embeddings** : Vecteurs denses qui capturent le sens  \n",
    "âœ… **SimilaritÃ©** : Mots similaires = vecteurs proches  \n",
    "âœ… **Embedding Layer** : Lookup table apprise  \n",
    "âœ… **Word2Vec** : Comment apprendre des embeddings  \n",
    "\n",
    "### Concepts ClÃ©s\n",
    "\n",
    "1. ðŸ”‘ **Dense Embedding** : ReprÃ©sentation compacte et riche\n",
    "2. ðŸ”‘ **Cosine Similarity** : Mesure de proximitÃ© sÃ©mantique\n",
    "3. ðŸ”‘ **Embedding Matrix** : (vocab_size, embedding_dim)\n",
    "4. ðŸ”‘ **Word2Vec** : Skip-gram et CBOW\n",
    "5. ðŸ”‘ **Contextuels** : GPT adapte les embeddings au contexte\n",
    "\n",
    "### Formules Importantes\n",
    "\n",
    "```python\n",
    "# Forward (lookup)\n",
    "embedding = E[token_id]  # Simple indexation !\n",
    "\n",
    "# SimilaritÃ©\n",
    "sim = (A Â· B) / (||A|| Ã— ||B||)\n",
    "\n",
    "# Dimension\n",
    "Embedding matrix : (vocab_size, embedding_dim)\n",
    "GPT-2 : (50257, 768)\n",
    "GPT-3 : (50257, 12288)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Prochaine Ã‰tape\n",
    "\n",
    "**Notebook 03 - Attention Mechanism** ðŸŽ¯\n",
    "\n",
    "Maintenant qu'on a des embeddings, on va apprendre le **mÃ©canisme d'attention** !\n",
    "\n",
    "```\n",
    "Texte â†’ Tokens â†’ IDs â†’ Embeddings â†’ ATTENTION â†’ Contexte enrichi\n",
    "                                     â†‘ Prochain !\n",
    "```\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- L'attention est le **cÅ“ur** du Transformer\n",
    "- Permet de capturer les **relations** entre tous les tokens\n",
    "- C'est ce qui rend les LLMs si puissants\n",
    "\n",
    "**Teaser** :\n",
    "```python\n",
    "# L'attention va rÃ©pondre Ã  des questions comme:\n",
    "# Dans \"Le chat de Marie est mignon. Il aime jouer.\"\n",
    "# \"Il\" fait rÃ©fÃ©rence Ã  quoi ? â†’ \"chat\" (attention Ã©levÃ©e)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercices (Optionnels)\n",
    "\n",
    "1. **ImplÃ©menter** : Ajouter des mÃ©thodes `most_similar()` Ã  l'EmbeddingLayer\n",
    "2. **Visualiser** : Utiliser t-SNE au lieu de PCA et comparer\n",
    "3. **ExpÃ©rimenter** : CrÃ©er des embeddings pour 50 mots et visualiser\n",
    "4. **Analogies** : ImplÃ©menter `king - man + woman = queen`\n",
    "5. **Comparer** : Charger de vrais embeddings Word2Vec ou GloVe et explorer\n",
    "\n",
    "---\n",
    "\n",
    "**PrÃªt pour l'attention ? C'est le concept le plus important ! ðŸš€**\n",
    "\n",
    "**â†’ Notebook suivant : `03_attention_mechanism.ipynb`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
