{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî§ Tokenization - Transformer le Texte en Nombres\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "Dans ce notebook, nous allons apprendre :\n",
    "\n",
    "- üî¢ **Pourquoi tokenizer ?** - Les r√©seaux ne comprennent que les nombres\n",
    "- üìö **Diff√©rentes approches** - Character, Word, Subword\n",
    "- üß© **BPE (Byte-Pair Encoding)** - L'algorithme utilis√© par GPT\n",
    "- üõ†Ô∏è **Impl√©menter un tokenizer** - From scratch en Python\n",
    "- üé® **Encoder/Decoder** - Texte ‚Üî Nombres\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Pourquoi la Tokenization ?\n",
    "\n",
    "### Le Probl√®me\n",
    "\n",
    "Les r√©seaux de neurones ne comprennent que des **nombres** :\n",
    "\n",
    "```\n",
    "‚ùå \"Bonjour le monde\"  ‚Üí ??? Comment donner √ßa au r√©seau ?\n",
    "‚úÖ [145, 298, 1023]     ‚Üí Le r√©seau comprend √ßa !\n",
    "```\n",
    "\n",
    "### La Solution : Tokenization\n",
    "\n",
    "**Tokenization** = D√©couper le texte en unit√©s (tokens) et les associer √† des nombres\n",
    "\n",
    "```\n",
    "Texte : \"Bonjour le monde\"\n",
    "  ‚Üì Tokenization\n",
    "Tokens : [\"Bonjour\", \"le\", \"monde\"]\n",
    "  ‚Üì Encoding\n",
    "IDs : [145, 298, 1023]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "print(\"‚úì Pr√™t √† tokenizer !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Trois Approches de Tokenization\n",
    "\n",
    "### 1Ô∏è‚É£ Character-Level (Par caract√®re)\n",
    "\n",
    "**Id√©e** : Chaque caract√®re = 1 token\n",
    "\n",
    "```python\n",
    "Texte : \"Bonjour\"\n",
    "Tokens : ['B', 'o', 'n', 'j', 'o', 'u', 'r']\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- ‚úÖ Vocabulaire tr√®s petit (~100 caract√®res)\n",
    "- ‚úÖ Pas de mots inconnus (OOV)\n",
    "\n",
    "**Inconv√©nients** :\n",
    "- ‚ùå S√©quences tr√®s longues\n",
    "- ‚ùå Perd la s√©mantique des mots\n",
    "\n",
    "### 2Ô∏è‚É£ Word-Level (Par mot)\n",
    "\n",
    "**Id√©e** : Chaque mot = 1 token\n",
    "\n",
    "```python\n",
    "Texte : \"Bonjour le monde\"\n",
    "Tokens : ['Bonjour', 'le', 'monde']\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- ‚úÖ Pr√©serve la s√©mantique\n",
    "- ‚úÖ S√©quences courtes\n",
    "\n",
    "**Inconv√©nients** :\n",
    "- ‚ùå Vocabulaire √©norme (100k+ mots)\n",
    "- ‚ùå Mots inconnus (OOV problem)\n",
    "- ‚ùå Ne g√®re pas bien les variations (\"jouer\", \"jouait\", \"jouerait\")\n",
    "\n",
    "### 3Ô∏è‚É£ Subword-Level (Par sous-mot) ‚≠ê **BEST**\n",
    "\n",
    "**Id√©e** : D√©couper en morceaux de mots (subwords)\n",
    "\n",
    "```python\n",
    "Texte : \"incroyablement\"\n",
    "Tokens : ['in', 'croy', 'able', 'ment']\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- ‚úÖ Vocabulaire raisonnable (30k-50k tokens)\n",
    "- ‚úÖ Pas de OOV (tout peut √™tre d√©compos√©)\n",
    "- ‚úÖ G√®re bien les variations\n",
    "- ‚úÖ **C'est ce qu'utilise GPT !**\n",
    "\n",
    "**Algorithmes** : BPE, WordPiece, SentencePiece\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple des 3 approches\n",
    "\n",
    "text = \"Bonjour le monde\"\n",
    "\n",
    "# 1. Character-level\n",
    "char_tokens = list(text)\n",
    "print(\"1Ô∏è‚É£ Character-level:\")\n",
    "print(f\"   Tokens: {char_tokens}\")\n",
    "print(f\"   Longueur: {len(char_tokens)} tokens\\n\")\n",
    "\n",
    "# 2. Word-level\n",
    "word_tokens = text.split()\n",
    "print(\"2Ô∏è‚É£ Word-level:\")\n",
    "print(f\"   Tokens: {word_tokens}\")\n",
    "print(f\"   Longueur: {len(word_tokens)} tokens\\n\")\n",
    "\n",
    "# 3. Subword-level (simulation)\n",
    "subword_tokens = ['Bon', 'jour', 'le', 'monde']  # Exemple\n",
    "print(\"3Ô∏è‚É£ Subword-level (simul√©):\")\n",
    "print(f\"   Tokens: {subword_tokens}\")\n",
    "print(f\"   Longueur: {len(subword_tokens)} tokens\")\n",
    "\n",
    "print(\"\\nüí° Subword = Meilleur compromis !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß© BPE - Byte-Pair Encoding\n",
    "\n",
    "### Concept\n",
    "\n",
    "**BPE** est un algorithme de compression adapt√© pour la tokenization :\n",
    "\n",
    "1. Commence avec un vocabulaire de caract√®res\n",
    "2. Trouve la paire de tokens la plus fr√©quente\n",
    "3. Fusionne cette paire en un nouveau token\n",
    "4. R√©p√®te jusqu'√† atteindre la taille de vocabulaire d√©sir√©e\n",
    "\n",
    "### Exemple Pas √† Pas\n",
    "\n",
    "```\n",
    "Texte initial : \"low low low low lower lower newest newest newest widest\"\n",
    "\n",
    "√âtape 0 (caract√®res) :\n",
    "Vocab : ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd']\n",
    "Tokens : ['l','o','w', 'l','o','w', ...]\n",
    "\n",
    "√âtape 1 - Paire la plus fr√©quente : ('l', 'o')\n",
    "Nouveau token : 'lo'\n",
    "Vocab : ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'lo']\n",
    "Tokens : ['lo','w', 'lo','w', ...]\n",
    "\n",
    "√âtape 2 - Paire la plus fr√©quente : ('lo', 'w')\n",
    "Nouveau token : 'low'\n",
    "Vocab : [..., 'lo', 'low']\n",
    "Tokens : ['low', 'low', 'low', 'low', 'lo','w','e','r', ...]\n",
    "\n",
    "√âtape 3 - Paire la plus fr√©quente : ('e', 's')\n",
    "Nouveau token : 'es'\n",
    "...\n",
    "```\n",
    "\n",
    "**R√©sultat** : Vocabulaire optimis√© pour le corpus !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Impl√©mentation d'un Tokenizer BPE Simple\n",
    "\n",
    "On va impl√©menter un tokenizer BPE from scratch !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer BPE simplifi√© pour l'apprentissage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}  # token ‚Üí id\n",
    "        self.inverse_vocab = {}  # id ‚Üí token\n",
    "        self.merges = {}  # paire ‚Üí nouveau_token\n",
    "\n",
    "    def get_pairs(self, word):\n",
    "        \"\"\"\n",
    "        R√©cup√®re toutes les paires cons√©cutives dans un mot\n",
    "        \n",
    "        Exemple : ['l', 'o', 'w'] ‚Üí [('l', 'o'), ('o', 'w')]\n",
    "        \"\"\"\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "\n",
    "    def train(self, corpus, verbose=True):\n",
    "        \"\"\"\n",
    "        Entra√Æne le tokenizer BPE sur un corpus\n",
    "        \n",
    "        Args:\n",
    "            corpus: Liste de mots\n",
    "            verbose: Afficher les √©tapes\n",
    "        \"\"\"\n",
    "        # 1. Initialiser avec les caract√®res\n",
    "        vocab = set()\n",
    "        for word in corpus:\n",
    "            vocab.update(word)\n",
    "\n",
    "        # Cr√©er le vocabulaire initial (caract√®res)\n",
    "        self.vocab = {char: idx for idx, char in enumerate(sorted(vocab))}\n",
    "        self.inverse_vocab = {idx: char for char, idx in self.vocab.items()}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Vocabulaire initial: {len(self.vocab)} caract√®res\")\n",
    "            print(f\"Objectif: {self.vocab_size} tokens\\n\")\n",
    "\n",
    "        # 2. R√©p√©ter les merges jusqu'√† atteindre vocab_size\n",
    "        word_freqs = Counter(corpus)\n",
    "        splits = {word: list(word) for word in word_freqs.keys()}\n",
    "\n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            # Compter toutes les paires\n",
    "            pairs = defaultdict(int)\n",
    "            for word, freq in word_freqs.items():\n",
    "                split = splits[word]\n",
    "                if len(split) == 1:\n",
    "                    continue\n",
    "                for j in range(len(split) - 1):\n",
    "                    pair = (split[j], split[j + 1])\n",
    "                    pairs[pair] += freq\n",
    "\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # Trouver la paire la plus fr√©quente\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Cr√©er le nouveau token\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            self.merges[best_pair] = new_token\n",
    "\n",
    "            # Ajouter au vocabulaire\n",
    "            new_id = len(self.vocab)\n",
    "            self.vocab[new_token] = new_id\n",
    "            self.inverse_vocab[new_id] = new_token\n",
    "\n",
    "            # Mettre √† jour les splits\n",
    "            for word in word_freqs:\n",
    "                split = splits[word]\n",
    "                if len(split) == 1:\n",
    "                    continue\n",
    "\n",
    "                new_split = []\n",
    "                j = 0\n",
    "                while j < len(split):\n",
    "                    if j < len(split) - 1 and (split[j], split[j + 1]) == best_pair:\n",
    "                        new_split.append(new_token)\n",
    "                        j += 2\n",
    "                    else:\n",
    "                        new_split.append(split[j])\n",
    "                        j += 1\n",
    "                splits[word] = new_split\n",
    "\n",
    "            if verbose and (i + 1) % 10 == 0:\n",
    "                print(f\"Merge {i+1}/{num_merges}: {best_pair} ‚Üí '{new_token}' (freq: {pairs[best_pair]})\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n‚úì Entra√Ænement termin√© ! Vocabulaire final: {len(self.vocab)} tokens\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode un texte en liste d'IDs\n",
    "        \n",
    "        Args:\n",
    "            text: Texte √† encoder\n",
    "        \n",
    "        Returns:\n",
    "            Liste d'IDs de tokens\n",
    "        \"\"\"\n",
    "        # Tokenize d'abord en caract√®res\n",
    "        tokens = list(text)\n",
    "\n",
    "        # Appliquer les merges\n",
    "        while len(tokens) >= 2:\n",
    "            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "            \n",
    "            # Trouver la premi√®re paire qui a un merge\n",
    "            merge_found = False\n",
    "            for i, pair in enumerate(pairs):\n",
    "                if pair in self.merges:\n",
    "                    # Appliquer le merge\n",
    "                    new_token = self.merges[pair]\n",
    "                    tokens = tokens[:i] + [new_token] + tokens[i + 2:]\n",
    "                    merge_found = True\n",
    "                    break\n",
    "            \n",
    "            if not merge_found:\n",
    "                break\n",
    "\n",
    "        # Convertir en IDs\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                ids.append(self.vocab[token])\n",
    "            else:\n",
    "                # Token inconnu ‚Üí utiliser un caract√®re sp√©cial ou ignorer\n",
    "                ids.append(0)  # <UNK>\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        D√©code une liste d'IDs en texte\n",
    "        \n",
    "        Args:\n",
    "            ids: Liste d'IDs de tokens\n",
    "        \n",
    "        Returns:\n",
    "            Texte d√©cod√©\n",
    "        \"\"\"\n",
    "        tokens = [self.inverse_vocab.get(id, '<UNK>') for id in ids]\n",
    "        return ''.join(tokens)\n",
    "\n",
    "print(\"‚úì Classe SimpleBPETokenizer d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du Tokenizer BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus d'exemple\n",
    "corpus = [\n",
    "    \"low\", \"low\", \"low\", \"low\",\n",
    "    \"lower\", \"lower\",\n",
    "    \"newest\", \"newest\", \"newest\",\n",
    "    \"widest\"\n",
    "]\n",
    "\n",
    "print(\"üìö Corpus d'entra√Ænement:\")\n",
    "print(f\"   {' '.join(corpus)}\")\n",
    "print(f\"   {len(corpus)} mots\\n\")\n",
    "\n",
    "# Cr√©er et entra√Æner le tokenizer\n",
    "tokenizer = SimpleBPETokenizer(vocab_size=30)\n",
    "tokenizer.train(corpus, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le vocabulaire appris\n",
    "print(\"\\nüìñ Vocabulaire appris:\")\n",
    "print(\"‚îÄ\" * 50)\n",
    "\n",
    "# Grouper par taille de token\n",
    "by_length = defaultdict(list)\n",
    "for token, idx in sorted(tokenizer.vocab.items(), key=lambda x: x[1]):\n",
    "    by_length[len(token)].append(token)\n",
    "\n",
    "for length in sorted(by_length.keys()):\n",
    "    tokens = by_length[length]\n",
    "    print(f\"\\nLongueur {length}: {tokens}\")\n",
    "\n",
    "print(f\"\\nüìä Total: {len(tokenizer.vocab)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester l'encodage/d√©codage\n",
    "test_texts = [\n",
    "    \"low\",\n",
    "    \"lower\",\n",
    "    \"newest\",\n",
    "    \"lowest\"  # Nouveau mot !\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Test Encode/Decode:\")\n",
    "print(\"‚îÄ\" * 70)\n",
    "\n",
    "for text in test_texts:\n",
    "    # Encoder\n",
    "    ids = tokenizer.encode(text)\n",
    "    \n",
    "    # D√©coder\n",
    "    decoded = tokenizer.decode(ids)\n",
    "    \n",
    "    # Afficher les tokens\n",
    "    tokens = [tokenizer.inverse_vocab[id] for id in ids]\n",
    "    \n",
    "    print(f\"\\nTexte:   '{text}'\")\n",
    "    print(f\"Tokens:  {tokens}\")\n",
    "    print(f\"IDs:     {ids}\")\n",
    "    print(f\"D√©cod√©:  '{decoded}'\")\n",
    "    print(f\"Match:   {'‚úì' if text == decoded else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® Visualisation du Processus de Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la distribution des longueurs de tokens\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "token_lengths = [len(token) for token in tokenizer.vocab.keys()]\n",
    "length_counts = Counter(token_lengths)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme des longueurs\n",
    "lengths = sorted(length_counts.keys())\n",
    "counts = [length_counts[l] for l in lengths]\n",
    "\n",
    "ax1.bar(lengths, counts, color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "ax1.set_xlabel('Longueur du Token', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Nombre de Tokens', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution des Longueurs de Tokens', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for l, c in zip(lengths, counts):\n",
    "    ax1.text(l, c + 0.2, str(c), ha='center', fontweight='bold')\n",
    "\n",
    "# Exemple de tokenization\n",
    "example = \"lowest\"\n",
    "example_ids = tokenizer.encode(example)\n",
    "example_tokens = [tokenizer.inverse_vocab[id] for id in example_ids]\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.text(0.5, 0.9, f'Exemple: \"{example}\"', \n",
    "         ha='center', fontsize=16, fontweight='bold', transform=ax2.transAxes)\n",
    "\n",
    "# Afficher la d√©composition\n",
    "y = 0.7\n",
    "for i, (token, tid) in enumerate(zip(example_tokens, example_ids)):\n",
    "    color = plt.cm.Set3(i % 10)\n",
    "    \n",
    "    ax2.add_patch(plt.Rectangle((0.2 + i*0.15, y - 0.05), 0.12, 0.08, \n",
    "                                 facecolor=color, edgecolor='black', linewidth=2,\n",
    "                                 transform=ax2.transAxes))\n",
    "    \n",
    "    ax2.text(0.26 + i*0.15, y, f'\"{token}\"', \n",
    "             ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "             transform=ax2.transAxes)\n",
    "    \n",
    "    ax2.text(0.26 + i*0.15, y - 0.15, f'ID: {tid}', \n",
    "             ha='center', va='center', fontsize=10,\n",
    "             transform=ax2.transAxes)\n",
    "\n",
    "ax2.text(0.5, 0.3, 'Les tokens BPE capturent les sous-mots fr√©quents !', \n",
    "         ha='center', fontsize=12, style='italic', transform=ax2.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåç Tokenizers R√©els : GPT-2, GPT-3, GPT-4\n",
    "\n",
    "### Sp√©cifications\n",
    "\n",
    "| Mod√®le | Algorithme | Vocab Size | Tokens Sp√©ciaux |\n",
    "|--------|-----------|------------|------------------|\n",
    "| **GPT-2** | BPE | 50,257 | `<|endoftext|>` |\n",
    "| **GPT-3** | BPE | 50,257 | `<|endoftext|>` |\n",
    "| **GPT-4** | BPE am√©lior√© | ~100,000 | Multiples |\n",
    "| **BERT** | WordPiece | 30,522 | `[CLS]`, `[SEP]`, `[MASK]` |\n",
    "| **T5** | SentencePiece | 32,000 | `<pad>`, `</s>` |\n",
    "\n",
    "### Tokens Sp√©ciaux\n",
    "\n",
    "Les tokenizers r√©els incluent des tokens sp√©ciaux :\n",
    "\n",
    "```python\n",
    "special_tokens = {\n",
    "    '<PAD>': 0,      # Padding (pour batch)\n",
    "    '<UNK>': 1,      # Unknown token\n",
    "    '<BOS>': 2,      # Beginning of sequence\n",
    "    '<EOS>': 3,      # End of sequence\n",
    "    '<MASK>': 4,     # Masking (BERT)\n",
    "}\n",
    "```\n",
    "\n",
    "### Exemple avec GPT-2 Tokenizer\n",
    "\n",
    "```python\n",
    "# Utiliser le vrai tokenizer GPT-2 (n√©cessite transformers)\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)  # [15496, 11, 703, 389, 345, 30]\n",
    "\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)  # \"Hello, how are you?\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Concepts Importants\n",
    "\n",
    "### 1. Vocabulaire et Taille\n",
    "\n",
    "**Trade-off** :\n",
    "- **Petit vocabulaire** (5k tokens)\n",
    "  - ‚úÖ Moins de param√®tres\n",
    "  - ‚ùå S√©quences plus longues\n",
    "  - ‚ùå Moins expressif\n",
    "\n",
    "- **Grand vocabulaire** (100k tokens)\n",
    "  - ‚úÖ S√©quences plus courtes\n",
    "  - ‚úÖ Plus expressif\n",
    "  - ‚ùå Plus de param√®tres\n",
    "  - ‚ùå Plus de m√©moire\n",
    "\n",
    "**Sweet spot** : 30k-50k tokens\n",
    "\n",
    "### 2. Out-of-Vocabulary (OOV)\n",
    "\n",
    "**Probl√®me** : Que faire avec un mot jamais vu ?\n",
    "\n",
    "```python\n",
    "# Word-level tokenizer\n",
    "vocab = {\"chat\": 0, \"chien\": 1, \"oiseau\": 2}\n",
    "text = \"hamster\"  # ‚ùå Pas dans le vocab !\n",
    "\n",
    "# Solution 1: Token <UNK>\n",
    "encode(\"hamster\") ‚Üí [3]  # ID de <UNK>\n",
    "\n",
    "# Solution 2: BPE (meilleur !)\n",
    "encode(\"hamster\") ‚Üí [\"ham\", \"ster\"] ‚Üí [456, 789]\n",
    "# Pas de OOV car d√©compos√© en sous-mots connus\n",
    "```\n",
    "\n",
    "### 3. Whitespace et Ponctuation\n",
    "\n",
    "**Important** : Comment g√©rer les espaces ?\n",
    "\n",
    "```python\n",
    "# GPT-2 utilise des \"ƒ†\" pour les espaces\n",
    "\"Hello world\" ‚Üí [\"Hello\", \"ƒ†world\"]\n",
    "#                         ‚Üë Espace pr√©serv√©\n",
    "\n",
    "# Permet de reconstruire exactement le texte\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Impact de la Tokenization sur les LLMs\n",
    "\n",
    "### 1. Longueur de S√©quence\n",
    "\n",
    "```python\n",
    "text = \"L'intelligence artificielle est fascinante\"\n",
    "\n",
    "# Character-level (43 tokens)\n",
    "chars = list(text)  # ['L', \"'\", 'i', 'n', 't', ...]\n",
    "\n",
    "# Word-level (4 tokens)\n",
    "words = text.split()  # [\"L'intelligence\", \"artificielle\", \"est\", \"fascinante\"]\n",
    "\n",
    "# BPE (6-8 tokens)\n",
    "bpe = [\"L\", \"'\", \"intelligence\", \"artific\", \"ielle\", \"est\", \"fasc\", \"inante\"]\n",
    "```\n",
    "\n",
    "**Impact** :\n",
    "- Plus de tokens = Plus de calcul\n",
    "- Context window limit√© (ex: 2048 tokens pour GPT-3)\n",
    "\n",
    "### 2. Multilinguisme\n",
    "\n",
    "**Probl√®me** : Langues non-latines\n",
    "\n",
    "```python\n",
    "# Anglais (efficace)\n",
    "\"Hello\" ‚Üí 1 token\n",
    "\n",
    "# Chinois (moins efficace avec BPE traditionnel)\n",
    "\"‰Ω†Â•Ω\" ‚Üí 2-4 tokens (selon le tokenizer)\n",
    "\n",
    "# Solution: SentencePiece (utilis√© par mT5, XLM-R)\n",
    "# G√®re mieux toutes les langues\n",
    "```\n",
    "\n",
    "### 3. Code et Symboles\n",
    "\n",
    "```python\n",
    "code = \"def hello(): print('Hi')\"\n",
    "\n",
    "# BPE peut mal d√©couper le code\n",
    "# ‚Üí Tokenizers sp√©cialis√©s pour le code (CodeGen, CodeT5)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Exercice Pratique : Analyser un Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texte √† analyser\n",
    "long_text = \"\"\"\n",
    "L'intelligence artificielle transforme notre monde. \n",
    "Les mod√®les de langage comme GPT-4 peuvent comprendre et g√©n√©rer du texte.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Texte original:\")\n",
    "print(long_text)\n",
    "print(f\"\\nLongueur: {len(long_text)} caract√®res\\n\")\n",
    "\n",
    "# Cr√©er un corpus simple\n",
    "words = long_text.lower().split()\n",
    "corpus_words = [word.strip('.,!?') for word in words if word.strip('.,!?')]\n",
    "\n",
    "print(f\"üìö Corpus: {len(corpus_words)} mots\")\n",
    "\n",
    "# Entra√Æner un tokenizer\n",
    "small_tokenizer = SimpleBPETokenizer(vocab_size=50)\n",
    "small_tokenizer.train(corpus_words, verbose=False)\n",
    "\n",
    "print(f\"\\nüî§ Vocabulaire: {len(small_tokenizer.vocab)} tokens\")\n",
    "\n",
    "# Encoder le texte\n",
    "encoded = small_tokenizer.encode(long_text.lower())\n",
    "print(f\"\\nüî¢ Encod√©: {len(encoded)} tokens\")\n",
    "print(f\"   Compression: {len(long_text) / len(encoded):.1f}√ó (caract√®res/tokens)\")\n",
    "\n",
    "# Afficher quelques tokens\n",
    "print(f\"\\nüìä Premiers tokens:\")\n",
    "for i, tid in enumerate(encoded[:15]):\n",
    "    token = small_tokenizer.inverse_vocab.get(tid, '<UNK>')\n",
    "    print(f\"   {i}: '{token}' (ID: {tid})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì R√©sum√© et Concepts Cl√©s\n",
    "\n",
    "### Ce qu'on a appris\n",
    "\n",
    "‚úÖ **Pourquoi tokenizer** : Les r√©seaux ne comprennent que les nombres  \n",
    "‚úÖ **3 approches** : Character, Word, Subword (BPE)  \n",
    "‚úÖ **BPE** : Algorithme de compression it√©ratif  \n",
    "‚úÖ **Impl√©mentation** : Tokenizer BPE from scratch  \n",
    "‚úÖ **Encode/Decode** : Texte ‚Üî IDs  \n",
    "\n",
    "### Concepts Cl√©s\n",
    "\n",
    "1. üîë **Token** : Unit√© de base (caract√®re, mot, ou sous-mot)\n",
    "2. üîë **Vocabulaire** : Ensemble de tous les tokens possibles\n",
    "3. üîë **BPE** : Fusionne it√©rativement les paires fr√©quentes\n",
    "4. üîë **OOV** : Out-of-Vocabulary, r√©solu par BPE\n",
    "5. üîë **Trade-offs** : Taille vocab vs longueur s√©quence\n",
    "\n",
    "### Impact sur les LLMs\n",
    "\n",
    "- Le tokenizer d√©termine **comment le texte est repr√©sent√©**\n",
    "- Impacte la **longueur des s√©quences** (donc le co√ªt)\n",
    "- Affecte le **multilinguisme** et la **gestion du code**\n",
    "- GPT utilise **BPE avec ~50k tokens**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Prochaine √âtape\n",
    "\n",
    "**Notebook 02 - Embeddings** üéØ\n",
    "\n",
    "Maintenant qu'on sait transformer du texte en IDs, on va apprendre √† transformer ces IDs en **vecteurs riches** !\n",
    "\n",
    "```\n",
    "Texte ‚Üí Tokens ‚Üí IDs ‚Üí EMBEDDINGS (vecteurs denses)\n",
    "                        ‚Üë Prochain notebook !\n",
    "```\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- Les IDs sont juste des nombres arbitraires (145, 298, 1023)\n",
    "- Les embeddings capturent la **s√©mantique** (sens)\n",
    "- Mots similaires ‚Üí vecteurs proches\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Exercices (Optionnels)\n",
    "\n",
    "1. **Modifier le tokenizer** : Ajouter des tokens sp√©ciaux `<PAD>`, `<UNK>`, `<BOS>`, `<EOS>`\n",
    "2. **Corpus plus grand** : Entra√Æner sur un fichier texte complet\n",
    "3. **Comparer** : Impl√©menter un tokenizer Word-level et comparer avec BPE\n",
    "4. **Visualiser** : Cr√©er un graphe montrant comment BPE merge les tokens\n",
    "5. **Explorer** : Utiliser `transformers` library pour essayer le vrai GPT-2 tokenizer\n",
    "\n",
    "---\n",
    "\n",
    "**Pr√™t pour les embeddings ? Let's go ! üöÄ**\n",
    "\n",
    "**‚Üí Notebook suivant : `02_embeddings.ipynb`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
