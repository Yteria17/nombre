# ğŸš€ Guide de DÃ©marrage - LLM from Scratch

## ğŸ¯ Bienvenue !

Ce guide te permettra de **construire ton propre Large Language Model (comme GPT) from scratch** et de comprendre en profondeur comment fonctionnent ChatGPT, Claude, et autres LLMs modernes.

---

## ğŸ“š Parcours d'Apprentissage Complet

### ğŸŒŸ Phase 1 : Fondamentaux (4-6 heures)

Avant de construire le Transformer, il faut comprendre les briques de base.

#### **Notebook 00 - Introduction aux LLMs** âœ… (30 min)
**Fichier** : `notebooks/00_introduction_llms.ipynb`

**Tu vas apprendre** :
- Qu'est-ce qu'un LLM ?
- L'histoire : RNN â†’ LSTM â†’ Transformer (2017)
- Pourquoi l'attention est rÃ©volutionnaire
- GPT vs BERT vs T5
- Notre objectif : mini-GPT (~10M paramÃ¨tres)

**Concepts clÃ©s** :
- Transformer architecture
- Autoregressive generation
- Attention mechanism (aperÃ§u)

---

#### **Notebook 01 - Tokenization** âœ… (1h)
**Fichier** : `notebooks/01_tokenization.ipynb`

**Tu vas apprendre** :
- Pourquoi tokenizer ? (rÃ©seaux = nombres seulement)
- 3 approches : Character, Word, **Subword (BPE)**
- ImplÃ©mentation complÃ¨te d'un tokenizer BPE
- Encode/Decode

**ImplÃ©mentation** :
```python
class SimpleBPETokenizer:
    def train(corpus)  # Apprendre le vocabulaire
    def encode(text)   # Texte â†’ IDs
    def decode(ids)    # IDs â†’ Texte
```

**RÃ©sultat** :
```python
"Bonjour le monde" â†’ [145, 298, 1023]
```

---

#### **Notebook 02 - Embeddings** âœ… (1h)
**Fichier** : `notebooks/02_embeddings.ipynb`

**Tu vas apprendre** :
- Le problÃ¨me des IDs bruts (pas de sÃ©mantique)
- Embeddings = vecteurs denses qui capturent le sens
- SimilaritÃ© cosinus
- Word2Vec (Skip-gram, CBOW)

**ImplÃ©mentation** :
```python
class EmbeddingLayer:
    def forward(token_ids)   # IDs â†’ Vecteurs
    def backward(gradients)  # Backprop
```

**RÃ©sultat** :
```python
"chat"  â†’ [0.2, -0.5, 0.8, ...]  # 256D
"chien" â†’ [0.3, -0.4, 0.7, ...]  # Proche !
```

---

#### **Notebook 03 - Attention Mechanism** ğŸ”„ (1h30)
**Fichier** : `notebooks/03_attention_mechanism.ipynb` (Ã€ venir)

**Tu vas apprendre** :
- Le cÅ“ur du Transformer : **Attention**
- Queries, Keys, Values (Q, K, V)
- Scaled Dot-Product Attention
- ImplÃ©mentation from scratch

**Formule clÃ©** :
```
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V
```

**RÃ©sultat** :
- Comprendre comment le modÃ¨le "regarde" le contexte
- Visualiser les scores d'attention

---

### ğŸ—ï¸ Phase 2 : Architecture Transformer (6-8 heures)

Maintenant qu'on a les bases, construisons le Transformer !

#### **Notebook 04 - Multi-Head Attention** ğŸ”„
Plusieurs "tÃªtes" d'attention en parallÃ¨le

#### **Notebook 05 - Positional Encoding** ğŸ”„
IntÃ©grer l'information de position (ordre des mots)

#### **Notebook 06 - Transformer Block** ğŸ”„
Assembler : Attention + Feed-Forward + LayerNorm + Residual

#### **Notebook 07 - Architecture GPT ComplÃ¨te** ğŸ”„
Le modÃ¨le final : Embedding â†’ NÃ—TransformerBlock â†’ Output

---

### ğŸš€ Phase 3 : EntraÃ®nement et GÃ©nÃ©ration (6-8 heures)

#### **Notebook 08 - Dataset et Preprocessing** ğŸ”„
PrÃ©parer Tiny Shakespeare pour l'entraÃ®nement

#### **Notebook 09 - Training Loop** ğŸ”„
EntraÃ®ner le modÃ¨le (forward, backward, update)

#### **Notebook 10 - Text Generation** ğŸ”„
GÃ©nÃ©rer du texte : Greedy, Top-k, Top-p, Temperature

#### **Notebook 11 - Fine-tuning** ğŸ”„
Adapter le modÃ¨le Ã  des tÃ¢ches spÃ©cifiques

---

### ğŸ“ Phase 4 : Projet Final (3-4 heures)

#### **Notebook 12 - Mini-ChatGPT Project** ğŸ”„
Projet complet end-to-end avec interface de chat

---

## ğŸ› ï¸ Installation et Setup

### PrÃ©requis

- Python 3.8+
- Jupyter Notebook
- Connaissances de base en Python et NumPy

### Installation

```bash
# Cloner le repo
cd llm/

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer Jupyter
jupyter notebook
```

### Ouvrir le premier notebook

```bash
jupyter notebook notebooks/00_introduction_llms.ipynb
```

---

## ğŸ“– Comment Utiliser ce Cours

### 1ï¸âƒ£ **Approche LinÃ©aire** (RecommandÃ©e pour dÃ©butants)

Suis les notebooks dans l'ordre :

```
00 â†’ 01 â†’ 02 â†’ 03 â†’ 04 â†’ 05 â†’ 06 â†’ 07 â†’ 08 â†’ 09 â†’ 10 â†’ 11 â†’ 12
```

**Temps total** : ~20 heures

**Rythme suggÃ©rÃ©** :
- Semaine 1 : Notebooks 00-03 (Fondamentaux)
- Semaine 2 : Notebooks 04-07 (Architecture)
- Semaine 3 : Notebooks 08-11 (Training)
- Semaine 4 : Notebook 12 (Projet final)

### 2ï¸âƒ£ **Approche par Objectifs**

Tu veux juste comprendre un concept spÃ©cifique ?

- **Comprendre l'attention** â†’ Notebooks 00, 03, 04
- **Tokenization** â†’ Notebooks 01
- **EntraÃ®ner un modÃ¨le** â†’ Notebooks 08, 09
- **GÃ©nÃ©rer du texte** â†’ Notebooks 10

### 3ï¸âƒ£ **Approche Pratique** (Pour les expÃ©rimentÃ©s)

Tu connais dÃ©jÃ  la thÃ©orie et veux coder ?

1. Lis rapidement les notebooks 00-02
2. Code avec les notebooks 03-07 (architecture)
3. Projet final (notebook 12)

---

## ğŸ’¡ Conseils d'Apprentissage

### âœ… Ã€ Faire

1. **ExÃ©cute TOUT le code** : Ne te contente pas de lire
2. **ExpÃ©rimente** : Modifie les paramÃ¨tres, observe les rÃ©sultats
3. **Prends des notes** : Ã‰cris ce que tu comprends
4. **Fais les exercices** : Ils renforcent la comprÃ©hension
5. **Visualise** : Les graphiques aident Ã©normÃ©ment

### âŒ Ã€ Ã‰viter

1. âŒ Ne saute pas les notebooks : Ils sont progressifs
2. âŒ Ne te dÃ©courage pas : Les LLMs sont complexes, c'est normal
3. âŒ Ne copie pas aveuglÃ©ment : Comprends chaque ligne
4. âŒ Ne t'arrÃªte pas aux erreurs : Debug et apprends

### ğŸ¯ Objectifs d'Apprentissage

Ã€ la fin de ce cours, tu sauras :

âœ… Comment fonctionne l'attention (le cÅ“ur des LLMs)
âœ… Pourquoi les Transformers sont rÃ©volutionnaires
âœ… Comment tokenizer du texte (BPE)
âœ… Comment les embeddings capturent la sÃ©mantique
âœ… Comment construire un Transformer from scratch
âœ… Comment entraÃ®ner un modÃ¨le de langage
âœ… Comment gÃ©nÃ©rer du texte de qualitÃ©
âœ… Les diffÃ©rences entre GPT, BERT, T5

---

## ğŸ§ª CompÃ©tences Acquises

### Niveau DÃ©butant (Notebooks 00-03)

- Comprendre ce qu'est un LLM
- Tokenizer du texte
- CrÃ©er des embeddings
- Le mÃ©canisme d'attention de base

### Niveau IntermÃ©diaire (Notebooks 04-07)

- Multi-head attention
- Positional encoding
- Construire un Transformer block
- Assembler un modÃ¨le GPT complet

### Niveau AvancÃ© (Notebooks 08-12)

- PrÃ©parer un dataset
- EntraÃ®ner un LLM
- GÃ©nÃ©rer du texte avec stratÃ©gies variÃ©es
- Fine-tuner pour des tÃ¢ches spÃ©cifiques
- Projet complet end-to-end

---

## ğŸ“Š Comparaison : Ce que tu vas construire

| ParamÃ¨tre | Notre Mini-GPT | GPT-2 | GPT-3 |
|-----------|----------------|-------|-------|
| **Vocabulaire** | ~5,000 | 50,257 | 50,257 |
| **Embedding dim** | 256 | 768 | 12,288 |
| **Layers** | 4-6 | 12-48 | 96 |
| **Attention heads** | 8 | 12-16 | 96 |
| **ParamÃ¨tres** | **~10M** | 117M-1.5B | **175B** |
| **Dataset** | Tiny Shakespeare | WebText | 45TB texte |
| **EntraÃ®nement** | Minutes (CPU) | Heures (GPU) | Semaines (cluster) |
| **Performance** | Style Shakespeare | Texte cohÃ©rent | ChatGPT level |

**Notre objectif** : Comprendre les concepts, pas rivaliser avec ChatGPT ! ğŸ“

---

## ğŸ—‚ï¸ Structure du Projet

```
llm/
â”œâ”€â”€ README.md                      # Vue d'ensemble
â”œâ”€â”€ GUIDE.md                       # Ce fichier - Guide de dÃ©marrage
â”œâ”€â”€ requirements.txt               # DÃ©pendances
â”‚
â”œâ”€â”€ notebooks/                     # Notebooks d'apprentissage
â”‚   â”œâ”€â”€ âœ… 00_introduction_llms.ipynb
â”‚   â”œâ”€â”€ âœ… 01_tokenization.ipynb
â”‚   â”œâ”€â”€ âœ… 02_embeddings.ipynb
â”‚   â”œâ”€â”€ ğŸ”„ 03_attention_mechanism.ipynb      (Ã€ venir)
â”‚   â”œâ”€â”€ ğŸ”„ 04_multi_head_attention.ipynb     (Ã€ venir)
â”‚   â”œâ”€â”€ ğŸ”„ 05_positional_encoding.ipynb      (Ã€ venir)
â”‚   â”œâ”€â”€ ğŸ”„ 06_transformer_block.ipynb        (Ã€ venir)
â”‚   â”œâ”€â”€ ğŸ”„ 07_gpt_architecture.ipynb         (Ã€ venir)
â”‚   â”œâ”€â”€ ğŸ”„ 08_dataset_preprocessing.ipynb    (Ã€ venir)
â”‚   â”œâ”€â”€ ğŸ”„ 09_training_loop.ipynb            (Ã€ venir)
â”‚   â”œâ”€â”€ ğŸ”„ 10_text_generation.ipynb          (Ã€ venir)
â”‚   â”œâ”€â”€ ğŸ”„ 11_fine_tuning.ipynb              (Ã€ venir)
â”‚   â””â”€â”€ ğŸ”„ 12_mini_chatgpt_project.ipynb     (Ã€ venir)
â”‚
â”œâ”€â”€ src/                           # Code source (Ã€ implÃ©menter)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tokenizer.py              # BPE tokenizer
â”‚   â”œâ”€â”€ embeddings.py             # Embedding layer
â”‚   â”œâ”€â”€ attention.py              # Attention mechanisms
â”‚   â”œâ”€â”€ transformer.py            # Transformer blocks
â”‚   â”œâ”€â”€ model.py                  # Full GPT model
â”‚   â”œâ”€â”€ training.py               # Training utilities
â”‚   â”œâ”€â”€ generation.py             # Text generation
â”‚   â””â”€â”€ utils.py                  # Helper functions
â”‚
â”œâ”€â”€ data/                          # Datasets
â”‚   â””â”€â”€ tiny_shakespeare.txt      # Dataset d'exemple
â”‚
â”œâ”€â”€ models/                        # ModÃ¨les entraÃ®nÃ©s
â”‚   â””â”€â”€ checkpoints/              # Sauvegardes
â”‚
â””â”€â”€ tests/                         # Tests unitaires
    â””â”€â”€ test_*.py
```

---

## ğŸ“š Ressources ComplÃ©mentaires

### Papers Fondamentaux

1. **"Attention Is All You Need"** (Vaswani et al., 2017)
   - LE paper du Transformer
   - https://arxiv.org/abs/1706.03762

2. **"Language Models are Unsupervised Multitask Learners"** (GPT-2)
   - https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

3. **"Language Models are Few-Shot Learners"** (GPT-3)
   - https://arxiv.org/abs/2005.14165

### Tutoriels RecommandÃ©s

- **The Illustrated Transformer** (Jay Alammar)
  - http://jalammar.github.io/illustrated-transformer/
  - Visualisations excellentes

- **The Annotated Transformer** (Harvard NLP)
  - http://nlp.seas.harvard.edu/annotated-transformer/
  - Code ligne par ligne avec PyTorch

- **Andrej Karpathy - "Let's build GPT"**
  - YouTube : Construction step-by-step
  - https://www.youtube.com/watch?v=kCc8FmEb1nY

### Livres

- **"Speech and Language Processing"** (Jurafsky & Martin)
  - Gratuit en ligne
- **"Natural Language Processing with Transformers"** (Tunstall et al.)
  - Livre pratique avec Hugging Face

---

## ğŸ†˜ Aide et Support

### Tu es bloquÃ© ?

1. **Relis le notebook** : La rÃ©ponse est souvent lÃ 
2. **VÃ©rifie les erreurs** : Lis les messages d'erreur
3. **Debug** : Utilise `print()` pour comprendre
4. **Compare** : VÃ©rifie avec le code du notebook
5. **ExpÃ©rimente** : Teste sur un exemple simple

### Ressources de Debug

```python
# VÃ©rifier les shapes
print(f"Shape: {tensor.shape}")

# VÃ©rifier les valeurs
print(f"Min: {tensor.min()}, Max: {tensor.max()}, Mean: {tensor.mean()}")

# VÃ©rifier les NaN
print(f"Has NaN: {np.isnan(tensor).any()}")
```

---

## ğŸ¯ Prochaines Ã‰tapes

### Tu as terminÃ© les 3 premiers notebooks ?

âœ… **FÃ©licitations !** Tu as compris les fondamentaux :
- Tokenization (texte â†’ nombres)
- Embeddings (nombres â†’ vecteurs riches)

### Continue avec :

1. **Notebook 03 - Attention** â† LE concept le plus important !
2. Puis les notebooks 04-07 (architecture)
3. Puis les notebooks 08-11 (training)
4. Enfin le projet final (12)

---

## ğŸ’¬ Feedback et Contributions

Ce projet est Ã©ducatif et open-source. Les suggestions d'amÃ©lioration sont bienvenues !

---

## ğŸ“ Citation

Si tu utilises ce cours pour apprendre ou enseigner, mentionne :

```
"LLM from Scratch - Cours Ã©ducatif complet pour comprendre
les Large Language Models (GPT) en construisant from scratch"
```

---

## â­ Philosophie du Cours

> "Si tu ne peux pas le construire from scratch,
> tu ne le comprends pas vraiment."

Ce cours te fait construire **chaque composant** d'un LLM pour que tu comprennes **vraiment** comment Ã§a marche.

---

**Bon apprentissage ! ğŸš€**

**Commence maintenant** â†’ Ouvre `notebooks/00_introduction_llms.ipynb`

---

## ğŸ—“ï¸ Changelog

- **2025-01-14** : CrÃ©ation du projet LLM
  - âœ… README complet
  - âœ… Notebook 00 - Introduction
  - âœ… Notebook 01 - Tokenization
  - âœ… Notebook 02 - Embeddings
  - ğŸ”„ Notebooks 03-12 en cours de crÃ©ation
