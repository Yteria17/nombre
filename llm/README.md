# ğŸ¤– LLM from Scratch - Apprendre Ã  crÃ©er un Grand ModÃ¨le de Langage

## ğŸ¯ Objectif

Construire un **Large Language Model** (type GPT) **from scratch** pour comprendre en profondeur comment fonctionnent ChatGPT, Claude, et autres LLMs modernes.

> âš ï¸ **Note** : On va crÃ©er un **mini-LLM** Ã©ducatif (millions de paramÃ¨tres), pas un modÃ¨le de production (milliards). L'objectif est la **comprÃ©hension**, pas la performance ultime.

---

## ğŸ—ºï¸ Parcours d'Apprentissage

### ğŸ“˜ Phase 1 : Fondamentaux (Notebooks 00-03)

| # | Notebook | Concepts ClÃ©s | DurÃ©e |
|---|----------|---------------|-------|
| **00** | Introduction aux LLMs | Architecture Transformer, GPT vs BERT, Tokens | 30min |
| **01** | Tokenization | BPE, WordPiece, Vocabulaire, Encoding/Decoding | 1h |
| **02** | Embeddings | Word2Vec concepts, Embedding layers, SimilaritÃ© | 1h |
| **03** | Attention Mechanism | Queries, Keys, Values, Scaled Dot-Product | 1h30 |

### ğŸ—ï¸ Phase 2 : Architecture Transformer (Notebooks 04-07)

| # | Notebook | Concepts ClÃ©s | DurÃ©e |
|---|----------|---------------|-------|
| **04** | Multi-Head Attention | Parallel attention, Concatenation, Projections | 1h30 |
| **05** | Positional Encoding | Position information, Sinusoidal encoding | 1h |
| **06** | Transformer Block | LayerNorm, Residual connections, FFN | 1h30 |
| **07** | Architecture GPT ComplÃ¨te | Decoder-only, Causal masking, Full model | 2h |

### ğŸš€ Phase 3 : EntraÃ®nement et GÃ©nÃ©ration (Notebooks 08-11)

| # | Notebook | Concepts ClÃ©s | DurÃ©e |
|---|----------|---------------|-------|
| **08** | Dataset et Preprocessing | Text corpus, Batching, Data loading | 1h |
| **09** | Training Loop | Loss calculation, Gradient descent, Monitoring | 2h |
| **10** | Text Generation | Greedy, Beam search, Top-k, Top-p, Temperature | 1h30 |
| **11** | Fine-tuning | Instruction tuning, RLHF concepts, Chat format | 2h |

### ğŸ“ Phase 4 : Projet Final (Notebook 12)

| # | Notebook | Description | DurÃ©e |
|---|----------|-------------|-------|
| **12** | **Mini-ChatGPT** | Projet complet end-to-end | 3h |

**Temps total estimÃ© : ~20 heures**

---

## ğŸ§  Concepts Couverts

### Fondamentaux MathÃ©matiques
- âœ… Produits matriciels et tenseurs
- âœ… Softmax et probabilitÃ©s
- âœ… Fonctions de perte (Cross-Entropy)
- âœ… Backpropagation Ã  travers le temps
- âœ… Optimisation (Adam, learning rate scheduling)

### Architecture Transformer
- âœ… **Self-Attention** : Comment le modÃ¨le "regarde" le contexte
- âœ… **Multi-Head Attention** : Plusieurs perspectives simultanÃ©es
- âœ… **Positional Encoding** : IntÃ©grer l'ordre des mots
- âœ… **Feed-Forward Networks** : Transformations non-linÃ©aires
- âœ… **Layer Normalization** : Stabiliser l'entraÃ®nement
- âœ… **Residual Connections** : Faciliter le gradient flow

### Training et GÃ©nÃ©ration
- âœ… **Tokenization** : Transformer texte â†’ nombres
- âœ… **Causal Masking** : EmpÃªcher de "tricher" (regarder le futur)
- âœ… **Teacher Forcing** : Technique d'entraÃ®nement
- âœ… **Sampling Strategies** : ContrÃ´ler la crÃ©ativitÃ©
- âœ… **Temperature** : Ajuster la diversitÃ© des rÃ©ponses
- âœ… **Top-k / Top-p** : Filtrage intelligent des tokens

---

## ğŸ“ Structure du Projet

```
llm/
â”œâ”€â”€ README.md                      # Ce fichier
â”œâ”€â”€ GUIDE.md                       # Guide de dÃ©marrage rapide
â”œâ”€â”€ requirements.txt               # DÃ©pendances Python
â”‚
â”œâ”€â”€ notebooks/                     # Notebooks d'apprentissage
â”‚   â”œâ”€â”€ 00_introduction_llms.ipynb
â”‚   â”œâ”€â”€ 01_tokenization.ipynb
â”‚   â”œâ”€â”€ 02_embeddings.ipynb
â”‚   â”œâ”€â”€ 03_attention_mechanism.ipynb
â”‚   â”œâ”€â”€ 04_multi_head_attention.ipynb
â”‚   â”œâ”€â”€ 05_positional_encoding.ipynb
â”‚   â”œâ”€â”€ 06_transformer_block.ipynb
â”‚   â”œâ”€â”€ 07_gpt_architecture.ipynb
â”‚   â”œâ”€â”€ 08_dataset_preprocessing.ipynb
â”‚   â”œâ”€â”€ 09_training_loop.ipynb
â”‚   â”œâ”€â”€ 10_text_generation.ipynb
â”‚   â”œâ”€â”€ 11_fine_tuning.ipynb
â”‚   â””â”€â”€ 12_mini_chatgpt_project.ipynb
â”‚
â”œâ”€â”€ src/                           # Code source
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tokenizer.py              # BPE tokenizer
â”‚   â”œâ”€â”€ embeddings.py             # Embedding layers
â”‚   â”œâ”€â”€ attention.py              # Attention mechanisms
â”‚   â”œâ”€â”€ transformer.py            # Transformer blocks
â”‚   â”œâ”€â”€ model.py                  # Full GPT model
â”‚   â”œâ”€â”€ training.py               # Training utilities
â”‚   â”œâ”€â”€ generation.py             # Text generation
â”‚   â””â”€â”€ utils.py                  # Helper functions
â”‚
â”œâ”€â”€ data/                          # Datasets
â”‚   â”œâ”€â”€ tiny_shakespeare.txt      # Dataset d'exemple
â”‚   â”œâ”€â”€ vocab/                    # Vocabulaires
â”‚   â””â”€â”€ processed/                # DonnÃ©es preprocessÃ©es
â”‚
â”œâ”€â”€ models/                        # ModÃ¨les entraÃ®nÃ©s
â”‚   â””â”€â”€ checkpoints/              # Sauvegardes
â”‚
â”œâ”€â”€ tests/                         # Tests unitaires
â”‚   â”œâ”€â”€ test_tokenizer.py
â”‚   â”œâ”€â”€ test_attention.py
â”‚   â””â”€â”€ test_model.py
â”‚
â”œâ”€â”€ train.py                       # Script d'entraÃ®nement CLI
â”œâ”€â”€ generate.py                    # Script de gÃ©nÃ©ration CLI
â””â”€â”€ chat.py                        # Interface de chat interactive
```

---

## ğŸ¯ Objectifs d'Apprentissage

Ã€ la fin de ce parcours, tu sauras :

1. âœ… **Comment fonctionne l'attention** (le cÅ“ur des LLMs)
2. âœ… **Pourquoi les Transformers sont rÃ©volutionnaires**
3. âœ… **Comment un texte est transformÃ© en nombres** (tokenization)
4. âœ… **Comment les LLMs "comprennent" le contexte**
5. âœ… **Comment entraÃ®ner un modÃ¨le de langage**
6. âœ… **Comment gÃ©nÃ©rer du texte de qualitÃ©**
7. âœ… **Les diffÃ©rences entre GPT, BERT, T5**
8. âœ… **Comment fine-tuner pour des tÃ¢ches spÃ©cifiques**

---

## ğŸ’» Technologies UtilisÃ©es

### Phase 1-3 (Apprentissage)
- **NumPy** : ImplÃ©mentation from scratch pour comprendre
- **Matplotlib** : Visualisations

### Phase 4 (Projet pratique)
- **PyTorch** : Framework moderne pour le projet final
- **Transformers (HuggingFace)** : Comparaison avec l'Ã©tat de l'art

---

## ğŸš€ Quick Start

```bash
# Installation
cd llm
pip install -r requirements.txt

# Lancer le premier notebook
jupyter notebook notebooks/00_introduction_llms.ipynb

# Ou suivre l'ordre recommandÃ© :
# 00 â†’ 01 â†’ 02 â†’ 03 â†’ ... â†’ 12
```

---

## ğŸ“Š Mini-LLM : SpÃ©cifications

Le modÃ¨le final que nous allons construire :

| ParamÃ¨tre | Valeur | Note |
|-----------|--------|------|
| **Vocabulaire** | ~5,000 tokens | Petit mais fonctionnel |
| **Embedding dimension** | 256 | ReprÃ©sentation de chaque token |
| **Nombre de layers** | 4-6 | GPT-3 en a 96 ! |
| **Attention heads** | 8 | Perspectives multiples |
| **Context window** | 128-256 tokens | GPT-4 va jusqu'Ã  128k |
| **ParamÃ¨tres totaux** | ~10M | GPT-3 : 175B |
| **Dataset** | Tiny Shakespeare | ~1MB de texte |

**Performance attendue** :
- âœ… GÃ©nÃ¨re du texte cohÃ©rent dans le style Shakespeare
- âœ… ComplÃ¨te des phrases correctement
- âœ… Peut Ãªtre fine-tunÃ© pour des tÃ¢ches simples
- âŒ Ne rivalise PAS avec ChatGPT (objectif pÃ©dagogique)

---

## ğŸ“ PrÃ©requis

### Connaissances
- âœ… Python (numpy, classes)
- âœ… RÃ©seaux de neurones basiques (si tu as fait le projet `nombre/`, parfait !)
- âœ… AlgÃ¨bre linÃ©aire (matrices, vecteurs)
- âš ï¸ **Pas besoin d'Ãªtre expert** - tout est expliquÃ© !

### MatÃ©riel
- **CPU suffit** pour les notebooks d'apprentissage
- **GPU recommandÃ©** pour entraÃ®ner le modÃ¨le final (ou Google Colab gratuit)

---

## ğŸ“š Ressources ComplÃ©mentaires

- **Paper original** : "Attention Is All You Need" (Vaswani et al., 2017)
- **GPT Paper** : "Language Models are Unsupervised Multitask Learners"
- **Illustrated Transformer** : http://jalammar.github.io/illustrated-transformer/
- **Andrej Karpathy** : "Let's build GPT from scratch" (YouTube)

---

## ğŸ—“ï¸ Plan de Progression RecommandÃ©

### Semaine 1 : Fondamentaux
- Jour 1-2 : Notebooks 00-01 (Intro, Tokenization)
- Jour 3-4 : Notebooks 02-03 (Embeddings, Attention)

### Semaine 2 : Architecture
- Jour 1-2 : Notebooks 04-05 (Multi-head, Positional)
- Jour 3-4 : Notebooks 06-07 (Transformer Block, GPT complet)

### Semaine 3 : Training
- Jour 1-2 : Notebooks 08-09 (Dataset, Training)
- Jour 3-4 : Notebooks 10-11 (Generation, Fine-tuning)

### Semaine 4 : Projet
- Jour 1-5 : Notebook 12 (Mini-ChatGPT complet)

**Ou Ã  ton rythme ! C'est auto-guidÃ©.**

---

## ğŸ¯ DiffÃ©rences avec le Projet `nombre/`

| Aspect | RÃ©seaux de Neurones | LLMs |
|--------|---------------------|------|
| **ComplexitÃ©** | â­â­ | â­â­â­â­â­ |
| **Architecture** | MLP, CNN | Transformer |
| **Input** | Images fixes | SÃ©quences variables |
| **Output** | Classification | GÃ©nÃ©ration de texte |
| **MÃ©canisme clÃ©** | Convolution | Attention |
| **Dataset** | 60k images | Millions de tokens |

**Les LLMs sont BEAUCOUP plus complexes**, mais on va y aller Ã©tape par Ã©tape !

---

## ğŸ¤ Contribution

Ce projet est Ã©ducatif. Suggestions et amÃ©liorations bienvenues !

---

**PrÃªt Ã  construire ton propre mini-GPT ? Let's go ! ğŸš€**

*Commence par le notebook `00_introduction_llms.ipynb`*
