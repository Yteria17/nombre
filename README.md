# ğŸ”¢ Nombre - Reconnaissance de Chiffres Manuscrits

Un projet d'apprentissage pour comprendre les rÃ©seaux de neurones en implÃ©mentant from scratch un systÃ¨me de reconnaissance de chiffres manuscrits (MNIST).

## ğŸ¯ Objectif

Apprendre le fonctionnement interne des rÃ©seaux de neurones en construisant un classificateur de chiffres **sans utiliser de frameworks** comme PyTorch, TensorFlow ou Keras. Seulement NumPy, Matplotlib et les mathÃ©matiques !

## âœ¨ FonctionnalitÃ©s

- âœ… ImplÃ©mentation from scratch d'un rÃ©seau de neurones multi-couches
- âœ… EntraÃ®nement sur le dataset MNIST (60,000 images)
- âœ… Interface graphique pour dessiner et tester des chiffres
- âœ… Visualisation de l'apprentissage (courbes, matrice de confusion)
- âœ… Visualisation des poids et features apprises
- âœ… Sauvegarde/chargement des modÃ¨les entraÃ®nÃ©s
- âœ… Comparaison de diffÃ©rentes architectures
- âœ… Data augmentation
- âœ… Tests unitaires pour valider l'implÃ©mentation

## ğŸš€ Quick Start

### Installation

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/Yteria17/nombre.git
cd nombre

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### EntraÃ®ner un modÃ¨le

```bash
# EntraÃ®nement simple avec paramÃ¨tres par dÃ©faut
python train.py

# EntraÃ®nement avec paramÃ¨tres personnalisÃ©s
python train.py --epochs 20 --batch-size 64 --lr 0.001 --hidden-layers 128 64
```

### Tester avec l'interface graphique

```bash
python draw_interface.py
```

Dessinez un chiffre et voyez la prÃ©diction en temps rÃ©el !

### Ã‰valuer le modÃ¨le

```bash
python evaluate.py --model-path models/best_model.pkl
```

## ğŸ“Š RÃ©sultats

| ModÃ¨le | Architecture | Accuracy | Temps d'entraÃ®nement |
|--------|-------------|----------|---------------------|
| Simple MLP | 784-128-64-10 | ~96% | 5 min (CPU) |
| MLP OptimisÃ© | 784-256-128-64-10 | ~98% | 15 min (CPU) |

## ğŸ§  Architecture

### RÃ©seau Simple (Version 1)

```
Input (784)  â†’  Hidden (128)  â†’  Hidden (64)  â†’  Output (10)
               [ReLU]           [ReLU]          [Softmax]
```

### Composants ImplÃ©mentÃ©s

- **Couches** : Dense (fully connected)
- **Activations** : Sigmoid, ReLU, Tanh, Softmax
- **Loss** : Cross-Entropy, MSE
- **Optimisateurs** : SGD, SGD + Momentum, Adam
- **RÃ©gularisation** : L2, Dropout (version avancÃ©e)

## ğŸ“ Structure du Projet

```
nombre/
â”œâ”€â”€ src/                   # Code source
â”‚   â”œâ”€â”€ network.py         # RÃ©seau de neurones
â”‚   â”œâ”€â”€ layers.py          # Couches
â”‚   â”œâ”€â”€ activations.py     # Fonctions d'activation
â”‚   â”œâ”€â”€ losses.py          # Fonctions de coÃ»t
â”‚   â”œâ”€â”€ optimizers.py      # Optimisateurs
â”‚   â”œâ”€â”€ utils.py           # Utilitaires
â”‚   â”œâ”€â”€ visualize.py       # Visualisation
â”‚   â””â”€â”€ metrics.py         # MÃ©triques
â”œâ”€â”€ notebooks/             # Notebooks Jupyter
â”œâ”€â”€ tests/                 # Tests unitaires
â”œâ”€â”€ models/                # ModÃ¨les sauvegardÃ©s
â”œâ”€â”€ train.py               # Script d'entraÃ®nement
â”œâ”€â”€ evaluate.py            # Script d'Ã©valuation
â””â”€â”€ draw_interface.py      # Interface de test
```

## ğŸ“š Documentation

- **[CLAUDE.md](./CLAUDE.md)** - Documentation complÃ¨te du projet
- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - Explications mathÃ©matiques dÃ©taillÃ©es
- **[notebooks/](./notebooks/)** - Tutoriels interactifs

## ğŸ“ Apprendre avec ce Projet

### 1. Comprendre les Bases
Lisez `ARCHITECTURE.md` pour comprendre :
- La propagation avant (forward pass)
- La rÃ©tropropagation (backpropagation)
- La descente de gradient
- Les fonctions d'activation

### 2. Explorer le Code
Commencez par les notebooks :
1. `01_exploration.ipynb` - Dataset MNIST
2. `02_simple_network.ipynb` - Premier rÃ©seau
3. `03_improvements.ipynb` - Optimisations

### 3. ExpÃ©rimenter
Testez diffÃ©rentes configurations :
- Nombre de couches
- Taille des couches
- Learning rate
- Fonctions d'activation
- Optimisateurs

## ğŸ§ª Tests

```bash
# Lancer tous les tests
pytest tests/

# Tests spÃ©cifiques
pytest tests/test_network.py
pytest tests/test_activations.py
```

## ğŸ”¬ ExpÃ©rimentations

Quelques idÃ©es d'expÃ©rimentations :

1. **Impact du learning rate**
   ```bash
   python train.py --lr 0.001
   python train.py --lr 0.01
   python train.py --lr 0.1
   ```

2. **Architecture profonde vs large**
   ```bash
   python train.py --hidden-layers 512 256 128 64  # Profond
   python train.py --hidden-layers 256             # Large
   ```

3. **DiffÃ©rents optimisateurs**
   ```bash
   python train.py --optimizer sgd
   python train.py --optimizer momentum
   python train.py --optimizer adam
   ```

## ğŸ“ˆ Visualisations

Le projet inclut plusieurs visualisations :

- **Courbes d'apprentissage** : Loss et accuracy au fil des epochs
- **Matrice de confusion** : Performance par classe
- **Poids de la premiÃ¨re couche** : Ce que les neurones "voient"
- **Exemples d'erreurs** : Images mal classifiÃ©es
- **Distribution des probabilitÃ©s** : Confiance du modÃ¨le

## ğŸ› ï¸ Technologies

- **Python 3.8+**
- **NumPy** - Calculs matriciels
- **Matplotlib** - Visualisation
- **Pillow** - Manipulation d'images
- **Jupyter** - Notebooks interactifs
- **pytest** - Tests unitaires

## ğŸ¤ Contribution

Ce projet est un projet d'apprentissage. Les suggestions d'amÃ©liorations sont les bienvenues !

## ğŸ“ TODO

- [ ] ImplÃ©mentation rÃ©seau simple (MLP)
- [ ] EntraÃ®nement et Ã©valuation
- [ ] Interface de dessin
- [ ] Visualisations avancÃ©es
- [ ] Dropout et batch normalization
- [ ] Data augmentation
- [ ] Optimisateurs avancÃ©s (Adam)
- [ ] Tests unitaires complets

## ğŸ“– Ressources

- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Michael Nielsen - Neural Networks Book](http://neuralnetworksanddeeplearning.com/)
- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)

## ğŸ“„ Licence

Ce projet est Ã  but Ã©ducatif.

---

**Note** : Ce projet est conÃ§u pour l'apprentissage. Pour des applications en production, utilisez des frameworks optimisÃ©s comme PyTorch ou TensorFlow.