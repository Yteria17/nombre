# ğŸ”¢ Nombre - Reconnaissance de Chiffres Manuscrits

Un projet d'apprentissage pour comprendre les rÃ©seaux de neurones en implÃ©mentant from scratch un systÃ¨me de reconnaissance de chiffres manuscrits (MNIST).

## ğŸ¯ Objectif

Apprendre le fonctionnement interne des rÃ©seaux de neurones en construisant un classificateur de chiffres **sans utiliser de frameworks** comme PyTorch, TensorFlow ou Keras. Seulement NumPy, Matplotlib et les mathÃ©matiques !

## âœ¨ FonctionnalitÃ©s

- âœ… ImplÃ©mentation from scratch d'un rÃ©seau de neurones multi-couches
- âœ… EntraÃ®nement sur le dataset MNIST (60,000 images)
- âœ… Interface graphique pour dessiner et tester des chiffres
- âœ… Visualisation de l'apprentissage (courbes, matrice de confusion)
- âœ… Visualisation des poids et features apprises
- âœ… Sauvegarde/chargement des modÃ¨les entraÃ®nÃ©s
- âœ… Comparaison de diffÃ©rentes architectures
- âœ… Data augmentation
- âœ… Tests unitaires pour valider l'implÃ©mentation

## ğŸš€ Quick Start

### Installation

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/Yteria17/nombre.git
cd nombre

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### EntraÃ®ner un modÃ¨le

```bash
# EntraÃ®nement simple avec paramÃ¨tres par dÃ©faut
python train.py

# EntraÃ®nement avec paramÃ¨tres personnalisÃ©s
python train.py --epochs 20 --batch-size 64 --lr 0.001 --hidden-layers 128 64
```

### Tester avec l'interface graphique

```bash
python draw_interface.py
```

Dessinez un chiffre et voyez la prÃ©diction en temps rÃ©el !

### Ã‰valuer le modÃ¨le

```bash
python evaluate.py --model-path models/best_model.pkl
```

## ğŸ“Š RÃ©sultats

| ModÃ¨le | Architecture | Accuracy | Temps d'entraÃ®nement |
|--------|-------------|----------|---------------------|
| Simple MLP | 784-128-64-10 | ~96% | 5 min (CPU) |
| MLP OptimisÃ© | 784-256-128-64-10 | ~98% | 15 min (CPU) |

## ğŸ§  Architecture

### RÃ©seau Simple (Version 1)

```
Input (784)  â†’  Hidden (128)  â†’  Hidden (64)  â†’  Output (10)
               [ReLU]           [ReLU]          [Softmax]
```

### Composants ImplÃ©mentÃ©s

- **Couches** : Dense (fully connected)
- **Activations** : Sigmoid, ReLU, Tanh, Softmax
- **Loss** : Cross-Entropy, MSE
- **Optimisateurs** : SGD, SGD + Momentum, Adam
- **RÃ©gularisation** : L2, Dropout (version avancÃ©e)

## ğŸ“ Structure du Projet

```
nombre/
â”œâ”€â”€ src/                              # Code source professionnel
â”‚   â”œâ”€â”€ network.py                    # Classe NeuralNetwork complÃ¨te
â”‚   â”œâ”€â”€ layers.py                     # Dense, Dropout, BatchNorm
â”‚   â”œâ”€â”€ activations.py                # ReLU, Sigmoid, Softmax, Tanh
â”‚   â”œâ”€â”€ losses.py                     # Cross-Entropy, MSE
â”‚   â”œâ”€â”€ optimizers.py                 # SGD, Momentum, Adam, RMSprop
â”‚   â”œâ”€â”€ utils.py                      # Chargement MNIST, helpers
â”‚   â”œâ”€â”€ visualize.py                  # Graphiques et visualisations
â”‚   â””â”€â”€ metrics.py                    # Accuracy, confusion matrix
â”‚
â”œâ”€â”€ notebooks/                        # ğŸ““ SÃ©rie complÃ¨te de tutoriels
â”‚   â”œâ”€â”€ 00_introduction_reseaux_neurones.ipynb    # Concepts de base
â”‚   â”œâ”€â”€ 01_exploration_mnist.ipynb                # Dataset MNIST
â”‚   â”œâ”€â”€ 02_forward_propagation.ipynb              # Forward pass
â”‚   â”œâ”€â”€ 03_backpropagation.ipynb                  # Backprop & learning
â”‚   â”œâ”€â”€ 04_building_complete_network.ipynb        # RÃ©seau complet
â”‚   â”œâ”€â”€ 05_improvements_optimization.ipynb        # Optimisations
â”‚   â”œâ”€â”€ 06_introduction_cnn.ipynb                 # CNN basics ğŸ”¥
â”‚   â””â”€â”€ 07_debugging_gradient_checking.ipynb      # Debugging tools ğŸ›
â”‚
â”œâ”€â”€ tests/                            # Tests unitaires
â”œâ”€â”€ models/                           # ModÃ¨les entraÃ®nÃ©s
â”œâ”€â”€ train.py                          # Script d'entraÃ®nement CLI
â”œâ”€â”€ evaluate.py                       # Ã‰valuation de modÃ¨les
â””â”€â”€ draw_interface.py                 # ğŸ¨ Interface graphique interactive
```

## ğŸ“š Documentation

- **[CLAUDE.md](./CLAUDE.md)** - Documentation complÃ¨te du projet
- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - Explications mathÃ©matiques dÃ©taillÃ©es
- **[notebooks/](./notebooks/)** - Tutoriels interactifs

## ğŸ“ Apprendre avec ce Projet

### ğŸ““ Parcours d'Apprentissage (8 Notebooks)

#### **Niveau DÃ©butant** ğŸŒ±

1. **`00_introduction_reseaux_neurones.ipynb`**
   - ğŸ§  Concepts fondamentaux
   - Anatomie d'un neurone artificiel
   - Fonctions d'activation (ReLU, Sigmoid, Softmax)
   - Architecture rÃ©seau et cycle d'apprentissage
   - Exemple : ProblÃ¨me XOR

2. **`01_exploration_mnist.ipynb`**
   - ğŸ” DÃ©couverte du dataset MNIST
   - Visualisation des donnÃ©es (25 Ã©chantillons)
   - Distribution des classes
   - Analyse pixel par pixel
   - VariabilitÃ© intra-classe

3. **`02_forward_propagation.ipynb`**
   - â¡ï¸ Comment un rÃ©seau fait des prÃ©dictions
   - ImplÃ©mentation pas Ã  pas
   - Initialisation des poids (He)
   - Test sur donnÃ©es rÃ©elles
   - Visualisation des activations

4. **`03_backpropagation.ipynb`**
   - â¬…ï¸ Comment un rÃ©seau apprend
   - Fonction de coÃ»t (Cross-Entropy)
   - Descente de gradient
   - ImplÃ©mentation complÃ¨te
   - Cycle d'entraÃ®nement

#### **Niveau IntermÃ©diaire** ğŸš€

5. **`04_building_complete_network.ipynb`**
   - ğŸ—ï¸ Construire un rÃ©seau complet from scratch
   - Classe NeuralNetwork modulaire
   - EntraÃ®nement sur MNIST (60k exemples)
   - Visualisations (loss, accuracy, confusion matrix)
   - Sauvegarde/chargement modÃ¨le
   - **RÃ©sultat : ~95-97% accuracy**

6. **`05_improvements_optimization.ipynb`**
   - ğŸš€ Optimisations avancÃ©es
   - Comparaison d'architectures
   - Impact du learning rate
   - Data augmentation
   - Optimiseurs (SGD, Momentum, Adam)
   - **Path to 98%+ accuracy**

#### **Niveau AvancÃ©** ğŸ”¥

7. **`06_introduction_cnn.ipynb`** ğŸ”¥
   - ğŸ–¼ï¸ RÃ©seaux Convolutifs (CNN)
   - Pourquoi les CNN pour les images ?
   - OpÃ©ration de convolution (filtres)
   - Max pooling
   - SimpleCNN from scratch
   - Feature maps visualization
   - **~98-99% accuracy possible**

8. **`07_debugging_gradient_checking.ipynb`** ğŸ›
   - ğŸ” Debugging et validation
   - Gradient checking numÃ©rique
   - Vanishing/exploding gradients
   - Checklist de debugging complÃ¨te
   - Outils de monitoring
   - **CompÃ©tence essentielle !**

### ğŸ’» Applications Pratiques

#### ğŸ¨ **Interface Graphique**
```bash
python draw_interface.py
```
- Dessiner Ã  la souris
- PrÃ©diction en temps rÃ©el
- Visualisation des probabilitÃ©s
- Parfait pour dÃ©mos et tests !

### ğŸ”¬ ExpÃ©rimenter
Testez diffÃ©rentes configurations :
- Nombre de couches
- Taille des couches
- Learning rate
- Fonctions d'activation
- Optimisateurs

## ğŸ§ª Tests

```bash
# Lancer tous les tests
pytest tests/

# Tests spÃ©cifiques
pytest tests/test_network.py
pytest tests/test_activations.py
```

## ğŸ”¬ ExpÃ©rimentations

Quelques idÃ©es d'expÃ©rimentations :

1. **Impact du learning rate**
   ```bash
   python train.py --lr 0.001
   python train.py --lr 0.01
   python train.py --lr 0.1
   ```

2. **Architecture profonde vs large**
   ```bash
   python train.py --hidden-layers 512 256 128 64  # Profond
   python train.py --hidden-layers 256             # Large
   ```

3. **DiffÃ©rents optimisateurs**
   ```bash
   python train.py --optimizer sgd
   python train.py --optimizer momentum
   python train.py --optimizer adam
   ```

## ğŸ“ˆ Visualisations

Le projet inclut plusieurs visualisations :

- **Courbes d'apprentissage** : Loss et accuracy au fil des epochs
- **Matrice de confusion** : Performance par classe
- **Poids de la premiÃ¨re couche** : Ce que les neurones "voient"
- **Exemples d'erreurs** : Images mal classifiÃ©es
- **Distribution des probabilitÃ©s** : Confiance du modÃ¨le

## ğŸ› ï¸ Technologies

- **Python 3.8+**
- **NumPy** - Calculs matriciels
- **Matplotlib** - Visualisation
- **Pillow** - Manipulation d'images
- **Jupyter** - Notebooks interactifs
- **pytest** - Tests unitaires

## ğŸ¤ Contribution

Ce projet est un projet d'apprentissage. Les suggestions d'amÃ©liorations sont les bienvenues !

## ğŸ“ TODO

- [ ] ImplÃ©mentation rÃ©seau simple (MLP)
- [ ] EntraÃ®nement et Ã©valuation
- [ ] Interface de dessin
- [ ] Visualisations avancÃ©es
- [ ] Dropout et batch normalization
- [ ] Data augmentation
- [ ] Optimisateurs avancÃ©s (Adam)
- [ ] Tests unitaires complets

## ğŸ“– Ressources

- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Michael Nielsen - Neural Networks Book](http://neuralnetworksanddeeplearning.com/)
- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)

## ğŸ“„ Licence

Ce projet est Ã  but Ã©ducatif.

---

**Note** : Ce projet est conÃ§u pour l'apprentissage. Pour des applications en production, utilisez des frameworks optimisÃ©s comme PyTorch ou TensorFlow.